{
    "cells": [
        {
            "cell_type": "raw",
            "id": "53d884f2-85f0-43dd-a0f3-21e3b03e4f16",
            "metadata": {},
            "source": [
                "### import the required modules\n",
                "import arcpy\n",
                "from arcpy.sa import *\n",
                "\n",
                "### Specify the workspace and enable overwrite\n",
                "workspace = 'D:/MyDataBase'\n",
                "arcpy.env.workspace = workspace\n",
                "arcpy.env.overwriteOutput = True\n",
                "RESOLUTION = 30\n",
                "### Specify the source DEM dataset\n",
                "DEM_source = f\"DEM/DEM_{RESOLUTION}m.tif\"\n",
                "\n",
                "### Get the extent of the DEM\n",
                "desc = arcpy.Describe(DEM_source)\n",
                "extent = desc.extent\n",
                "\n",
                "### Calculate the width and height of the extent\n",
                "width = extent.width\n",
                "height = extent.height\n",
                "\n",
                "### Calculate the number of rows and columns based on the desired resolution (250 m)\n",
                "\n",
                "num_rows = int(height / RESOLUTION)\n",
                "num_cols = int(width / RESOLUTION)\n",
                "\n",
                "### Create a fishnet point feature class\n",
                "output_point_shp = f'Grid/Centroid_DEM_{RESOLUTION}m.shp'\n",
                "\n",
                "# Convert raster to point\n",
                "arcpy.RasterToPoint_conversion(DEM_source, output_point_shp, \"Value\")\n",
                "\n",
                "# Print a message indicating the completion of the conversion\n",
                "print(\"Converted DEM to point shapefile successfully.\")\n"
            ]
        },
        {
            "cell_type": "raw",
            "id": "b3c7f26b-0f3d-4541-8803-291d0e8fa0e1",
            "metadata": {},
            "source": [
                "import arcpy\n",
                "min_raster = '/data/MyDataBase/SWATGenXAppData/DEM/DEM_30m.tif'\n",
                "\n",
                "arcpy.env.overwriteOutput=True\n",
                "# Loop through each resolution\n",
                "for RESOLUTION in [250]:\n",
                "    raster_and_new_name=[\n",
                "        #(fr\"/data/MyDataBase/SWATGenXAppData/Geomorphology/landforms_30m_250Dis.tif\", \"landforms_250m_250Dis.tif\"), \n",
                "         (fr\"/data/MyDataBase/SWATGenXAppData/Geomorphology/geomorphons_30m_250Dis.tif\", f\"geomorphons_250m_250Dis.tif\")]\n",
                " \n",
                "    # Loop through each raster and its new name\n",
                "    for raster_path, new_name in raster_and_new_name:\n",
                "        # Update environment settings for each raster\n",
                "        arcpy.env.extent = min_raster\n",
                "        arcpy.env.outputCoordinateSystem = min_raster\n",
                "        # Define output path\n",
                "        output_path = fr\"/data/MyDataBase/SWATGenXAppData/Geomorphology/{new_name}\"  # Replace with your actual output directory\n",
                "        # Perform the resampling\n",
                "        arcpy.Resample_management(\n",
                "            in_raster=raster_path,\n",
                "            out_raster=output_path,\n",
                "            cell_size=RESOLUTION,\n",
                "            resampling_type=\"MAJORITY\"\n",
                "        )\n",
                "        print(f\"Raster {raster_path} has been resampled to {RESOLUTION}m resolution.\")"
            ]
        },
        {
            "cell_type": "raw",
            "id": "b3ae2319-13e9-4f8d-82e2-9f611020fd9c",
            "metadata": {},
            "source": [
                "################################# THIS FUNCTION IS FOR CREATING OBSERVATION THAT THAT WE STORED IN THE MyDataBase/observation directory  ######################\n",
                "\n",
                "import geopandas as gpd\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import rasterio\n",
                "import os\n",
                "from pyproj import Transformer\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats\n",
                "\n",
                "def qq_density_plots(data, save_path=None):\n",
                "    \n",
                "        # Example variables\n",
                "    variables = [  'AQ_THK_1', 'AQ_THK_2',\n",
                "           'AQ_THK_D','H_COND_1', 'H_COND_2', 'H_COND_D', 'V_COND_1', 'V_COND_2', 'V_COND_D',\n",
                "           'TRANSMSV_1', 'TRANSMSV_2', 'TRANS_D','SWL','PMP_CPCITY', 'SPC_CPCITY']\n",
                "    n = len(variables)\n",
                "    fig, axes = plt.subplots(n // 3 + (n % 3 > 0), 3, figsize=(10, 2.5 * (n // 3 + (n % 3 > 0))))\n",
                "    axes = axes.flatten()\n",
                "    \n",
                "    for i, var in enumerate(variables):\n",
                "        ax = axes[i]\n",
                "        ax.set_facecolor('whitesmoke')\n",
                "        \n",
                "        # Setting x-limits based on quantiles\n",
                "        lower_limit = data[var].dropna().quantile(0.005)\n",
                "        upper_limit = data[var].dropna().quantile(0.995)\n",
                "        min_lim = data[var].dropna().min()\n",
                "        print(f'{var} stats\\n')\n",
                "        print('Min:',min_lim)\n",
                "        print('lower_limit:',lower_limit)\n",
                "        print('upper_limit:',upper_limit)\n",
                "        max_lim = data[var].dropna().max()\n",
                "        print('Max:',max_lim,'\\n')\n",
                "        # Plotting histogram and KDE\n",
                "        sns.histplot(data[var].dropna(), bins='fd', kde=True, color='tab:blue', ax=ax).set(xlim=(lower_limit, upper_limit))\n",
                "        \n",
                "        # Generating sample and theoretical quantiles\n",
                "        sorted_data = sorted(data[var].dropna())\n",
                "        theoretical_quantiles = stats.norm.ppf([(i+1)/(len(sorted_data)+1) for i in range(len(sorted_data))],\n",
                "                                               loc=np.mean(sorted_data), scale=np.std(sorted_data))\n",
                "        ax2 = ax.twinx()\n",
                "        ax2.scatter(theoretical_quantiles, sorted_data, c='r', marker='.', alpha=0.2)\n",
                "        ax2.set_ylabel('Sample Quantiles', color='tab:red')\n",
                "        \n",
                "        ymin1, ymax1 = ax.get_ylim()\n",
                "        ymin2, ymax2 = ax2.get_ylim()\n",
                "        ax.set_yticks(np.linspace(ymin1, ymax1, 4))\n",
                "        ax2.set_yticks(np.linspace(ymin2, ymax2, 4))\n",
                "        ax.set_xlabel(var)\n",
                "        ax.set_ylabel('Density', color='tab:blue')\n",
                "        ax.grid(alpha=0.3, linewidth=0.25)\n",
                "\n",
                "    # Remove extra subplots\n",
                "    for i in range(n, len(axes)):\n",
                "        fig.delaxes(axes[i])\n",
                "\n",
                "    plt.tight_layout()\n",
                "    \n",
                "  #  if save_path:\n",
                "  #      plt.savefig(save_path, dpi=600)\n",
                "        \n",
                "    plt.show()\n",
                "\n",
                "def filter_GW_data(GW_data):\n",
                "    initial_count = len(GW_data)\n",
                "    counts = {'Initial': initial_count}\n",
                "    # First criteria\n",
                "    GW_data = GW_data[GW_data.SWL < 999]\n",
                "    counts['After 1st Criteria (SWL<999)'] = len(GW_data)\n",
                "    # Second criteria\n",
                "    GW_data = GW_data[~GW_data.ELEV_DEM.isna()]\n",
                "    counts['After 2nd Criteria (No DEM value)'] = len(GW_data)\n",
                "    # Third criteria\n",
                "    GW_data = GW_data[(GW_data.SWL - GW_data.ELEV_DEM) < 0]\n",
                "    counts['After 3rd Criteria (SWL-ELEVATION<0)'] = len(GW_data)\n",
                "    # Fourth criteria\n",
                "    GW_data = GW_data[(GW_data.SWL - GW_data.WELL_DEPTH) < 0]\n",
                "    counts['After 4th Criteria (SWL-WELL DEPTH <0)'] = len(GW_data)\n",
                "    # Report the number of rows eliminated after each criteria\n",
                "    eliminated_counts = {key: initial_count - value for key, value in counts.items()}\n",
                "    \n",
                "    print(\"Initial number of well data: \", initial_count)\n",
                "    for key, value in eliminated_counts.items():\n",
                "        if key != 'Initial':\n",
                "            print(f\"Number of rows eliminated {key}: {value}\")\n",
                "\n",
                "    return GW_data.reset_index(drop=True)\n",
                "\n",
                "\n",
                "def getting_huc_numbers(dic, observations):\n",
                "    \n",
                "    NHDPlusID = gpd.GeoDataFrame(pd.read_pickle(os.path.join(dic, \"NHDPlusData/catchments.pk1\"))).to_crs('EPSG:26990')[['NHDPlusID', 'geometry']]\n",
                "    observations = observations.sjoin(NHDPlusID, how='left', predicate='within').drop(columns='index_right')\n",
                "    \n",
                "    WBDHU4 = gpd.read_file(os.path.join(dic, \"NHDPlusData/WBDHU4/WBDHU4_26990.shp\")).to_crs('EPSG:26990')[['huc4', 'geometry']]\n",
                "    WBDHU8 = gpd.read_file(os.path.join(dic, \"NHDPlusData/WBDHU8/WBDHU8_26990.shp\")).to_crs('EPSG:26990')[['huc8', 'geometry']]\n",
                "    WBDHU12 = gpd.read_file(os.path.join(dic, \"NHDPlusData/WBDHU12/WBDHU12_26990.shp\")).to_crs('EPSG:26990')[['huc12', 'geometry']]\n",
                "    \n",
                "    \n",
                "    observations = observations.sjoin(WBDHU12, how='left', predicate='within').drop(columns='index_right')\n",
                "    observations = observations.sjoin(WBDHU8, how='left', predicate='within').drop(columns='index_right')\n",
                "    observations = observations.sjoin(WBDHU4, how='left', predicate='within').drop(columns='index_right')\n",
                "    \n",
                "    observations = observations.dropna(subset=['huc12', 'huc8', 'huc4'])\n",
                "\n",
                "    \n",
                "    observations['huc8'] = observations['huc8'].astype('int64')\n",
                "    observations['huc4'] = observations['huc4'].astype('int64')\n",
                "    observations['huc12'] = observations['huc12'].astype('int64')\n",
                "                            \n",
                "    return observations\n",
                "\n",
                "print('start')\n",
                "dic=r\"/data/MyDataBase/SWATGenXAppData/\"\n",
                "# Load groundwater database\n",
                "observations = pd.read_pickle(os.path.join(dic,\"Well_data_krigging/WaterWells_Michigan_combined_projected_gr.pk1\"))\n",
                "observations = filter_GW_data (observations)\n",
                "observations = getting_huc_numbers(dic, observations)\n",
                "\n",
                "\n",
                "observations = observations.drop(columns=['OBJECTID','PERMIT_NUM','TOWN','RANGE', 'SECTION', 'OWNER_NAME', 'WELL_ADDR', 'WELL_CITY', 'WELL_ZIP',\n",
                "       'WELL_DEPTH', 'WELL_TYPE', 'TYPE_OTHER', 'WEL_STATUS', 'STATUS_OTH',\n",
                "       'WSSN', 'WELL_NUM', 'DRILLER_ID', 'DRILL_METH', 'METH_OTHER',\n",
                "       'CONST_DATE', 'CASE_TYPE', 'CASE_OTHER', 'CASE_DIA', 'CASE_DEPTH',\n",
                "       'SCREEN_FRM', 'SCREEN_TO', 'FLOWING', 'AQ_TYPE', 'TEST_DEPTH',\n",
                "       'TEST_HOURS', 'TEST_RATE', 'TEST_METHD', 'TEST_OTHER', 'GROUT',\n",
                "       'PMP_CPCITY', 'LATITUDE', 'LONGITUDE', 'METHD_COLL', 'ELEVATION',\n",
                "       'ELEV_METHD', 'WITHIN_CO', 'WITHIN_SEC', 'LOC_MATCH', 'SEC_DIST',\n",
                "       'ELEV_DEM', 'ELEV_DIF', 'LANDSYS', 'DEPTH_FLAG', 'ELEV_FLAG',\n",
                "       'SWL_FLAG', 'SPC_CPCITY', 'AQ_CODE', 'ROCK_TOP','NOTES','B_AQ_THK', 'B_H_COND', 'B_V_COND',\n",
                "       'B_TRANS', 'AQ_THICK_D', 'H_COND_D', 'V_COND_D', 'TRANS_D','WWAT_ID','AQ_FLAG', 'SCRN_FLAG', 'WELLCODE','AQ_THK_D','NHDPlusID','TOPAQ', 'BOTAQ'])\n",
                "\n",
                "observations[['SWL', 'AQ_THK_1', 'AQ_THK_2',\n",
                "       'H_COND_1', 'H_COND_2', 'V_COND_1', 'V_COND_2', 'TRANSMSV_1',\n",
                "       'TRANSMSV_2']]=observations[['SWL', 'AQ_THK_1', 'AQ_THK_2',\n",
                "       'H_COND_1', 'H_COND_2', 'V_COND_1', 'V_COND_2', 'TRANSMSV_1',\n",
                "       'TRANSMSV_2']].astype('float64')\n",
                "\n",
                "\n",
                "observations[['huc12', 'huc8', 'huc4']] = observations[['huc12', 'huc8', 'huc4']].astype('int64')\n",
                "\n",
                "observations.to_pickle(os.path.join(dic , 'observations/observations.pk1'))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "id": "347e2587-9444-4d35-9b7e-cc5e24a9245a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Raster Boundary/HUC8_250m.tif has been clipped and aligned.\n",
                        "Raster Boundary/HUC12_250m.tif has been clipped and aligned.\n",
                        "Raster Boundary/BaseRaster_250m.tif has been clipped and aligned.\n",
                        "Raster Boundary/COUNTY_250m.tif has been clipped and aligned.\n",
                        "Raster Geomorphology/Glacial_Landsystems_250m.tif has been clipped and aligned.\n",
                        "Raster Geomorphology/Aquifer_Characteristics_Of_Glacial_Drift_250m.tif has been clipped and aligned.\n",
                        "Raster Geomorphology/MI_geol_poly_250m.tif has been clipped and aligned.\n",
                        "Raster NHDPlusData\\NHDPlusID_250m.tif has been clipped and aligned.\n",
                        "Raster DEM\\DEM_250m.tif has been clipped and aligned.\n",
                        "Raster Geomorphology/landforms_250m_250Dis.tif has been clipped and aligned.\n",
                        "Raster Geomorphology/geomorphons_250m_250Dis.tif has been clipped and aligned.\n",
                        "Raster LandUse/landuse_250m.tif has been clipped and aligned.\n",
                        "Raster Soil/Soil_STATSGO_250m.tif has been clipped and aligned.\n",
                        "Raster Soil/gSSURGO_swat_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_TRANSMSV_1_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_TRANSMSV_2_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_AQ_THK_1_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_AQ_THK_2_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_H_COND_1_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_H_COND_2_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_V_COND_1_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_V_COND_2_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_output_SWL_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_SWL_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_AQ_THK_1_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_AQ_THK_2_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_H_COND_1_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_H_COND_2_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_V_COND_1_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_V_COND_2_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_TRANSMSV_1_250m.tif has been clipped and aligned.\n",
                        "Raster Krigging_results/kriging_stderr_TRANSMSV_2_250m.tif has been clipped and aligned.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import rasterio\n",
                "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
                "import numpy as np\n",
                "import geopandas as gpd\n",
                "from shapely.geometry import Point\n",
                "\n",
                "RESOLUTION = 250\n",
                "\n",
                "raster_paths = [\n",
                "    (fr\"Boundary/HUC8_{RESOLUTION}m.tif\", f\"huc8\",'int64'),\n",
                "    (fr\"Boundary/HUC12_{RESOLUTION}m.tif\", f\"huc12\",'int64'),\n",
                "    (fr\"Boundary/BaseRaster_{RESOLUTION}m.tif\", f\"Domain\",'int32'),\n",
                "    (fr\"Boundary/COUNTY_{RESOLUTION}m.tif\", f\"COUNTY\",'int32'), \n",
                "    (fr\"Geomorphology/Glacial_Landsystems_{RESOLUTION}m.tif\", f\"GeoLandSy\",'int32'), \n",
                "    (fr'Geomorphology/Aquifer_Characteristics_Of_Glacial_Drift_{RESOLUTION}m.tif',f'AQU_CHAR','int32'),\n",
                "    (fr\"Geomorphology/MI_geol_poly_{RESOLUTION}m.tif\", f\"GeologUnit\",'int32'),\n",
                "    (fr\"NHDPlusData\\NHDPlusID_{RESOLUTION}m.tif\", f\"NHDPlusID\", 'int64'), \n",
                "    (fr\"DEM\\DEM_{RESOLUTION}m.tif\", f\"Elevation\", \"float32\"), ### CONSIDE THAT THIS IS THE BASE MODEL FOR INDEXING\n",
                "    (fr\"Geomorphology/landforms_{RESOLUTION}m_250Dis.tif\", f\"landforms\",'int32'),\n",
                "    (fr\"Geomorphology/geomorphons_{RESOLUTION}m_250Dis.tif\", f\"geomorph\",'int64'),\n",
                "    (fr\"LandUse/landuse_{RESOLUTION}m.tif\", f\"landuse\",'int32'),\n",
                "    (fr\"Soil/Soil_STATSGO_{RESOLUTION}m.tif\", f\"STATSGO\",\"int64\"),\n",
                "    (fr\"Soil/gSSURGO_swat_{RESOLUTION}m.tif\", f\"gSSURGO\",\"int64\"),\n",
                "   (fr\"Krigging_results/kriging_output_TRANSMSV_1_{RESOLUTION}m.tif\",   f\"S_TRSMV_1\",  \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_output_TRANSMSV_2_{RESOLUTION}m.tif\",   f\"S_TRSMV_2\",  \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_output_AQ_THK_1_{RESOLUTION}m.tif\",   f\"S_AQTHK_1\",  \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_output_AQ_THK_2_{RESOLUTION}m.tif\",   f\"S_AQTHK_2\",  \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_output_H_COND_1_{RESOLUTION}m.tif\",   f\"S_HCOND_1\",  \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_output_H_COND_2_{RESOLUTION}m.tif\",   f\"S_HCOND_2\",  \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_output_V_COND_1_{RESOLUTION}m.tif\",   f\"S_VCOND_1\",  \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_output_V_COND_2_{RESOLUTION}m.tif\",   f\"S_VCOND_2\",  \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_output_SWL_{RESOLUTION}m.tif\",   f\"S_SWL\",      \"float64\"),\n",
                "   (fr\"Krigging_results/kriging_stderr_SWL_{RESOLUTION}m.tif\",   f\"er_SWL\",     \"float64\"),\n",
                "    (fr\"Krigging_results/kriging_stderr_AQ_THK_1_{RESOLUTION}m.tif\",   f\"er_AQTHK_1\", \"float64\"),\n",
                "    (fr\"Krigging_results/kriging_stderr_AQ_THK_2_{RESOLUTION}m.tif\",   f\"er_AQTHK_2\", \"float64\"),\n",
                "    (fr\"Krigging_results/kriging_stderr_H_COND_1_{RESOLUTION}m.tif\",   f\"er_HCOND_1\", \"float64\"),\n",
                "    (fr\"Krigging_results/kriging_stderr_H_COND_2_{RESOLUTION}m.tif\",   f\"er_HCOND_2\", \"float64\"),\n",
                "    (fr\"Krigging_results/kriging_stderr_V_COND_1_{RESOLUTION}m.tif\",   f\"er_VCOND_1\", \"float64\"),\n",
                "    (fr\"Krigging_results/kriging_stderr_V_COND_2_{RESOLUTION}m.tif\",   f\"er_VCOND_2\", \"float64\"),\n",
                "  (fr\"Krigging_results/kriging_stderr_TRANSMSV_1_{RESOLUTION}m.tif\",   f\"er_TRSMV_1\", \"float64\"),\n",
                "  (fr\"Krigging_results/kriging_stderr_TRANSMSV_2_{RESOLUTION}m.tif\",   f\"er_TRSMV_2\", \"float64\") \n",
                "]\n",
                "\n",
                "\n",
                "dic = '/data/MyDataBase/SWATGenXAppData/'\n",
                "output_dir = os.path.join(dic, \"all_rasters\")\n",
                "\n",
                "# Ensure temp directory exists\n",
                "if not os.path.exists(output_dir):\n",
                "    os.makedirs(output_dir)\n",
                "\n",
                "reference_raster = os.path.join(dic, f\"DEM/DEM_{RESOLUTION}m.tif\")\n",
                "# Read properties from the reference (minimum) raster\n",
                "with rasterio.open(reference_raster) as src:\n",
                "    min_transform = src.transform\n",
                "    min_crs = src.crs\n",
                "    min_height = src.height\n",
                "    min_width = src.width\n",
                "\n",
                "output_paths = []\n",
                "col_names = []\n",
                "cols_dtype = []\n",
                "\n",
                "# Loop through each raster to clip and align\n",
                "for raster_path, col_name, dtype in raster_paths:\n",
                "    full_raster_path = os.path.join(dic, raster_path)\n",
                "    \n",
                "    with rasterio.open(full_raster_path) as src:\n",
                "        # Set the parameters for reprojection to match reference_raster\n",
                "        transform, width, height = calculate_default_transform(\n",
                "            src.crs, min_crs, src.width, src.height, *src.bounds)\n",
                "        \n",
                "        # Initialize destination raster specs\n",
                "        kwargs = src.meta.copy()\n",
                "        kwargs.update({\n",
                "            'crs': min_crs,\n",
                "            'transform': min_transform,\n",
                "            'width': min_width,\n",
                "            'height': min_height\n",
                "        })\n",
                "        # Output path for the clipped and aligned raster\n",
                "        output_path = os.path.join(output_dir, os.path.basename(raster_path))\n",
                "        output_paths.append(output_path)\n",
                "        col_names.append(col_name)\n",
                "        cols_dtype.append(dtype)\n",
                "        # Perform the reprojection (clip and align)\n",
                "        with rasterio.open(output_path, 'w', **kwargs) as dst:\n",
                "            for i in range(1, src.count + 1):\n",
                "                \n",
                "                reproject(\n",
                "                    source=rasterio.band(src, i),\n",
                "                    destination=rasterio.band(dst, i),\n",
                "                    src_transform=src.transform,\n",
                "                    src_crs=src.crs,\n",
                "                    dst_transform=min_transform,\n",
                "                    dst_crs=min_crs,\n",
                "                    resampling=Resampling.nearest\n",
                "                )\n",
                "\n",
                "    print(f\"Raster {raster_path} has been clipped and aligned.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "id": "b6c572ba-166c-48d5-8362-fbf36254c9e8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\HUC8_250m.tif\n",
                        "number of rows:1849 number of cols: 1458\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\HUC12_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\BaseRaster_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\COUNTY_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\Glacial_Landsystems_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\Aquifer_Characteristics_Of_Glacial_Drift_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\MI_geol_poly_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\NHDPlusID_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\DEM_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\landforms_250m_250Dis.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\geomorphons_250m_250Dis.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\landuse_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\Soil_STATSGO_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\gSSURGO_swat_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_TRANSMSV_1_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_TRANSMSV_2_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_AQ_THK_1_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_AQ_THK_2_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_H_COND_1_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_H_COND_2_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_V_COND_1_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_V_COND_2_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_output_SWL_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_SWL_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_AQ_THK_1_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_AQ_THK_2_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_H_COND_1_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_H_COND_2_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_V_COND_1_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_V_COND_2_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_TRANSMSV_1_250m.tif\n",
                        "/data/MyDataBase/SWATGenXAppData/all_rasters\\kriging_stderr_TRANSMSV_2_250m.tif\n",
                        "number of unique NHDPlusIDs: 77789\n"
                    ]
                }
            ],
            "source": [
                "# Initialize an empty list to store the DataFrames\n",
                "\n",
                "dfs = []\n",
                "# Initialize geometry and coordinate columns to None (they will be populated later)\n",
                "geometry_col = None\n",
                "x_coords_transformed = None\n",
                "y_coords_transformed = None\n",
                "\n",
                "for i, output_path in enumerate(output_paths):\n",
                "    print(output_path)\n",
                "    with rasterio.open(output_path) as src:\n",
                "        band1 = src.read(1)  # Read band 1\n",
                "    flattened_array = band1.flatten()\n",
                "    df = pd.DataFrame({col_names[i]: flattened_array})\n",
                "    if geometry_col is None:\n",
                "        rows, cols = band1.shape\n",
                "        print(f'number of rows:{rows}', f\"number of cols: {cols}\")\n",
                "        # Use np.indices to generate row and column indices\n",
                "        row_inds, col_inds = np.indices((rows, cols))\n",
                "        # Use the affine transformation to convert pixel indices to geographic coordinates\n",
                "        x_coords_transformed = []\n",
                "        y_coords_transformed = []\n",
                "        for r, c in zip(row_inds.ravel(), col_inds.ravel()):\n",
                "            x, y = src.transform * (c, r)\n",
                "            x_coords_transformed.append(x)\n",
                "            y_coords_transformed.append(y)\n",
                "        # Create a list of Point geometries from the x and y coordinates\n",
                "        geometry_col = [Point(x, y) for x, y in zip(x_coords_transformed, y_coords_transformed)]\n",
                "\n",
                "    # Add geometry and coordinate columns to the DataFrame\n",
                "    df['x_coord'] = x_coords_transformed\n",
                "    df['y_coord'] = y_coords_transformed\n",
                "    df['geometry'] = geometry_col\n",
                "    # Convert DataFrame to GeoDataFrame\n",
                "    gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=src.crs)\n",
                "    dfs.append(gdf.drop(columns=['x_coord', 'y_coord', 'geometry']))\n",
                "\n",
                "centroid_data_df = pd.concat(dfs, axis=1)\n",
                "\n",
                "centroid_df = gpd.GeoDataFrame(centroid_data_df, geometry=geometry_col, crs=src.crs)\n",
                "centroid_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "centroid_df = centroid_df[centroid_df.NHDPlusID>0]\n",
                "centroid_df = centroid_df[centroid_df.Domain==1]\n",
                "centroid_df = centroid_df[centroid_df.gSSURGO>0] \n",
                "centroid_df = centroid_df[centroid_df.STATSGO>0]\n",
                "centroid_df = centroid_df[centroid_df.AQU_CHAR>0]\n",
                "centroid_df = centroid_df[centroid_df.GeoLandSy>0]\n",
                "centroid_df = centroid_df[centroid_df.S_AQTHK_1>0]\n",
                "print('number of unique NHDPlusIDs:',centroid_df.NHDPlusID.unique().shape[0])\n",
                "county=gpd.read_file(r\"D:\\MyDataBase\\NHDPlusData\\Counties_(v17a)\\Counties_(v17a).shp\")\n",
                "county.rename(columns={'COUNTY':'COUNTY_shape'}, inplace=True)\n",
                "centroid_df = centroid_df.merge(county[['OBJECTID','COUNTY_shape']], left_on='COUNTY', right_on='OBJECTID')\n",
                "centroid_df.drop(columns=['OBJECTID'], inplace=True)\n",
                "centroid_df.drop(columns=['COUNTY'], inplace=True)\n",
                "centroid_df.rename(columns={'COUNTY_shape':'COUNTY'}, inplace=True)\n",
                "centroid_df = centroid_df.dropna(subset=['NHDPlusID','GeoLandSy','AQU_CHAR','GeologUnit','STATSGO','gSSURGO'])\n",
                "centroid_df[['NHDPlusID','STATSGO','gSSURGO']] = centroid_df[['NHDPlusID','STATSGO','gSSURGO']].astype('int64')\n",
                "centroid_df[['huc12','huc8']] = centroid_df[['huc12','huc8']].astype('int64')\n",
                "\n",
                "centroid_df[['geomorph']] = centroid_df[['geomorph']].astype('int32')\n",
                "centroid_df[['GeoLandSy','AQU_CHAR','GeologUnit','landuse']]=centroid_df[['GeoLandSy','AQU_CHAR','GeologUnit','landuse']].astype('int16')\n",
                "cols_to_modify = ['Elevation', 'S_TRSMV_1', 'S_TRSMV_2', 'S_AQTHK_1', 'S_AQTHK_2', 'S_HCOND_1',\n",
                "                  'S_HCOND_2', 'S_VCOND_1', 'S_SWL', 'er_SWL', 'er_AQTHK_1', 'er_AQTHK_2',\n",
                "                  'er_HCOND_1', 'er_HCOND_2', 'er_VCOND_1', 'er_VCOND_2', 'er_TRSMV_1', 'er_TRSMV_2']\n",
                "\n",
                "for col in cols_to_modify:\n",
                "    centroid_df[col] = centroid_df[col].astype(float).round(2)\n",
                "# Save DataFrame to pickle\n",
                "centroid_df.to_pickle(os.path.join(dic, 'observations', f'rasters_{RESOLUTION}m.pk1'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "id": "d2d1ead7-1d2d-4c81-9395-f1688d7468b3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[40601051136 40700030976           0 40700039168 40700051456 40700059648\n",
                        " 40601038848 40601030656 40700071936 40601018368 40801009664 40601010176\n",
                        " 40802009088 40601022464 40802021376 40801021952 40801030144 40801038336\n",
                        " 40802050048 40900009984 40802058240 40802029568 40500060160 40802041856\n",
                        " 40500051968 40900030464 40500072448 40500068352 40500031488 40500019200\n",
                        " 40900022272 40500039680 41000128512 40900038656 41000009728 41000022016\n",
                        " 40500011008 41000058880 41000030208]\n"
                    ]
                }
            ],
            "source": [
                "print(centroid_df.huc12.unique())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "id": "09f7c4b6-0bc8-4994-8cdb-cdf43cfa7b0b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The raster file has 42 unique values.\n"
                    ]
                }
            ],
            "source": [
                "import rasterio\n",
                "import numpy as np\n",
                "\n",
                "# Open the raster file\n",
                "with rasterio.open('D:\\\\MyDataBase\\\\Boundary\\\\HUC12_250m.tif') as src:\n",
                "    # Read the raster into a numpy array\n",
                "    data = src.read(1)\n",
                "\n",
                "    # Count unique values, excluding NaN values\n",
                "    unique_values = np.unique(data[~np.isnan(data)].astype('int64'))\n",
                "\n",
                "# Print the unique values and their count\n",
                "print(f\"The raster file has {len(unique_values)} unique values.\")\n",
                "\n",
                "# If you want to see the unique values themselves:\n",
                "# print(unique_values)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "id": "18d364d0-258f-408a-952e-742c1e6ad382",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0       40500010803\n",
                            "1       40500011201\n",
                            "2       40500011605\n",
                            "3       40500011701\n",
                            "4       40500011702\n",
                            "           ...     \n",
                            "1755    41000120304\n",
                            "1756    41000120405\n",
                            "1757    41000120503\n",
                            "1758    41000030403\n",
                            "1759    41000030601\n",
                            "Name: huc12, Length: 1760, dtype: int64"
                        ]
                    },
                    "execution_count": 68,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test.huc12"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7fee26cd-93ed-4791-a5a4-43d4355a9737",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the pickled DataFrames\n",
                "import pandas as pd\n",
                "import geopandas as gpd\n",
                "import numpy as np\n",
                "\n",
                "centroid_df = pd.read_pickle(dic + f\"observations/rasters_{RESOLUTION}m.pk1\")\n",
                "centroid_df = gpd.GeoDataFrame(centroid_df,geometry='geometry')\n",
                "centroid_df = centroid_df[centroid_df.huc8!=0].sort_values('huc12').reset_index(drop=True)\n",
                "centroid_df['OBJECTID'] = np.arange(1, len(centroid_df)+1)\n",
                "observation_df = pd.read_pickle(os.path.join(dic , 'observations/observations.pk1'))\n",
                "observation_df = gpd.GeoDataFrame(observation_df,geometry='geometry')\n",
                "observation_df.drop(columns=['COUNTY','huc12','huc8','huc4'], inplace=True)\n",
                "# Step 1: Buffer the points to create small polygons\n",
                "centroid_df['geometry'] = centroid_df['geometry'].buffer(int(RESOLUTION/2))\n",
                "# Step 2: Create bounding rectangles\n",
                "centroid_df['geometry'] = centroid_df['geometry'].envelope\n",
                "# Now, you can use spatial join to find the points within the rectangles\n",
                "grids_observations = gpd.sjoin(observation_df, centroid_df, how=\"right\", predicate=\"within\")\n",
                "grids_observations['x'] = grids_observations.geometry.centroid.x\n",
                "grids_observations['y'] = grids_observations.geometry.centroid.y\n",
                "grids_observations['geometry'] = grids_observations.geometry.centroid\n",
                "grids_observations.to_pickle(os.path.join(dic,'observations',f'rasters_{RESOLUTION}m_with_observations.pk1'))\n",
                "\n",
                "print('writing observations.....')\n",
                "#grids_observations[['OBJECTID','geometry']].to_file(os.path.join(dic,'observations',f'rasters_{RESOLUTION}m_with_observations'))\n",
                "grids_observations.drop(columns='geometry').to_pickle(os.path.join(dic,'observations',f'rasters_{RESOLUTION}m_with_observations_without_geometry.pk1'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c3d040e5-7b4e-4960-ad97-0d99fbd806e4",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\catchments.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\Flowlines.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDFlowline.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDPlusCatchment.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDPlusEROMMA.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDPlusFlowlineVAA.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDPlusIncrPrecip.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDPlusIncrPrecipMA.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDPlusIncrTemp.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDPlusIncrTempMA.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\NHDWaterbody.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\streams.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\subbasins.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\waterbodies.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\watersheds.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\WBDHU12.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\WBDHU12_streams.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\WBDHU12_watersheds.pk1',\n",
                            " '/data/MyDataBase/SWATGenXAppData/NHDPlusData\\\\WBDHU8.pk1']"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "glob.glob(os.path.join(dic,\"NHDPlusData\" ,\"*.pk1\" ))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "id": "a15b734b-97be-45b1-9435-03e7ad719d30",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import geopandas as gpd\n",
                "import numpy as np\n",
                "import os\n",
                "import glob\n",
                "dic=\"/data/MyDataBase/SWATGenXAppData/\"\n",
                "RESOLUTION = 250\n",
                "NHDPlusEROMMA=pd.read_pickle(os.path.join(dic,\"NHDPlusData\" ,\"NHDPlusEROMMA.pk1\" ))[['NHDPlusID','QAMA']]  #QAMA: mean annual flow, cubic feet per second \n",
                "#NHDPlusIncrTempMA=pd.read_pickle(os.path.join(dic,\"NHDPlusData\" ,\"NHDPlusIncrTempMA.pk1\" ))[['NHDPlusID','TempMA']]  #QAMA: mean annual flow, cubic feet per second \n",
                "#NHDPlusIncrPrecipMA=pd.read_pickle(os.path.join(dic,\"NHDPlusData\" ,\"NHDPlusIncrPrecipMA.pk1\" ))[['NHDPlusID','PrecipMA']]  #QAMA: mean annual flow, cubic feet per second \n",
                "NHDPlusFlowlineVAA=pd.read_pickle(os.path.join(dic,\"NHDPlusData\" ,\"NHDPlusFlowlineVAA.pk1\" ))[['NHDPlusID', 'StreamLeve', 'StreamOrde','SlopeLenKm','AreaSqKm','MaxElevSmo', 'MinElevSmo', 'Slope']]\n",
                "grids_observations=pd.read_pickle(os.path.join(dic,'observations',f'rasters_{RESOLUTION}m_with_observations_without_geometry.pk1')).drop(columns='index_left')\n",
                "grids_observations = grids_observations.merge(NHDPlusEROMMA, on=\"NHDPlusID\")\n",
                "#grids_observations = grids_observations.merge(NHDPlusIncrTempMA, on=\"NHDPlusID\")\n",
                "#grids_observations = grids_observations.merge(NHDPlusIncrPrecipMA, on=\"NHDPlusID\")\n",
                "grids_observations = grids_observations.merge(NHDPlusFlowlineVAA, on=\"NHDPlusID\")\n",
                "grids_observations.to_pickle((os.path.join(dic,'observations',f'rasters_{RESOLUTION}m_with_observations_without_geometry_with_NHDPlusData.pk1')))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43130d3c-5a72-46a0-9afa-ff4049e21750",
            "metadata": {},
            "outputs": [],
            "source": [
                "### Test of geometry transform correcness. \n",
                "import pandas as pd\n",
                "import os\n",
                "RESOLUTION = 250\n",
                "dic=r\"/data/MyDataBase/SWATGenXAppData/\"\n",
                "df=pd.read_pickle(os.path.join(dic,'observations',f'observations.pk1'))  \n",
                "df[df.COUNTY=='Barry'].TRANSMSV_1.dropna()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}