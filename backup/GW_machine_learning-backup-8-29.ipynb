{
    "cells": [
        {
            "cell_type": "raw",
            "id": "1819828f-6d70-43d2-8f9b-7c4563a60f57",
            "metadata": {},
            "source": [
                "#models = [\n",
                "#    (LinearRegression(), {'scaler': [StandardScaler(), MinMaxScaler(), \n",
                "#                                                        MaxAbsScaler(), RobustScaler(),\n",
                "#                                     PowerTransformer()]}, \"linear_regression\",\"Random\"),\n",
                "\n",
                "#    (Ridge(), {'model__alpha': np.logspace(-6, 6, 13),'scaler': [StandardScaler(), MinMaxScaler(), \n",
                "#                                                         RobustScaler(),\n",
                "#                                    PowerTransformer()]},\"ridge_regression\",\"Random\"),\n",
                "\n",
                "#    (Lasso(max_iter=10000), {'model__alpha': np.logspace(-6, 6, 13), 'scaler': [StandardScaler(), MinMaxScaler(), \n",
                "#                                                         RobustScaler(),\n",
                "#                                    PowerTransformer()]},\"lasso_regression\",\"Random\"),\n",
                "\n",
                "#    (ElasticNet(max_iter=10000), {'model__alpha': np.logspace(-6, 6, 13), \n",
                "#                                    'scaler': [StandardScaler(), MinMaxScaler(),  RobustScaler(),\n",
                "#                                    PowerTransformer()],\n",
                "#                                  'model__l1_ratio': np.linspace(0.1, 0.9, 9)}, \"elastic_net_regression\",\"Random\"),\n",
                "\n",
                "#    (DecisionTreeRegressor(random_state=42), {'model__max_depth': [None, 2, 7, 17], \n",
                "#                                      'model__min_samples_split': [2, 5, 10], \n",
                "#                                      'model__min_samples_leaf': [1, 2, 4], \n",
                "#                                       'scaler': [StandardScaler(), MinMaxScaler(),  RobustScaler(),\n",
                "#                                        PowerTransformer()],\n",
                "#                                      'model__max_features': ['sqrt']}, \"decision_tree\",\"Random\"),\n",
                "\n",
                "#    (BaggingRegressor(base_estimator=DecisionTreeRegressor(random_state=42),\n",
                "#                  random_state=42), {'model__n_estimators': [10, 50, 100],\n",
                "#                                     'model__max_samples': [0.5, 1.0],\n",
                "#                                      'scaler': [StandardScaler(), MinMaxScaler(),  RobustScaler(),\n",
                "#                                      PowerTransformer()],\n",
                "#                                     'model__max_features': [0.5, 1.0]}, \"bagging_decision_tree\",\"Random\"),\n",
                "\n",
                "#    (BaggingRegressor(base_estimator=Ridge(), random_state=42), {'model__n_estimators': [10, 50, 100],\n",
                "#                                                              'model__max_samples': [0.5, 1.0],\n",
                "#                                                              'model__max_features': [0.5, 1.0],\n",
                "#                                                                'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "#                                                               PowerTransformer()],\n",
                "#                                                              'model__base_estimator__alpha': np.logspace(-6, 6, 13)}, \"bagging_ridge\",\"Random\"),\n",
                "\n",
                "#    (BaggingRegressor(base_estimator=KNeighborsRegressor(), random_state=42), {'model__n_estimators': [10, 50],\n",
                "#                                                                            'model__max_samples': [0.5, 1.0],\n",
                "#                                                                            'model__base_estimator__n_neighbors': range(1, 6),\n",
                "#                                                                            'model__base_estimator__weights': ['uniform'],\n",
                "#                                                                          'scaler': [StandardScaler(), MinMaxScaler(),  RobustScaler()],\n",
                "#                                                                            'model__base_estimator__p': [1, 2]}, \"bagging_KNeighborsReg\",\"Random\"),\n",
                "\n",
                "#    (KNeighborsRegressor(), {'model__n_neighbors': range(1, 11), \n",
                "#                     'model__weights': ['uniform', 'distance'], \n",
                "#                     'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "#                    PowerTransformer()],\n",
                "#                     'model__p': [1, 2]}, \"k_nearest_neighbors\",\"Random\"),\n",
                "\n",
                "#    (ExtraTreesRegressor(random_state=42), {'model__n_estimators': [50, 100], \n",
                "#                                    'model__max_depth': [None, 2, 7, 17], \n",
                "#                                    'model__min_samples_split': [2, 5, 10], \n",
                "#                                    'model__min_samples_leaf': [1, 2, 4], \n",
                "#                                  'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "#                                     PowerTransformer()],\n",
                "#                                    'model__max_features': ['sqrt'],\n",
                "#                                    'model__bootstrap': [True]}, \"extra_trees\",\"Random\"),\n",
                "\n",
                "#    (XGBRegressor(random_state=42), {'model__n_estimators': [100, 200], \n",
                "#                              'model__max_depth': [3, 6, 9], \n",
                "#                              'model__min_child_weight': [1, 2, 4], \n",
                "#                              'model__gamma': [0, 0.1, 0.2], \n",
                "#                               'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "#                               PowerTransformer()],\n",
                "#                              'model__subsample': [0.8, 1], \n",
                "#                              'model__colsample_bytree': [0.8, 1], \n",
                "#                              'model__learning_rate': [0.01, 0.1]}, \"xgboost\",\"Random\"),\n",
                "\n",
                "#    (GradientBoostingRegressor(random_state=42),\n",
                "#                             {'model__n_estimators': [50, 100, 200],\n",
                "#                              'model__max_depth': [3, 4, 5, 6, 8],\n",
                "#                              'model__min_samples_split': [2, 5, 10],\n",
                "#                              'model__min_samples_leaf': [1, 2, 4],\n",
                "#                              'model__learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
                "#                              'model__subsample': [0.5, 0.8, 1.0],\n",
                "#                              'model__max_features': ['auto', 'sqrt', 'log2']},\n",
                "#                             \"gradient_boosting\",\"Random\"),\n",
                "\n",
                "#(GradientBoostingRegressor(random_state=42), {'model__n_estimators': [50, 100, 200], \n",
                "#                                          'model__max_depth': [3, 6], \n",
                "#                                          'model__learning_rate': [0.01, 0.1]}, \"gradient_boosting\",\"Random\"),\n",
                "\n",
                "#    (RandomForestRegressor(random_state=42), {'model__n_estimators': [50, 100, 200], \n",
                "#                                              'model__max_depth': [None, 2, 5], \n",
                "#                                              'model__min_samples_split': [4, 8],\n",
                "#                               'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "#                               PowerTransformer()],\n",
                "#                                              'model__min_samples_leaf': [1, 2], \n",
                "#                                              'model__max_features': ['sqrt'],\n",
                "#                                              'model__bootstrap': [True]}, \"random_forest\",\"Random\"),\n",
                "    \n",
                "#(BaggingRegressor(base_estimator=RandomForestRegressor(random_state=42),\n",
                "#                  random_state=42), {'model__n_estimators': [10, 50, 100],\n",
                "#                                     'model__max_samples': [0.5, 1.0],\n",
                "#                                     'model__base_estimator__n_estimators': [50, 100, 200],\n",
                "#                                     'model__base_estimator__max_depth': [None, 2, 10, 15],\n",
                "#                                     'model__base_estimator__min_samples_split': [4, 10, 15],\n",
                "#                                     'model__base_estimator__min_samples_leaf': [1, 2, 4],\n",
                "#                                     'model__base_estimator__max_features': ['sqrt'],\n",
                "#                                     'model__base_estimator__bootstrap': [True]}, \"bagging_random_forest\",\"Random\")    \n",
                " #   \n",
                " \n",
                " \n",
                "            (XGBRegressor(random_state=42, tree_method='gpu_hist'), {\n",
                "        'model__n_estimators': [50, 100, 200],\n",
                "        'model__max_depth': [None, 2, 5],\n",
                "        'model__min_child_weight': [1, 2],\n",
                "        'model__colsample_bynode': [0.5],\n",
                "                'model__grow_policy':['depthwise', 'lossguide'],\n",
                "                'model__max_leaves':[0,5,10],\n",
                "        'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(), PowerTransformer()],\n",
                "        }, \"random_forest_gpu\",\"Random\"),\n",
                "    \n",
                "\n",
                "    (XGBRegressor(random_state=42, tree_method='gpu_hist'),\n",
                "     {'model__n_estimators': optuna.distributions.IntDistribution(50, 200, step=50),\n",
                "      'model__max_depth': optuna.distributions.IntDistribution(2, 10),\n",
                "      'model__min_child_weight': optuna.distributions.IntDistribution(4,8),\n",
                "      'model__colsample_bytree': optuna.distributions.CategoricalDistribution([0.5])},\n",
                "     \"gradient_boosting_gpu\", 'Optuna'),\n",
                "    \n",
                " #   (XGBRFRegressor(random_state=42, tree_method='gpu_hist'), \n",
                "     \n",
                " #    {'model__n_estimators': optuna.distributions.IntDistribution(50, 500, step=50),\n",
                " #     'model__max_depth': optuna.distributions.IntDistribution(2, 10),\n",
                " #     'model__min_child_weight': optuna.distributions.IntDistribution(1,10),\n",
                " #      'model__colsample_bynode': optuna.distributions.CategoricalDistribution([0.5])}, \n",
                " #    \"random_forest_gpu\", 'Optuna'),\n",
                " \n",
                " \n",
                "\n",
                "    \n",
                "    \n",
                "#        (GradientBoostingRegressor(random_state=42), {'model__n_estimators': [50, 100, 200], \n",
                "#                                          'model__max_depth': [3, 6], \n",
                "#                                          'model__learning_rate': [0.01, 0.1]}, \"gradient_boosting\",\"Random\"),\n",
                "    \n",
                "    \n",
                "#        (KNeighborsRegressor(), {'model__n_neighbors': range(1, 11), \n",
                "#                     'model__weights': ['uniform', 'distance'], \n",
                "#                     'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "#                    PowerTransformer()],\n",
                "#                     'model__p': [1, 2]}, \"k_nearest_neighbors\",\"Random\"),\n",
                "    \n",
                "    \n",
                "#            (XGBRegressor(random_state=42, tree_method='gpu_hist'), {\n",
                "#        'model__n_estimators': [50, 100, 200],\n",
                "#        'model__max_depth': [None, 2, 5],\n",
                "#        'model__min_child_weight': [1, 2],\n",
                "#        'model__colsample_bynode': [0.5],\n",
                "#                'model__grow_policy':['depthwise', 'lossguide'],\n",
                "#                'model__max_leaves':[0,5,10],\n",
                "#        'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(), PowerTransformer()],\n",
                "#        }, \"gradient_boosting_gpu\",\"Random\"),\n",
                "#    \n",
                "\n",
                "#    (XGBRegressor(random_state=42, tree_method='gpu_hist'),\n",
                "#     {\n",
                "#      'model__n_estimators': optuna.distributions.IntDistribution(50, 200, step=50),\n",
                "#      'model__max_depth': optuna.distributions.IntDistribution(2, 10),\n",
                "#      'model__min_child_weight': optuna.distributions.IntDistribution(4, 8),\n",
                "#      'model__colsample_bytree': optuna.distributions.CategoricalDistribution([0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
                "#      'model__subsample': optuna.distributions.FloatDistribution(0.5, 1.0),\n",
                "#      'model__learning_rate': optuna.distributions.FloatDistribution(0.01, 0.2), # Changed this line\n",
                "#      'model__gamma': optuna.distributions.FloatDistribution(0, 5),\n",
                "#      'model__reg_alpha': optuna.distributions.FloatDistribution(0, 1),\n",
                "#      'model__reg_lambda': optuna.distributions.FloatDistribution(0, 1),\n",
                "#      'model__scale_pos_weight': optuna.distributions.FloatDistribution(1, 10)\n",
                "#     },\n",
                "#     \"gradient_boosting_gpu\", 'Optuna'),\n",
                "\n",
                "    \n",
                "\n",
                "#    (RandomForestRegressor(random_state=42), {'model__n_estimators': [50, 100, 200, 250], \n",
                "#                                              'model__max_depth': [None, 2, 5, 10, 15, 20], \n",
                "#                                              'model__min_samples_split': [4, 8, 12],\n",
                "#                               'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "#                               PowerTransformer()],\n",
                "#                                              'model__min_samples_leaf': [1, 2,3], \n",
                "#                                              'model__max_features': ['sqrt'],\n",
                "#                                              'model__bootstrap': [True]}, \"random_forest\",\"Random\"),\n",
                "#       (SVR(kernel='rbf'), {\n",
                "#    'model__C': [0.1, 1, 10],\n",
                "#    'model__epsilon': [0.1, 0.2],\n",
                "#    'model__gamma': [0.1, 1, 10], # gamma parameter for the RBF kernel\n",
                "#    'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(), PowerTransformer()],\n",
                "#}, \"SVR\", 'Random'),\n",
                "    \n",
                " \n",
                "\n",
                "#(SVR(kernel='rbf'), {\n",
                "#    'model__C': optuna.distributions.FloatDistribution(0.1, 10, log=True),\n",
                "#    'model__epsilon': optuna.distributions.FloatDistribution(0.1, 0.2),\n",
                "#    'model__gamma': optuna.distributions.FloatDistribution(0.1, 10, log=True), # gamma parameter for the RBF kernel\n",
                " #   'scaler': optuna.distributions.CategoricalDistribution([StandardScaler(), MinMaxScaler(), RobustScaler(), PowerTransformer()]),\n",
                "#}, \"SVR\", 'Optuna'),\n",
                "\n",
                "\n",
                "  #  (RandomForestRegressor(random_state=42), {'model__n_estimators': [50, 100, 200], \n",
                "  #                                            'model__max_depth': [None, 2, 5], \n",
                "  #                                            'model__min_samples_split': [4, 8],\n",
                "  #                             'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "  #                             PowerTransformer()],\n",
                "  #                                            'model__min_samples_leaf': [1, 2], \n",
                "  #                                            'model__max_features': ['sqrt'],\n",
                "  #                                            'model__bootstrap': [True]}, \"random_forest\",\"Random\"),\n",
                "\n",
                "    \n",
                "    (RandomForestRegressor(random_state=42), {'model__n_estimators': [50, 100, 200, 250], \n",
                "                                              'model__max_depth': [None, 2, 5, 10, 15, 20], \n",
                "                                              'model__min_samples_split': [4, 8, 12],\n",
                "                               'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                "                               PowerTransformer()],\n",
                "                                              'model__min_samples_leaf': [1, 2,3], \n",
                "                                              'model__max_features': ['sqrt'],\n",
                "                                              'model__bootstrap': [True]}, \"random_forest\",\"Random\"),\n",
                "       (SVR(kernel='rbf'), {\n",
                "    'model__C': [0.1, 1, 10],\n",
                "    'model__epsilon': [0.1, 0.2],\n",
                "    'model__gamma': [0.1, 1, 10], # gamma parameter for the RBF kernel\n",
                "    'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(), PowerTransformer()],\n",
                "}, \"SVR\", 'Random'),\n",
                "    \n",
                "        (GradientBoostingRegressor(random_state=42), {'model__n_estimators': [50, 100, 200], \n",
                "                                          'model__max_depth': [3, 6], \n",
                "                                          'model__learning_rate': [0.01, 0.1]}, \"gradient_boosting\"),\n",
                "    \n",
                "#        (RandomForestRegressor(random_state=42), {\n",
                "#            'model__n_estimators': optuna.distributions.IntDistribution(50, 200),\n",
                "#            'model__max_depth': optuna.distributions.IntDistribution(2, 5), # None can be handled separately if needed\n",
                "#            'model__min_samples_split': optuna.distributions.IntDistribution(4, 8),\n",
                "#            'model__min_samples_leaf': optuna.distributions.IntDistribution(1, 2),\n",
                "#            'model__max_features': optuna.distributions.CategoricalDistribution(['sqrt']),\n",
                "#            'model__bootstrap': optuna.distributions.CategoricalDistribution([True]),\n",
                "#            'scaler': optuna.distributions.CategoricalDistribution([StandardScaler(), MinMaxScaler(), RobustScaler(), PowerTransformer()]),\n",
                "#        }, \"random_forest\", \"Optuna\")\n",
                "\n",
                "        \n",
                "#            (XGBRFRegressor(random_state=42, tree_method='gpu_hist'), \n",
                "#     \n",
                "#     {'model__n_estimators': [50, 100],\n",
                "#      'model__max_depth': [None, 2, 5, 10, 15],\n",
                "#      'model__min_child_weight': [1,5,10],\n",
                "#      'model__colsample_bynode': [0.5]}, \n",
                "#      \"random_forest_gpu\", 'Random'),\n",
                "    \n",
                "        ]\n",
                "        \n",
                "        \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9bfeb353-939f-402d-b30f-d374f4968dce",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c27f08fb-8b24-4394-aefb-61b5e78e8151",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "\n",
                "#   'Simulated_SWL', 'Simulated_Error_SWL',\n",
                "#       'Simulated_AQ_THK_2',  \n",
                "#       'Simulated_V_COND_1', 'Simulated_TRANSMSV_1', 'Simulated_TRANSMSV_2',\n",
                "#       'er_AQ_THK_2',  'er_H_COND_2','er_AQ_THK_1', 'Simulated_AQ_THK_1', \n",
                "#       'er_V_COND_1', 'er_V_COND_2', 'er_TRANSMSV_1', 'er_TRANSMSV_2']\n",
                "\n",
                "\n",
                "     #   'SWL', 'AQ_THK_2', 'AQ_THK_D',, 'H_COND_2', 'V_COND_1',\n",
                "     #  'V_COND_2', 'TRANSMSV_1', 'TRANSMSV_2', 'B_AQ_THK', 'B_H_COND',\n",
                "     #  'B_V_COND', 'B_TRANS', 'AQ_THICK_D', 'H_COND_D', 'V_COND_D', 'TRANS_D',#'H_COND_1',\n",
                "                 #  'AQ_THK_1',"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a1f79deb-f461-4f40-a615-a662a8f5ce2a",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "###### CLEAN UP PREVIOUS RUN (dont need to be changed)\n",
                "from ml_function import *\n",
                "remove_files('*.jpeg', '*.joblib', 'Permutation*.csv','*.h5')\n",
                "path_to_dataset = '/data/MyDataBase/SWATGenXAppData/observations/rasters_250m_with_observations_without_geometry.pk1'\n",
                "\n",
                "\n",
                "params = {'font.size': 15,\n",
                "          'axes.titlesize': 22,\n",
                "          'axes.labelsize': 22,\n",
                "          'xtick.labelsize': 18,\n",
                "          'ytick.labelsize': 18,\n",
                "          'legend.fontsize': 14,\n",
                "          'figure.titlesize': 18}\n",
                "\n",
                "formatted_results = pd.DataFrame(np.zeros((1000,13)), columns=['Model', 'Scenario', 'target', 'avg_nse', 'avg_rmse', \n",
                "                                                               'avg_nrmse', 'avg_r2', 'avg_willmott', 'Best_NSE',\n",
                "                                                               'Best_RMSE', 'Best_nRMSE', 'Best_R2', 'Best_Willmott'])\n",
                "\n",
                "target_variable = [ \n",
                "                    'TRANSMSV_1',\n",
                "                   ]\n",
                "\n",
                "##### one example of creating sceanrios.\n",
                "\n",
                "\n",
                "\n",
                "categorical_features = ['OBJECTID_AQU_CHAR', 'OBJECTID_LDSYSTEM', 'GeologicUnit_GENERALIZE']#, 'flowline', 'FCode', 'FType', 'WB','Soil','landforms' ,'landuse', 'StreamLeve', 'StreamOrde']  ## this is the categorical values that we are interested to incorporate in CNN\n",
                "num_features = ['Simulated_TRANSMSV_1','er_TRANSMSV_1']#, 'SlopeLenKm', 'Slope',  'MaxElevSmo', 'MinElevSmo']  #\n",
                "\n",
                "rbf_features=['Elevation']  #'x','y',\n",
                "########################################### set your scenarios name\n",
                "\n",
                "scenario_names = [\n",
                "            'TRANSMSV_1_optimization',\n",
                "                ]\n",
                "\n",
                "########################################### CREATE YOU OWN SCENARIOS, exclude parameters, specify categorical features if you need\n",
                "\n",
                "scenarios = {       \n",
                "    'TRANSMSV_1_optimization': {'num_features': num_features,\n",
                "                                'target_variable':target_variable,\n",
                "                              'rbf_features':rbf_features,\n",
                "        'exclude_parameter': [],\n",
                "        'categorical_features': categorical_features\n",
                "    \n",
                "    },  \n",
                "    }\n",
                "\n",
                "\n",
                "models= [\n",
                "    \n",
                " #    (RandomForestRegressor(random_state=42), {'model__n_estimators': [50, 100, 200], \n",
                " #                                             'model__max_depth': [None, 2, 5], \n",
                " #                                             'model__min_samples_split': [4, 8],\n",
                " #                              'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(),\n",
                " #                              PowerTransformer()],\n",
                " #                                             'model__min_samples_leaf': [1, 2], \n",
                " #                                             'model__max_features': ['sqrt'],\n",
                " #                                             'model__bootstrap': [True]}, \"random_forest\",\"Random\"),\n",
                "    \n",
                "        (XGBRegressor(random_state=42, tree_method='hist', booster='dart'),\n",
                "     {\n",
                "   #   'model__booster': optuna.distributions.CategoricalDistribution(['gbtree', 'gblinear', 'dart']),\n",
                "       'model__booster': optuna.distributions.CategoricalDistribution(['dart']),\n",
                "\n",
                "      'model__rate_drop': optuna.distributions.FloatDistribution(0, 0.2),  # DART-specific: rate_drop\n",
                "      'model__one_drop': optuna.distributions.CategoricalDistribution([0, 1]),  # DART-specific: one_drop\n",
                "         \n",
                "    # 'model__sample_type': optuna.distributions.CategoricalDistribution(['uniform', 'weighted']),  # DART-specific: sample_type\n",
                "      'model__sample_type': optuna.distributions.CategoricalDistribution(['weighted']),  # DART-specific: sample_type\n",
                "\n",
                "      'model__normalize_type': optuna.distributions.CategoricalDistribution(['forest']),  # DART-specific: normalize_type\n",
                "         \n",
                "      'model__n_estimators': optuna.distributions.IntDistribution(30, 50, step=10),\n",
                "      'model__max_depth': optuna.distributions.IntDistribution(2, 6),\n",
                "      'model__min_child_weight': optuna.distributions.IntDistribution(4, 8),\n",
                "         \n",
                "   #   'model__colsample_bytree': optuna.distributions.CategoricalDistribution([0.5, 0.6, 0.7]),\n",
                "      'model__subsample': optuna.distributions.FloatDistribution(0.6, 0.9),\n",
                "         \n",
                "      'model__learning_rate': optuna.distributions.FloatDistribution(0.01, 0.1),\n",
                "   #   'model__gamma': optuna.distributions.FloatDistribution(0, 2),\n",
                "         \n",
                "   #   'model__reg_alpha': optuna.distributions.FloatDistribution(0.1, 0.5),\n",
                "   #   'model__reg_lambda': optuna.distributions.FloatDistribution(0.1, 0.5),\n",
                "         \n",
                "  #    'model__scale_pos_weight': optuna.distributions.FloatDistribution(1, 5)\n",
                "     },\n",
                "     \"gradient_boosting_gpu\", 'Optuna') ,\n",
                "    \n",
                "    \n",
                "    \n",
                "    \n",
                "        (XGBRFRegressor(random_state=42, tree_method='hist'), \n",
                "     {'model__n_estimators': optuna.distributions.IntDistribution(50, 250, step=50),\n",
                "      'model__max_depth': optuna.distributions.IntDistribution(2, 10),\n",
                "      'model__min_child_weight': optuna.distributions.IntDistribution(1,10),\n",
                "       'model__colsample_bynode': optuna.distributions.CategoricalDistribution([0.5])}, \n",
                "     \"random_forest_gpu\", 'Optuna'),\n",
                "    \n",
                "\n",
                "]\n",
                "\n",
                "######## MAKING STACKED MODELS? if yes, Turn STACK_MODELS==True\n",
                "models = stacking_models (models, optimization='Optuna',STACK_MODELS=True)\n",
                "\n",
                "\n",
                "### costum scorer will be used in grid search (hyper parameter optimization) and permutation analysis\n",
                "### for outter  \n",
                "DDDD=0\n",
                "number_of_samples=500000\n",
                "County_specific=None\n",
                "# Nested cross-validation configuation\n",
                "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "print(\"Scenario names: \",scenario_names)\n",
                "\n",
                "for scenario_name in scenario_names:\n",
                "    \n",
                "    print(\" ############## Scenario name:\",scenario_name,' ############## ')\n",
                "        \n",
                "    num_features =scenarios[scenario_name]['num_features']\n",
                "    exclude_parameter=scenarios[scenario_name]['exclude_parameter']\n",
                "    target_variable=scenarios[scenario_name]['target_variable']\n",
                "    categorical_features=scenarios[scenario_name]['categorical_features']\n",
                "    rbf_features=scenarios[scenario_name]['rbf_features']\n",
                "    \n",
                "    print('num_features length:',len(num_features))\n",
                "    print('rbf_features length:', len(rbf_features))\n",
                "    print('categorical_features length:', len(categorical_features))\n",
                "    \n",
                "    print('Categorical features:',categorical_features)\n",
                "    \n",
                "    custome_scorer = make_scorer(kling_gupta_efficiency, greater_is_better=True)                                       ##### step1: define scorer for inner loop\n",
                "\n",
                "    for model, param_grid, model_name, optimization_method in models:\n",
                "        \n",
                "        print(' ############## Model being used for prediction:',model_name,' ############## ')\n",
                "        \n",
                "        input_dataset = import_dataset(path_to_dataset,number_of_samples, COUNTY=County_specific, sampling_group=['TOWNSHIP'],grid=False)                                                ##### import dataset, COUNTY: for working on one or several counties instead of entire data\n",
                "        print('number of Counties in the dataset:',len(input_dataset.COUNTY.unique()))\n",
                "\n",
                "        \n",
                "        print('############## target parameter for prediction:',target_variable,' ############## ')\n",
                "        \n",
                "        input_dataset = filter_by_quantiles(input_dataset, target_variable, lower_quantile=0, upper_quantile=1)           #### filter importer dataset based on quantile\n",
                "\n",
                "        input_dataset[categorical_features] = input_dataset[categorical_features].astype('category')                           #### define categorical feature data type\n",
                "        input_dataset[rbf_features] = input_dataset[rbf_features].astype(int)                             #### define categorical feature data type\n",
                "\n",
                "        output_variable = input_dataset.loc[:, target_variable[0]]                                                    #### define target feature\n",
                "\n",
                "        print('$$$$$ PIPLINE: Ensamble model $$$$$')\n",
                "        input_variables = input_dataset.loc[0:,num_features+categorical_features+rbf_features+['COUNTY']]                            ####  define input features (both categorical and numerical) \n",
                "        groups = input_variables['COUNTY'].values  # note: we have 68 counties\n",
                "        group_split = int(len(np.unique(list(groups)))/17)\n",
                "        group_kfold = GroupKFold(n_splits=group_split)\n",
                "        input_variables = input_variables.drop(columns='COUNTY')\n",
                "        outer_cv = GroupKFold(n_splits=group_split).split(input_variables, output_variable, groups)\n",
                "\n",
                "        print('numerical features:',num_features)                                                                 \n",
                "        print_statistics(output_variable,target_variable[0])                                                           #### print target feature stats\n",
                "\n",
                "        rbf_processor = RBFSampler(gamma=1, random_state=42)\n",
                "\n",
                "        preprocessor = ColumnTransformer(\n",
                "            transformers=[\n",
                "            ('num', 'passthrough', num_features),\n",
                "            ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),\n",
                "\n",
                "                ('rbf', rbf_processor, rbf_features)\n",
                "        ])\n",
                "\n",
                "        pipeline = Pipeline([ \n",
                "\n",
                "            ('preprocessor', preprocessor),\n",
                "            ('scaler', 'passthrough'),  # Placeholder for scaler\n",
                "            ('model', model)\n",
                "        ])\n",
                "\n",
                "        if optimization_method=='Random':\n",
                "            optimization_search = RandomizedSearchCV(pipeline, param_grid, scoring=custome_scorer, cv=inner_cv, n_jobs=60, verbose=0)                                    #### randomize search                                   \n",
                "        elif optimization_method=='Optuna':\n",
                "            optimization_search = OptunaSearchCV(estimator=pipeline,param_distributions=param_grid,scoring=custome_scorer,n_trials=25, cv=inner_cv ,n_jobs=60,verbose=0)   ##### Optuna Search\n",
                "        elif optimization_method=='Grid':\n",
                "            optimization_search = GridSearchCV(pipeline, param_grid, scoring=custome_scorer, cv=inner_cv, n_jobs=60, verbose=0)                                          ##### Grid Search\n",
                "\n",
                "\n",
                "        print(f\" ############## optimization method: {optimization_method} ############## \")\n",
                "        \n",
                "        best_score, best_model, nse_scores, r2_scores, rmse_scores, nrmse_scores, willmott_scores = initialize_metrics()                                      ## initialize\n",
                "        \n",
                "        avg_nse, avg_r2, avg_rmse, avg_nrmse, avg_willmott, best_model         = nested_cross_validation(outer_cv, input_variables, output_variable, optimization_search, nse_function, rmse_function, willmott_index, groups,group_kfold)   ## Perform nested cross validation \n",
                "       \n",
                "        best_y_pred, best_wi_score, best_nse, best_rmse, best_nrmse, best_r2   = test_best_model(model_name, scenario_name, target_variable[0], input_variables, output_variable, best_model, willmott_index)  ## save the best model found in nested cross validation and then load it and tested and get the performance.\n",
                "\n",
                "        plot_results(output_variable, target_variable[0], best_y_pred, avg_rmse, avg_nrmse, avg_nse, avg_r2, avg_willmott, best_rmse, best_nrmse, best_nse, best_r2, best_wi_score, model_name, scenario_name, DDDD,params)  ## plot the results and present the best performance and the average of nested cross validation for the outer layer\n",
                "        \n",
                "        formatted_results = summarize_results(formatted_results, DDDD, model_name, scenario_name, target_variable,best_rmse, best_nrmse, best_nse, best_r2, best_wi_score,avg_nse,avg_rmse,avg_nrmse,avg_r2,avg_willmott)   ## summarize the results\n",
                "        \n",
                "        model_name_label=' '.join(word.capitalize() for word in model_name.split('_'))\n",
                "        permutation_importance_analysis(optimization_search.best_estimator_, input_variables, output_variable,model_name_label,scenario_name,target_variable[0],custome_scorer)  ## perform permutation analysis\n",
                "        DDDD=DDDD+1\n",
                "\n",
                "formatted_results[formatted_results.avg_rmse>0].to_csv('Scenarios_all_models_bagging.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "17dee13b-d49b-4197-8663-5c1d18932218",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import geopandas as gpd\n",
                "import pandas as pd\n",
                "from shapely.geometry import Point\n",
                "from ml_function import *\n",
                "\n",
                "categorical_features = [ 'Soil','landuse',\n",
                "                        'StreamLeve', \n",
                "                        'StreamOrde',\n",
                "                        'AQU_CHAR', \n",
                "                        'LDSYSTEM', \n",
                "                        'GeologicUnit_GENERALIZE',\n",
                "                        'flowline',\n",
                "                        'FCode', \n",
                "                        'FType', \n",
                "                         'WB'\n",
                "                       ]\n",
                "\n",
                "num_features=['er_TRANSMSV_1',\n",
                "            'Simulated_TRANSMSV_1',\n",
                "            'SlopeLenKm',\n",
                "            'Slope',\n",
                "            'MaxElevSmo',\n",
                "            'MinElevSmo']\n",
                "\n",
                "rbf_features=['x','y','Top_Elev']\n",
                "def plot_county_point(dataset, COUNTY, col_sim, col_ml, col_obs):\n",
                "    dataset = dataset[~dataset.COUNTY.isna()]\n",
                "   # dataset = dataset[~dataset.col_sim.isna()]\n",
                "\n",
                "    if 'geometry' not in dataset.columns:\n",
                "        X = dataset.x.values\n",
                "        Y = dataset.y.values\n",
                "        points = [Point(x, y) for x, y in zip(X, Y)]\n",
                "        dataset.loc[:,'geometry'] = points\n",
                "        dataset = gpd.GeoDataFrame(dataset, geometry='geometry', crs='EPSG:26990').reset_index(drop=True)\n",
                "\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(50, 30))\n",
                "    \n",
                "    dataset.plot(column=col_sim, s=1, cmap='Spectral_r', ax=axes[0])\n",
                "    axes[0].set_title('Krigging')\n",
                "    axes[0].set_axis_off()\n",
                "    dataset.plot(column=col_ml, s=1, cmap='Spectral_r', ax=axes[1])\n",
                "    axes[1].set_title('Krigging Error')\n",
                "    axes[1].set_axis_off()\n",
                "    dataset.plot(column=col_obs, s=1, cmap='Spectral_r', ax=axes[2])\n",
                "    axes[2].set_title('Machine learning')\n",
                "    axes[2].set_axis_off()\n",
                "    plt.savefig(f'_{uuid.uuid4()}_{COUNTY}.jpeg', dpi=300)\n",
                "    plt.show()\n",
                "\n",
                "    return dataset\n",
                "\n",
                "COUNTY=['Huron','Tuscola','Sanilac']\n",
                "\n",
                "dataset = pd.read_pickle('grid_points_well_obs.pk1')\n",
                "#best_rf_model = joblib.load('best_rf_model_gradient_boosting_gpu_TRANSMSV_1_optimization_TRANSMSV_1.joblib')\n",
                "best_rf_model = joblib.load('best_rf_model_random_forest_gpu_TRANSMSV_1_optimization_TRANSMSV_1.joblib')\n",
                "y_pred = best_rf_model.predict(dataset.loc[:,num_features+categorical_features+rbf_features] )\n",
                "dataset['ML']=y_pred\n",
                "dataset = plot_county_point(dataset, COUNTY, 'Simulated_TRANSMSV_1', 'er_TRANSMSV_1','ML')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dceb368f-2d6b-44b1-a2a8-2fee8b45d7cc",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import geopandas as gpd\n",
                "import pandas as pd\n",
                "from shapely.geometry import Point\n",
                "\n",
                "def plot_county_point(dataset, COUNTY, col_sim, col_ml, col_obs):\n",
                "    #dataset = dataset[dataset.COUNTY == COUNTY]\n",
                "    dataset = dataset[~dataset.COUNTY.isna()]\n",
                "    if 'geometry' not in dataset.columns:\n",
                "        X = dataset.x.values\n",
                "        Y = dataset.y.values\n",
                "        points = [Point(x, y) for x, y in zip(X, Y)]\n",
                "        dataset['geometry'] = points\n",
                "        dataset = gpd.GeoDataFrame(dataset, geometry='geometry', crs='EPSG:26990').reset_index(drop=True)\n",
                "\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "    \n",
                "    # Define a common color range for all plots\n",
                "    vmin = dataset[[col_sim, col_ml, col_obs]].min().min()\n",
                "    vmax = dataset[[col_sim, col_ml, col_obs]].max().max()\n",
                "    \n",
                "    # Plot with the same color range and add color bars\n",
                "    plot_sim = dataset.plot(column=col_sim, s=1, cmap='Spectral_r', vmin=vmin, vmax=vmax, ax=axes[0])\n",
                "    axes[0].set_title('Krigging')\n",
                "    axes[0].set_axis_off()\n",
                "    plt.colorbar(plot_sim.get_children()[0], ax=axes[0], orientation='vertical')\n",
                "\n",
                "    plot_ml = dataset.plot(column=col_ml, s=1, cmap='Spectral_r', vmin=vmin, vmax=vmax, ax=axes[1])\n",
                "    axes[1].set_title('Krigging Error')\n",
                "    axes[1].set_axis_off()\n",
                "    plt.colorbar(plot_ml.get_children()[0], ax=axes[1], orientation='vertical')\n",
                "\n",
                "    plot_obs = dataset.plot(column=col_obs, s=1, cmap='Spectral_r', vmin=vmin, vmax=vmax, ax=axes[2])\n",
                "    axes[2].set_title('Machine learning')\n",
                "    axes[2].set_axis_off()\n",
                "    plt.colorbar(plot_obs.get_children()[0], ax=axes[2], orientation='vertical')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(f'_{uuid.uuid4()}_{COUNTY}.jpeg', dpi=300)\n",
                "    plt.show()\n",
                "\n",
                "    return dataset\n",
                "\n",
                "\n",
                "\n",
                "dataset = pd.read_pickle('grid_points_well_obs.pk1')\n",
                "#best_rf_model = joblib.load('best_rf_model_gradient_boosting_gpu_TRANSMSV_1_optimization_TRANSMSV_1.joblib')\n",
                "best_rf_model = joblib.load('best_rf_model_random_forest_gpu_TRANSMSV_1_optimization_TRANSMSV_1.joblib')\n",
                "\n",
                "y_pred = best_rf_model.predict(dataset.loc[:,num_features+categorical_features+rbf_features] )\n",
                "\n",
                "dataset['ML']=y_pred\n",
                "\n",
                "\n",
                "dataset = plot_county_point(dataset, COUNTY, 'Simulated_TRANSMSV_1', 'er_TRANSMSV_1','ML')\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c908a66-3925-4554-890b-95a0bc8ec243",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}