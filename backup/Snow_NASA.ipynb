{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80b67f1e-8497-4a92-9dac-29c7f910dca1",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b875bd61-ca0f-427a-943a-8e3068b535f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Snow Water Equivalent (SWE): Measures the amount of water contained in the snowpack.\n",
                "# Snow Depth: The depth of the snowpack.\n",
                "# Snow Melt Runoff: The amount of meltwater running off at the base of the snowpack.\n",
                "# Sublimation from the Snow Pack: The process where snow changes directly from solid to gas.\n",
                "# Sublimation of Blowing Snow: Sublimation specific to snow that's being transported by wind.\n",
                "# Solid Precipitation: Snowfall or other forms of solid precipitation.\n",
                "# Liquid Precipitation: Rainfall or other forms of liquid precipitation.\n",
                "# Snow Pack Average Temperature: The average temperature within the snowpack.\n",
                "\n",
                "import arcpy\n",
                "from arcpy.sa import *\n",
                "import tarfile\n",
                "import gzip\n",
                "import shutil\n",
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import glob\n",
                "from datetime import datetime\n",
                "from tqdm import tqdm\n",
                "from osgeo import gdal, osr\n",
                "import geopandas as gpd\n",
                "import rasterio\n",
                "def reading_geoTIFF_metafile(meta_data_path,file_path):\n",
                "    geoTIFF_metadata = {}\n",
                "    with open(meta_data_path, 'r') as f:\n",
                "        meta = f.readlines()\n",
                "        for line in meta:\n",
                "            if 'Description' in line:\n",
                "                geoTIFF_metadata['variable_name'] = line.split(':')[1]\n",
                "            if 'Number of columns' in line:\n",
                "                geoTIFF_metadata['num_columns'] = int(line.split(':')[1].strip())\n",
                "            elif 'Number of rows' in line:\n",
                "                geoTIFF_metadata['num_rows'] = int(line.split(':')[1].strip())\n",
                "            elif 'No data value' in line:\n",
                "                geoTIFF_metadata['no_data_value'] = float(line.split(':')[1].strip())\n",
                "            elif 'Minimum x-axis coordinate' in line:\n",
                "                geoTIFF_metadata['x_min'] = float(line.split(':')[1].strip())\n",
                "            elif 'Maximum y-axis coordinate' in line:\n",
                "                geoTIFF_metadata['y_max'] = float(line.split(':')[1].strip())\n",
                "            elif 'X-axis resolution' in line:\n",
                "                geoTIFF_metadata['x_res'] = float(line.split(':')[1].strip())\n",
                "            elif 'Y-axis resolution' in line:\n",
                "                geoTIFF_metadata['y_res'] = -float(line.split(':')[1].strip())  \n",
                "            elif 'X-axis offset' in line:\n",
                "                geoTIFF_metadata['x_skew'] = float(line.split(':')[1].strip())\n",
                "            elif 'Y-axis offset' in line:\n",
                "                geoTIFF_metadata['y_skew'] = float(line.split(':')[1].strip())\n",
                "            \n",
                "            elif 'Start year' in line:\n",
                "                geoTIFF_metadata['year'] = int(line.split(':')[1].strip())\n",
                "            elif 'Start month' in line:\n",
                "                geoTIFF_metadata['month'] = int(line.split(':')[1].strip())\n",
                "            elif 'Start day' in line:\n",
                "                geoTIFF_metadata['day'] = int(line.split(':')[1].strip())\n",
                "\n",
                "\n",
                "            elif 'Minimum data value' in line:\n",
                "                geoTIFF_metadata['min'] = float(line.split(':')[1].strip())\n",
                "            elif 'Maximum data value' in line:\n",
                "                geoTIFF_metadata['max'] = float(line.split(':')[1].strip())\n",
                "            elif 'No data value' in line:\n",
                "                geoTIFF_metadata['novalue'] = int(line.split(':')[1].strip())\n",
                "\n",
                "    return geoTIFF_metadata\n",
                "    \n",
                "def create_raster(data, metadata, output_path):\n",
                "    \"\"\"\n",
                "    Create a raster file from the provided data and metadata.\n",
                "    \"\"\"\n",
                "    # Create a driver to write the raster file\n",
                "    driver = gdal.GetDriverByName('GTiff')\n",
                "#    print( data.shape)\n",
                "    out_raster = driver.Create(output_path, data.shape[1], data.shape[0], 1, gdal.GDT_UInt16)\n",
                "\n",
                "    \n",
                "    # Set geotransform and projection\n",
                "    geotransform = [metadata['x_min'], metadata['x_res'], metadata['x_skew'],\n",
                "                    metadata['y_max'], metadata['y_skew'], metadata['y_res']]\n",
                "\n",
                "    out_raster.SetGeoTransform(geotransform)    \n",
                "    srs = osr.SpatialReference()\n",
                "    srs.ImportFromEPSG(4326)  # WGS84\n",
                "    out_raster.SetProjection(srs.ExportToWkt())\n",
                "\n",
                "    # Write data to the raster band\n",
                "    out_band = out_raster.GetRasterBand(1)\n",
                "    out_band.WriteArray(data)\n",
                "    out_band.SetNoDataValue(metadata['no_data_value'])\n",
                "    out_band.FlushCache()\n",
                "\n",
                "    # Close the dataset\n",
                "    out_raster = None\n",
                "\n",
                "\n",
                "def create_plot(data_2d, variable_name):\n",
                "                plt.imshow(data_2d, cmap='gray')\n",
                "                variable_name = variable_name.split(',')[0]\n",
                "                variable_name = variable_name.replace(' ','_')    \n",
                "                plt.title(variable_name)\n",
                "                plt.colorbar()\n",
                "                plt.show()\n",
                "\n",
                "def open_gz_files(base_directory, target_dir):\n",
                "    file_paths = glob.glob(base_directory + '*.gz')\n",
                "    os.makedirs(target_dir, exist_ok=True)\n",
                "    for file_path in file_paths:\n",
                "        new_file_path = os.path.join(target_dir, os.path.basename(file_path[:-3]))\n",
                "        with gzip.open(file_path, 'rb') as f_in:\n",
                "            with open(new_file_path, 'wb') as f_out:\n",
                "                shutil.copyfileobj(f_in, f_out)\n",
                "        os.remove(file_path)\n",
                "\n",
                "\n",
                "        \n",
                "def remove_dat_txt_file (target_dir):\n",
                "    files= os.listdir(target_dir)\n",
                "    for file in files:\n",
                "        if '.dat' in file or '.txt' in file or '.bil' in file or '.hdr' in file:\n",
                "            os.remove(os.path.join(target_dir, file))\n",
                "\n",
                "\n",
                "\n",
                "# Function to generate a continuous date range\n",
                "def generate_date_range(start_date, end_date):\n",
                "    return [start_date + datetime.timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
                "\n",
                "def remove_dat_txt_file (target_dir):\n",
                "    files= os.listdir(target_dir)\n",
                "    for file in files:\n",
                "        if '.dat' in file or '.txt' in file or '.bil' in file or '.hdr' in file:\n",
                "          #  os.remove(os.path.join(target_dir, file))\n",
                "            arcpy.Delete_management(os.path.join(target_dir, file))\n",
                "\n",
                "\n",
                "\n",
                "# Main processing\n",
                "tar_file_paths = glob.glob(r'/data/MyDataBase/SWATGenXAppData/snow/snow/' + '*.tar')\n",
                "print(f'Number of tar files: {len(tar_file_paths)}')\n",
                "\n",
                "entire_data = {}\n",
                "dates_processed = set()\n",
                "\n",
                "\n",
                "def rename_dat_to_bil(target_dir, file, file_path):\n",
                "\n",
                "    new_file_path = os.path.join(target_dir, file[:-4] + \".bil\")\n",
                "    \n",
                "    os.rename(file_path, new_file_path)\n",
                "\n",
                "\n",
                "\n",
                "for tar_file_path in tqdm(tar_file_paths, desc='Processing tar files'):\n",
                "    # Your existing code for processing each tar file\n",
                "\n",
                "    with tarfile.open(tar_file_path, \"r:\") as tar:\n",
                "        tar.extractall(path=os.path.dirname(tar_file_path))\n",
                "\n",
                "    base_directory = '/data/MyDataBase/SWATGenXAppData/snow/snow/'\n",
                "    subset_name = 'michigan'\n",
                "    target_dir = os.path.join(base_directory, subset_name)\n",
                "    open_gz_files(base_directory, target_dir)\n",
                "\n",
                "    for file in os.listdir(target_dir):\n",
                "        if file.endswith(\".dat\"):\n",
                "         #   print(file)\n",
                "            file_path = os.path.join(target_dir,file)\n",
                "            rename_dat_to_bil(target_dir,file,file_path)\n",
                "            \n",
                "            bil_file_path = file_path[:-4] + '.bil'\n",
                "            meta_data_path = file_path[:-4] + '.hdr'\n",
                "            \n",
                "            with open(meta_data_path, 'w') as f:\n",
                "                f.write('byteorder M\\nlayout bil\\n nbands 1\\n nbits 16 \\n ncols 6935\\n nrows 3351\\n ulxmap -124.729583333331703\\n ulymap 52.871249516804028\\n xdim 0.0083333333\\n ydim 0.00833333333\\n' \n",
                ")\n",
                "                \n",
                "            meta_data_path = file_path[:-4] + '.txt'\n",
                "            geoTIFF_metadata = reading_geoTIFF_metafile(meta_data_path,file_path)\n",
                "\n",
                "\n",
                "            day = geoTIFF_metadata['day']\n",
                "            month = geoTIFF_metadata['month']\n",
                "            year = geoTIFF_metadata['year']      \n",
                "\n",
                "            variable_name = geoTIFF_metadata['variable_name'].split(',')[0].replace('-', '_').replace(' ', '_').replace('\\n', '')\n",
                "\n",
                "            \n",
                "            out_geotif_path = os.path.join(os.path.dirname(file_path),f'{year}',f'{month}',f'{day}',f'{variable_name}.tif')\n",
                "            os.makedirs(os.path.dirname(out_geotif_path), exist_ok=True)\n",
                "\n",
                "\n",
                "            arcpy.env.overwriteOutput=True\n",
                "            clipped_raster_path = out_geotif_path\n",
                "            \n",
                "            # Michigan extent (example values, replace with actual extents)\n",
                "            x_min, y_min, x_max, y_max = -87, 41.5, -82, 47\n",
                "            rectangle = f\"{x_min} {y_min} {x_max} {y_max}\"\n",
                "            \n",
                "            # Open the .bil file as a raster\n",
                "            bil_raster = arcpy.Raster(bil_file_path)\n",
                "            \n",
                "            min_value = geoTIFF_metadata['min'] \n",
                "            max_value =geoTIFF_metadata['max'] \n",
                "\n",
                "           \n",
                " \n",
                "            arcpy.Clip_management(bil_raster, rectangle, clipped_raster_path)\n",
                "            \n",
                "            # Read the clipped raster\n",
                "            clipped_raster = Raster(clipped_raster_path)\n",
                "            \n",
                "            # Apply minimum and maximum constraints\n",
                "            constrained_raster = Con((clipped_raster < min_value) | (clipped_raster > max_value), np.nan, clipped_raster)\n",
                "            \n",
                "            # Save the constrained raster\n",
                "            constrained_raster_path = clipped_raster_path.replace(\".tif\", \"_constrained.tif\")\n",
                "            \n",
                "            constrained_raster.save(constrained_raster_path)\n",
                "            arcpy.Delete_management(clipped_raster_path)\n",
                "            \n",
                "    remove_dat_txt_file(target_dir)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42b2745a-fc85-494a-b01c-486a9bf9230f",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da552741-78d2-4fc2-98ee-fdebacb2328b",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import rasterio\n",
                "import numpy as np\n",
                "import os\n",
                "from calendar import monthrange\n",
                "# Initialize an empty list for Snow Melt Rate (SMR)\n",
                "\n",
                "def storing_rasters(start_year, end_year, name_of_dataset):\n",
                "    smr = []\n",
                "    for year in range(start_year, end_year+1):\n",
                "        print(year)\n",
                "        lat_long = 0\n",
                "        for month in range(1, 13):\n",
                "            days_in_month = monthrange(year, month)[1]\n",
                "            for day in range(1, days_in_month + 1):\n",
                "                file_path = fr'/data/MyDataBase/SWATGenXAppData/snow/snow/michigan/{year}/{month}/{day}/{name_of_dataset}'\n",
                "                \n",
                "                # Check if the file exists\n",
                "                if not os.path.exists(file_path):\n",
                "                    data = np.nan * np.zeros([660, 600]) \n",
                "                else:\n",
                "                    # Open the file\n",
                "                    with rasterio.open(file_path) as src:\n",
                "                        # Read the raster values\n",
                "                        data = src.read(1)\n",
                "                      #  print(data.shape)\n",
                "                        data = np.where(abs(data) > 1e12, np.nan, data)\n",
                "            \n",
                "                        if lat_long==0:\n",
                "                            lat_long = 1\n",
                "                            transform = src.transform\n",
                "                            latitudes = np.zeros(data.shape, dtype=np.float64)\n",
                "                            longitudes = np.zeros(data.shape, dtype=np.float64)\n",
                "            \n",
                "                            # Calculate latitude and longitude for each pixel\n",
                "                            for row in range(data.shape[0]):\n",
                "                                for col in range(data.shape[1]):\n",
                "                                    x, y = rasterio.transform.xy(transform, row, col, offset='center')\n",
                "                                    longitudes[row, col] = x\n",
                "                                    latitudes[row, col] = y\n",
                "                                \n",
                "                smr.append(data)\n",
                "\n",
                "    smr = np.stack(smr, axis=0)\n",
                "    return smr, longitudes, latitudes\n",
                "\n",
                "\n",
                "\n",
                "import netCDF4 as nc\n",
                "\n",
                "\n",
                "def create_netcdf(stacked_data, lons, lats, start_year, end_year,name_of_dataset):\n",
                "    filename=f'D:\\MyDataBase\\snow\\SNODAS{name_of_dataset[:-4]}_{start_year}_{end_year}.nc'\n",
                "    # Determine the dimensions\n",
                "    time, lat, lon = stacked_data.shape\n",
                "\n",
                "    # Create a new NetCDF file\n",
                "    dataset = nc.Dataset(filename, 'w', format='NETCDF4_CLASSIC')\n",
                "\n",
                "    # Create dimensions - time, latitude, longitude\n",
                "    dataset.createDimension('time', time)\n",
                "    dataset.createDimension('lat', lat)\n",
                "    dataset.createDimension('lon', lon)\n",
                "\n",
                "    # Create variables\n",
                "    times = dataset.createVariable('time', 'i4', ('time',))\n",
                "    latitudes = dataset.createVariable('lat', 'f4', ('lat',))\n",
                "    longitudes = dataset.createVariable('lon', 'f4', ('lon',))\n",
                "    values = dataset.createVariable('value', 'f4', ('time', 'lat', 'lon',))\n",
                "\n",
                "    # Define units and other attributes\n",
                "    times.units = 'days since ' + str(start_year) + '-01-01'\n",
                "    latitudes.units = 'degrees_north'\n",
                "    longitudes.units = 'degrees_east'\n",
                "    values.units = 'Unknown'  # Replace with the actual unit of your data\n",
                "\n",
                "    # Assign data to the variables\n",
                "    times[:] = np.arange(time)  # Assuming each time step is one day\n",
                "    \n",
                "    # Flatten the latitudes and longitudes if they are 2D\n",
                "    if lats.ndim > 1:\n",
                "        latitudes[:] = lats[:, 0]  # Assuming latitudes are constant across rows\n",
                "    else:\n",
                "        latitudes[:] = lats\n",
                "\n",
                "    if lons.ndim > 1:\n",
                "        longitudes[:] = lons[0, :]  # Assuming longitudes are constant across columns\n",
                "    else:\n",
                "        longitudes[:] = lons\n",
                "\n",
                "    values[:, :, :] = stacked_data\n",
                "\n",
                "    # Close the NetCDF file\n",
                "    dataset.close()\n",
                "\n",
                "name_of_datasets = [   '_Modeled_melt_rate_constrained.tif',\n",
                "    #'_Modeled_average_temperature_constrained.tif', '_Modeled_blowing_snow_sublimation_rate_constrained.tif', '_Modeled_snow_layer_thickness_constrained.tif', '_Modeled_snow_water_equivalent_constrained.tif', \n",
                "   # '_Modeled_snowpack_sublimation_rate_constrained.tif', '_Non_snow_accumulation_constrained.tif', '_Snow_accumulation_constrained.tif' \n",
                "                   ]  #'_Modeled_melt_rate_constrained.tif',\n",
                "\n",
                "np.savetxt(fname='/data/MyDataBase/SWATGenXAppData/snow/SNODAS_latitudes_michigan.txt', X = latitudes)\n",
                "np.savetxt(fname='/data/MyDataBase/SWATGenXAppData/snow/SNODAS_longitudes_michigan.txt', X = longitudes)\n",
                "\n",
                "start_year = 2004\n",
                "end_year = 2023\n",
                "for name_of_dataset in name_of_datasets:\n",
                "    print(name_of_dataset)\n",
                "    stacked_dataset, longitudes, latitudes  = storing_rasters(start_year, end_year, name_of_dataset)  \n",
                "    create_netcdf(stacked_dataset, longitudes, latitudes, start_year, end_year, name_of_dataset)\n",
                "\n",
                "\n",
                "########################### creating location shapeifles\n",
                "\n",
                "from shapely.geometry import Point\n",
                "import geopandas as gpd\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "latitudes = np.loadtxt('/data/MyDataBase/SWATGenXAppData/snow/SNODAS_latitudes_michigan.txt')                    \n",
                "longitudes = np.loadtxt('/data/MyDataBase/SWATGenXAppData/snow/SNODAS_longitudes_michigan.txt')\n",
                "latitudes_shape = latitudes.shape  # Assuming latitudes is a 2D NumPy array\n",
                "longitudes_shape = longitudes.shape  # Assuming longitudes is a 2D NumPy array\n",
                "\n",
                "# Flatten the arrays and also get the corresponding row and column indices\n",
                "lat_flat = latitudes.flatten()\n",
                "long_flat = longitudes.flatten()\n",
                "row_indices, col_indices = np.indices(latitudes_shape)\n",
                "\n",
                "# Create DataFrame\n",
                "SNODAS_stations = pd.DataFrame({\n",
                "    'geometry': [Point(x, y) for x, y in zip(long_flat, lat_flat)],\n",
                "    'lat': lat_flat,\n",
                "    'long': long_flat,\n",
                "    'row_id': row_indices.flatten(),\n",
                "    'col_id': col_indices.flatten(),\n",
                "    'ID': np.arange(100000, 100000 + lat_flat.size)\n",
                "})\n",
                "\n",
                "# Convert to GeoDataFrame\n",
                "SNODAS_stations = gpd.GeoDataFrame(SNODAS_stations, crs='EPSG:4326', geometry='geometry')\n",
                "\n",
                "\n",
                "# Save to shapefile\n",
                "SNODAS_stations.to_file(fr'D:\\MyDataBase\\snow\\SNODAS_locations.shp')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c202510c-9f49-4378-a140-acaf7bb347e2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import geopandas as gpd\n",
                "import numpy as np\n",
                "import concurrent.futures\n",
                "import netCDF4 as nc\n",
                "import pandas as pd\n",
                "\n",
                "def load_variable(file_path, var_name):\n",
                "    with nc.Dataset(file_path) as dataset:\n",
                "        return dataset.variables[var_name][:]\n",
                "\n",
                "def loading_dataset(BASE_PATH):\n",
                "    # Load latitudes and longitudes\n",
                "    SNODAS_stations=gpd.read_file(os.path.join(BASE_PATH, 'snow/SNODAS_locations.shp'))\n",
                "    latitudes  = np.loadtxt(os.path.join(BASE_PATH,'snow/SNODAS_latitudes_michigan.txt'))                   \n",
                "    longitudes = np.loadtxt(os.path.join(BASE_PATH,'snow/SNODAS_longitudes_michigan.txt'))    \n",
                "    # Define file paths and variable names\n",
                "    datasets = {\n",
                "        \n",
                "        'average_temperature':             os.path.join(BASE_PATH, 'snow/SNODAS_Modeled_average_temperature_constrained_2004_2023.nc'),\n",
                "        'blowing_snow_sublimation_rate':   os.path.join(BASE_PATH, 'snow/SNODAS_Modeled_blowing_snow_sublimation_rate_constrained_2004_2023.nc'),\n",
                "        'melt_rate':                       os.path.join(BASE_PATH, 'snow/SNODAS_Modeled_melt_rate_constrained_2004_2023.nc'),\n",
                "        'snow_layer_thickness':            os.path.join(BASE_PATH, 'snow/SNODAS_Modeled_snow_layer_thickness_constrained_2004_2023.nc'),\n",
                "        'snow_water_equivalent':           os.path.join(BASE_PATH, 'snow/SNODAS_Modeled_snow_water_equivalent_constrained_2004_2023.nc'),\n",
                "        'snowpack_sublimation_rate':       os.path.join(BASE_PATH, 'snow/SNODAS_Modeled_snowpack_sublimation_rate_constrained_2004_2023.nc'),\n",
                "        'non_snow_accumulation':           os.path.join(BASE_PATH, 'snow/SNODAS_Non_snow_accumulation_constrained_2004_2023.nc'),\n",
                "        'snow_accumulation':               os.path.join(BASE_PATH, 'snow/SNODAS_Snow_accumulation_constrained_2004_2023.nc')\n",
                "    }\n",
                "    \n",
                "    # Load data from NetCDF files\n",
                "    variables = {key: load_variable(path, 'value') for key, path in datasets.items()}\n",
                "\n",
                "    return variables, longitudes, latitudes, SNODAS_stations, datasets\n",
                "\n",
                "BASE_PATH = f'D:/MyDataBase'\n",
                "\n",
                "variables, longitudes, latitudes , SNODAS_stations, datasets = loading_dataset(BASE_PATH)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee6acaff-664f-45cb-a903-c0f60fd7d836",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "from datetime import datetime\n",
                "\n",
                "\n",
                "variable_names = [ 'average_temperature',             \n",
                "        'melt_rate',               \n",
                "        'snow_layer_thickness',           \n",
                "            'snow_water_equivalent',          \n",
                "        'snowpack_sublimation_rate',      \n",
                "        'non_snow_accumulation',       \n",
                "        'snow_accumulation']\n",
                "\n",
                "converters = [1,\n",
                "        1/100,\n",
                "        1/1,\n",
                "        1/1,\n",
                "        1/ 100,\n",
                "        1/10,\n",
                "        1/10]\n",
                "\n",
                "units = ['Kelvin',\n",
                "'mm',\n",
                "'mm',\n",
                "'mm',\n",
                "'mm',\n",
                "'mm',\n",
                "'kg/sqm',\n",
                "'kg/sqm']\n",
                "              \n",
                "for variable_name,converter, unit in zip(variable_names, converters, units): \n",
                "    # Assuming 'melt_rate' is a 3D array with shape (days, height, width)\n",
                "    melt_rate = converter*variables[variable_name][:]  # Example array\n",
                "\n",
                "    # Define the start year and the total number of days\n",
                "    start_year = 2000  # Example start year\n",
                "    num_days = melt_rate.shape[0]\n",
                "\n",
                "    # Create a date range\n",
                "    dates = pd.date_range(start=datetime(start_year, 1, 1), periods=num_days)\n",
                "\n",
                "    # Calculate the number of years\n",
                "    num_years = len(np.unique(dates.year))\n",
                "\n",
                "    # Reshape melt_rate to have years, months, and days\n",
                "    annual_melt_rate = np.empty((num_years, 12, *melt_rate.shape[1:]))\n",
                "\n",
                "    # Calculate monthly mean melt rate\n",
                "    for year in range(num_years):\n",
                "        for month in range(1, 13):\n",
                "            # Select data for the current month and year\n",
                "            mask = (dates.year == start_year + year) & (dates.month == month)\n",
                "            monthly_data = melt_rate[mask, :, :]\n",
                "\n",
                "            # Check if there are any non-NaN values before calculating the mean\n",
                "            if np.any(~np.isnan(monthly_data)):\n",
                "                # Calculate the mean, ignoring NaN values\n",
                "                monthly_mean = np.nanmean(monthly_data, axis=0)\n",
                "            else:\n",
                "                # If all values are NaN, fill the array segment with NaNs\n",
                "                monthly_mean = np.full(melt_rate.shape[1:], np.nan)\n",
                "\n",
                "            annual_melt_rate[year, month-1, :, :] = monthly_mean\n",
                "\n",
                "    # Calculate annual mean melt rate\n",
                "    with np.errstate(invalid='ignore'):\n",
                "        annual_mean_melt_rate = np.nanmean(annual_melt_rate, axis=(0, 1))\n",
                "\n",
                "    # Plotting the monthly mean for each month across all years\n",
                "    fig, axs = plt.subplots(3, 4, figsize=(15, 10))  # Adjust the size as needed\n",
                "    axs = axs.flatten()\n",
                "    month_label = ['Jan','Feb','March','April','May','June','July','Agu','Sep','Oct','Nov','Dec']\n",
                "\n",
                "    for month in range(12):\n",
                "        # Only plot if the monthly mean contains non-NaN values\n",
                "        if not np.all(np.isnan(annual_melt_rate[:, month, :, :])):\n",
                "            avm=np.nanmean(annual_melt_rate[:, month, :, :], axis=0)\n",
                "            im = axs[month].imshow(avm, cmap='winter_r', vmax=np.nanpercentile(avm, 97.5), vmin=np.nanpercentile(avm, 2.5))\n",
                "            axs[month].set_title(f'Mean. {variable_name}({unit}):{month_label[month]}')\n",
                "            fig.colorbar(im, ax=axs[month])\n",
                "        else:\n",
                "            # If the entire month has no data, display a message\n",
                "            axs[month].text(0.5, 0.5, 'No data for this month', horizontalalignment='center', verticalalignment='center', transform=axs[month].transAxes)\n",
                "            axs[month].set_title(f'Mean. Monthly: {month_label[month]}')\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(f'D:\\MyDataBase\\Documentations\\SNODAS_annual_monthly_figures\\Average_Monthly_{variable_name}_Michigan_SNODAS.jpeg',dpi=300)\n",
                "    plt.title(f'Montly {variable_name}({unit})')\n",
                "    plt.show()\n",
                "\n",
                "    # Plotting the annual mean across all months and years\n",
                "    plt.figure(figsize=(10, 10))  # Adjust the size as needed\n",
                "    im = plt.imshow(annual_mean_melt_rate, cmap='winter_r', vmax=np.nanpercentile(annual_mean_melt_rate,97.25), vmin=np.nanpercentile(annual_mean_melt_rate,2.5))\n",
                "    plt.title(f'Mean Annual {variable_name}({unit})')\n",
                "    plt.colorbar(im)\n",
                "    plt.savefig(f'/data/MyDataBase/SWATGenXAppData/Documentations/SNODAS_annual_monthly_figures/Average_Annual_{variable_name}_michigan_SNODAS.jpeg',dpi=300)\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c6c30b37-ff71-44a2-88a2-f5e2c30dc6ce",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "raw",
            "id": "99eafa54-c746-4459-9159-af7120ae1590",
            "metadata": {},
            "source": [
                "##### examine variables in motion !\n",
                "\n",
                "from IPython.display import clear_output, display\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "\n",
                "# Number of frames to display\n",
                "variable_name = 'snowpack_sublimation_rate'\n",
                "\n",
                "num_frames = variables[variable_name].shape[0]\n",
                "\n",
                "for frame in range(num_frames):\n",
                "    plt.imshow(variables[variable_name][frame, :, :], cmap='viridis')\n",
                "    plt.title(f'Time: {frame}')\n",
                "    plt.colorbar()\n",
                "    plt.show()\n",
                "\n",
                "    # Clear the output to make the animation effect\n",
                "    clear_output(wait=True)\n",
                "\n",
                "    # Pause for a brief period to control the speed of the animation\n",
                "    time.sleep(0.1)  # Adjust the time as needed\n"
            ]
        },
        {
            "cell_type": "raw",
            "id": "7d72d2bd-4e5e-4109-ba58-cdc3a58b480a",
            "metadata": {},
            "source": [
                "### example of extracting variable for a specific location\n",
                "\n",
                "from shapely.geometry import Point\n",
                "import geopandas as gpd\n",
                "\n",
                "\n",
                "def getting_closest_station (target_lat, target_lon, shape_location):\n",
                "    target_point = Point(target_lon, target_lat)\n",
                "    shape_location['distance'] = shape_location.apply(lambda row: target_point.distance(row['geometry']), axis=1)\n",
                "    closest_point = shape_location.loc[shape_location['distance'].idxmin()]\n",
                "    return closest_point.row_id, closest_point.col_id, closest_point.ID\n",
                "\n",
                "target_lat = 43.0  \n",
                "target_lon = -85.0 \n",
                "row, col, ID = getting_closest_station (target_lat, target_lon, SNODAS_stations)\n",
                "print(variables['snow_accumulation'][:, row, col])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f9353ea-fd90-4993-b970-8cb2e98faa04",
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "import os\n",
                "from shapely import Point\n",
                "import shutil\n",
                "\n",
                "def saving_output(output,out_file_path):\n",
                "    \n",
                "    output = output.rename(columns = {\n",
                "        'average_temperature': 'SAT',              #  units: Kelvin                                Description: Modeled average temperature, SWE-weighted average of snow layers  \n",
                "        'blowing_snow_sublimation_rate': 'BSSR',   #  units: Meters / 100000                       Description: Modeled blowing snow sublimation rate, 24-hour total         ### convert it to mm  by multiplying with 1/100\n",
                "        'melt_rate': 'MR',                         #  units: Meters / 100000                       Description: Modeled melt rate, bottom of snow layers, 24-hour total      ### convert it to mm  by multiplying with 1/100\n",
                "        'snow_layer_thickness': 'SLT',             #  units: Meters / 1000                         Description: Modeled snow layer thickness, total of snow layer            ### convert it to mm  by multiplying with 1/1\n",
                "        'snow_water_equivalent': 'SWE',            #  units: Meters / 1000                         Description: Modeled snow water equivalent, total of snow layers          ### convert it to mm  by multiplying with 1/1\n",
                "        'snowpack_sublimation_rate': 'SSR',        #  units: Meters / 100000                       Description: Modeled snowpack sublimation rate, 24-hour total             ### convert it to mm  by multiplying with 1/ 100\n",
                "        'non_snow_accumulation': 'NSA',            #  units: Kilograms per square meter / 10       Description: Non-snow accumulation, 24-hour total                         ### convert it to kg/sqm by multiplying with 1/10\n",
                "        'snow_accumulation': 'SA'                  #  units: Kilograms per square meter / 10       Description: Snow accumulation, 24-hour total                             ### convert it to kg/sqm by multiplying with 1/10\n",
                "    })\n",
                "    \n",
                "    output['year'] = output.date.dt.year\n",
                "    output['day'] = output.date.dt.dayofyear    #### i want to get the day based on 0 to 365\n",
                "    # Append the DataFrame without header\n",
                "    output = output[['year','day','SA','NSA','MR','SWE', 'BSSR','SLT','SSR','SAT']]\n",
                "    \n",
                "    # Unit conversions\n",
                "    output['SAT']  = output['SAT']   -273.15             # Kelvin to celsius degree\n",
                "    output['BSSR'] = output['BSSR']  * 1000/100000       # Meters/100000 to mm\n",
                "    output['MR']   = output['MR']    * 1000/100000       # Meters/100000 to mm\n",
                "    output['SLT']  = output['SLT']   * 1000/1000         # Meters/1000 to mm\n",
                "    output['SWE']  = output['SWE']   * 1000/1000         # Meters/1000 to mm\n",
                "    output['SSR']  = output['SSR']   * 1000/100000       # Meters/100000 to mm\n",
                "    output['NSA']  = output['NSA']   * 1/10              # Kg/m2\n",
                "    output['SA']   = output['SA']    * 1/10              # Kg/m2\n",
                "    \n",
                "    output = output.fillna(-99)\n",
                "    output.to_csv(out_file_path, mode='a', header=True, index=False, sep=' ')\n",
                "\n",
                "\n",
                "def getting_closest_station (target_lat, target_lon, shape_location):    \n",
                "    shape_location = shape_location[(shape_location.lat<target_lat+0.01) & (shape_location.lat>target_lat-0.01) & (shape_location.long<target_lon+0.01) & (shape_location.long>target_lon-0.01)].copy()\n",
                "    target_point = Point(target_lon, target_lat)\n",
                "    shape_location['distance'] = shape_location.apply(lambda row: target_point.distance(row['geometry']), axis=1)\n",
                "    closest_point = shape_location.loc[shape_location['distance'].idxmin()]\n",
                "    return closest_point.row_id, closest_point.col_id, closest_point.ID\n",
                "\n",
                "\n",
                "def writing_snow_database(target_hru_revised,original_snow_path,target_snow_path): \n",
                "    \n",
                "    snow=pd.read_csv(original_snow_path, skiprows= 1, sep='\\s+')\n",
                "    hru_data=pd.read_csv(target_hru_revised, skiprows= 1, sep='\\s+')\n",
                "    snow = pd.merge(snow, hru_data, left_on='name', right_on='snow', how='right')[['snow', 'fall_tmp', 'melt_tmp', 'melt_max', 'melt_min', 'tmp_lag', 'snow_h2o', 'cov50', 'snow_init']]\n",
                "    snow = snow.rename(columns={'snow':'name'})    \n",
                "    snow = snow.drop_duplicates(subset='name').reset_index(drop=True)\n",
                "    \n",
                "    snow ['fall_tmp']  = 1\n",
                "    snow ['melt_tmp']  = 0.5\n",
                "    snow ['melt_max']  = 4.5\n",
                "    snow ['melt_min']  = 4.5\n",
                "    snow ['tmp_lag']   = 1.0\n",
                "    snow ['snow_h2o']  = 1.0\n",
                "    snow ['cov50']     = 0.5\n",
                "    snow ['snow_init'] = 0.0\n",
                "    \n",
                "    custom_header = 'SNODAS database for Michigan, vahid rafiei'\n",
                "    \n",
                "    with open(target_snow_path, 'w') as file:\n",
                "        file.write(custom_header + '\\n')\n",
                "    snow.to_csv(target_snow_path, mode='a', sep=' ' , index=False)\n",
                "\n",
                "def clean_and_copytree(source, target):\n",
                "    if os.path.exists(target):\n",
                "        shutil.rmtree(target)\n",
                "    shutil.copytree(source, target)\n",
                "\n",
                "def writing_hru_data(hrus_data, target_hru_revised):\n",
                "    custom_header = 'revised hrus, vahid rafiei'\n",
                "    with open(target_hru_revised, 'w') as file:\n",
                "        file.write(custom_header + '\\n')\n",
                "    hrus_data = hrus_data.fillna('null')\n",
                "    hrus_data.to_csv(target_hru_revised, mode='a', header=True, index=False, sep=' ')\n",
                "\n",
                "\n",
                "def adding_statistics_to_snow_sno(BASE_PATH, LEVEL, NAME, MODEL_NAME):\n",
                "    SOURCE_path = os.path.join(BASE_PATH, f'SWATplus_by_VPUID/{LEVEL}/{NAME}/{MODEL_NAME}/Scenarios/Default/TxtInOut')\n",
                "    snow_sno_path = os.path.join(SOURCE_path, 'snow.sno')\n",
                "    snowsno = pd.read_csv(snow_sno_path, skiprows = 1, sep='\\s+')\n",
                "    for i in range(len(snowsno.name)):\n",
                "        \n",
                "        snodas_file = pd.read_csv(os.path.join(SOURCE_path, snowsno.name[i]), skiprows=3, sep='\\s+')\n",
                "        start_date = datetime.datetime(2004, 1, 1)\n",
                "        end_date = datetime.datetime(2023, 12, 31)\n",
                "        \n",
                "        date_range = [start_date + datetime.timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
                "        snodas_file['date'] = date_range\n",
                "        #snodas_file = snodas_file.set_index('date')\n",
                "        snodas_file = snodas_file.replace(-99,np.nan)\n",
                "        \n",
                "        # Add year and month columns for grouping\n",
                "        snodas_file['year'] = snodas_file.date.dt.year\n",
                "        snodas_file['month'] = snodas_file.date.dt.month\n",
                "        \n",
                "        # Group by year and month, then calculate the mean of the specified column\n",
                "        monthly_means = snodas_file.groupby(['year', 'month'])['MR'].quantile(0.5)\n",
                "        \n",
                "        # Now calculate the mean of these monthly means for each month across all years\n",
                "        mean_annual_monthly = monthly_means.groupby('month').mean()\n",
                "        \n",
                "        # mean_annual_monthly will now have the average of each month's mean over the years\n",
                "        \n",
                "        snowsno.loc[i,'MR_Jan']=round(list(mean_annual_monthly)[0], 2)\n",
                "        snowsno.loc[i,'MR_Feb']=round(list(mean_annual_monthly)[1], 2)\n",
                "        snowsno.loc[i,'MR_Mar']=round(list(mean_annual_monthly)[2],2)\n",
                "        snowsno.loc[i,'MR_Apr']=round(list(mean_annual_monthly)[3],2)\n",
                "        snowsno.loc[i,'MR_May']=round(list(mean_annual_monthly)[4],2)\n",
                "        snowsno.loc[i,'MR_Jun']=round(list(mean_annual_monthly)[5],2)\n",
                "        snowsno.loc[i,'MR_Jul']=round(list(mean_annual_monthly)[6],2)\n",
                "        snowsno.loc[i,'MR_Aug']=round(list(mean_annual_monthly)[7],2)\n",
                "        snowsno.loc[i,'MR_Sep']=round(list(mean_annual_monthly)[8],2)\n",
                "        snowsno.loc[i,'MR_Oct']=round(list(mean_annual_monthly)[9],2)\n",
                "        snowsno.loc[i,'MR_Nov']=round(list(mean_annual_monthly)[10],2)\n",
                "        snowsno.loc[i,'MR_Dec']=round(list(mean_annual_monthly)[11],2)\n",
                "        \n",
                "    custom_header = 'snow data with 0.95 quantile average monthly melting rate, Vahid Rafiei 12-19-2023'    \n",
                "    target_snow_path = os.path.join(SOURCE_path, 'snow.sno')    \n",
                "    with open(target_snow_path, 'w') as file:\n",
                "        file.write(custom_header + '\\n')\n",
                "    snowsno = snowsno.replace(np.nan, '-99')\n",
                "    snowsno.to_csv(target_snow_path, mode='a', sep=' ' , index=False)\n",
                "    \n",
                "def creating_SWAT_SNODAS_files(BASE_PATH, NAME, LEVEL, MODEL_NAME, original_MODEL_NAME ):\n",
                "    print(NAME)\n",
                "    SOURCE = f'SWATplus_by_VPUID/{LEVEL}/{NAME}/{MODEL_NAME}/Scenarios/Default/TxtInOut/'\n",
                "    out_file_path = fr'/data/MyDataBase/SWATGenXAppData/SWATplus_by_VPUID/{LEVEL}/{NAME}/{MODEL_NAME}'\n",
                "    try:\n",
                "        Files_to_delete = os.listdir(out_file_path)\n",
                "        Files_to_delete.remove('Scenarios')\n",
                "        for file in Files_to_delete:\n",
                "            os.remove(os.path.join(out_file_path,file))\n",
                "    except Exception as e:\n",
                "        print(e)\n",
                "        \n",
                "\n",
                "    original_source_path = os.path.join(BASE_PATH, f'SWATplus_by_VPUID/{LEVEL}/{NAME}/{original_MODEL_NAME}/Scenarios/Default/TxtInOut')\n",
                "\n",
                "    SOURCE_path = os.path.join(BASE_PATH, SOURCE)\n",
                "    \n",
                "    clean_and_copytree(original_source_path, SOURCE_path)\n",
                "\n",
                "    shutil.copy2('/data/MyDataBase/SWATGenXAppData/bin/swatplus_sno.exe', SOURCE_path)\n",
                "\n",
                "    target_hru      =  os.path.join(BASE_PATH, SOURCE, 'hru.con'      )\n",
                "    target_hru_data =  os.path.join(BASE_PATH, SOURCE, 'hru-data.hru' )\n",
                "    \n",
                "    hrus = pd.read_csv(target_hru, skiprows=1, sep='\\s+'          )\n",
                "    hrus_data = pd.read_csv(target_hru_data, skiprows=1, sep='\\s+')\n",
                "    \n",
                "    for hru_id in range(len(hrus)):\n",
                "        \n",
                "        row, col, ID = getting_closest_station (hrus.lat.values[hru_id], hrus.lon.values[hru_id], SNODAS_stations)   ##### this line is very time consuming.\n",
                "        #print(row, col, ID, hru_id)\n",
                "        output = {}\n",
                "        \n",
                "        out_file_path = os.path.join(BASE_PATH,SOURCE, f'SNODAS{ID}')\n",
                "        \n",
                "        hrus_data.loc[hru_id,'snow'] = f'SNODAS{ID}'  \n",
                "\n",
                "        if os.path.exists(out_file_path):\n",
                "            continue\n",
                "    \n",
                "        variables_name = [x for x in datasets.keys()]\n",
                "        \n",
                "        for variable_name in variables_name:\n",
                "            output[variable_name] = variables[variable_name][:, row, col]\n",
                "        \n",
                "        output = pd.DataFrame(output)\n",
                "        now = datetime.datetime.now()\n",
                "        formatted_date = now.strftime('%D')\n",
                "\n",
                "        custom_header = f'SNODAS modeling output, created by Vahid Rafiei {formatted_date}\\nlat\\tlon\\n{hrus.lat.values[hru_id]}\\t{hrus.lon.values[hru_id]}'\n",
                "\n",
                "        # Write the custom header\n",
                "        with open(out_file_path, 'w') as file:\n",
                "            file.write(custom_header + '\\n')\n",
                "            \n",
                "        # Start date\n",
                "        start_date = datetime.datetime(2004, 1, 1)\n",
                "        dates = []\n",
                "        # Loop to generate dates\n",
                "        current_date = start_date\n",
                "        while len(dates) < len(output):  # continue until the end of the final day\n",
                "            dates.append(current_date)\n",
                "            current_date += datetime.timedelta(days=1)\n",
                "        output['date'] = dates\n",
                "        saving_output(output,out_file_path)\n",
                "\n",
                "    target_hru_revised = os.path.join(BASE_PATH,SOURCE, 'hru-data.hru')\n",
                "    writing_hru_data(hrus_data, target_hru_revised )\n",
                "    \n",
                "    writing_snow_database(target_hru_revised,os.path.join(BASE_PATH, SOURCE, 'snow.sno'), os.path.join(BASE_PATH, SOURCE, 'snow.sno'))\n",
                "\n",
                "LEVEL = 'huc12'\n",
                "NAMES = os.listdir(f'/data/MyDataBase/SWATGenXAppData/SWATplus_by_VPUID/{LEVEL}/')\n",
                "#NAMES.remove('log.txt')\n",
                "\n",
                "MODEL_NAME = 'SWAT_gwflow_snow_MODEL'\n",
                "original_MODEL_NAME = 'SWAT_gwflow_MODEL'\n",
                "\n",
                "for NAME in ['40802010304']:#NAMES:\n",
                "    \n",
                "    original_model_name_path =  os.path.join(BASE_PATH,f\"SWATplus_by_VPUID/{LEVEL}/{NAME}/{original_MODEL_NAME}\")\n",
                "    \n",
                "    if os.path.exists(original_model_name_path):\n",
                "        if NAME=='4060103':\n",
                "            creating_SWAT_SNODAS_files(BASE_PATH, NAME, LEVEL, MODEL_NAME, original_MODEL_NAME )\n",
                "            adding_statistics_to_snow_sno(BASE_PATH, LEVEL, NAME, MODEL_NAME)\n",
                "        \n",
                "        if os.path.exists(fr'/data/MyDataBase/SWATGenXAppData/SWATplus_by_VPUID/huc8/{NAME}/SWAT_gwflow_snow_MODEL'):\n",
                "            print(f'{NAME} is already created')\n",
                "\n",
                "            continue\n",
                "            \n",
                "        creating_SWAT_SNODAS_files(BASE_PATH, NAME, LEVEL, MODEL_NAME, original_MODEL_NAME )\n",
                "        adding_statistics_to_snow_sno(BASE_PATH, LEVEL, NAME, MODEL_NAME)\n",
                "\n",
                "    else:\n",
                "        print('#####################################     ERROR        ERROR        ########################################')\n",
                "        print(f'##################################### model {NAME} does not exists ########################################')\n",
                "        print('#####################################     ERROR        ERROR        ########################################')\n",
                "print('done')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2f7fc65f-c3bd-4112-9363-f63bc029458d",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e74ba1b2-7137-4c30-97f4-ec4849332ff8",
            "metadata": {},
            "outputs": [],
            "source": [
                "variable_name ='melt_rate'\n",
                "output = pd.DataFrame( {'snomlt':np.array(variables[variable_name][:, row, col] )}  )\n",
                "\n",
                "start_date = datetime.datetime(2004, 1, 1)\n",
                "dates = []\n",
                "# Loop to generate dates\n",
                "current_date = start_date\n",
                "while len(dates) < len(output):  # continue until the end of the final day\n",
                "    dates.append(current_date)\n",
                "    current_date += datetime.timedelta(days=1)\n",
                "    \n",
                "output['date'] = dates\n",
                "SNODAS_station = pd.DataFrame(output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3750e499-4c52-4c80-b835-b1ea22a06238",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Assuming SNODAS_station is already defined as per your code\n",
                "# Convert 'date' to datetime if not already\n",
                "SNODAS_station['date'] = pd.to_datetime(SNODAS_station['date'])\n",
                "\n",
                "# Extract month from each date\n",
                "SNODAS_station['month'] = SNODAS_station['date'].dt.month\n",
                "\n",
                "# Group by month and calculate max, min, and median melt rates\n",
                "SNODAS_station = SNODAS_station.dropna()\n",
                "monthly_melt_stats = SNODAS_station.groupby('month')['snomlt'].agg(['max', 'min', 'median','mean'])\n",
                "\n",
                "print(monthly_melt_stats)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7131c75b-f4c4-4d74-bcdc-040feb0a2751",
            "metadata": {},
            "outputs": [],
            "source": [
                "target_snow_path   = os.path.join(BASE_PATH, f'SWATplus_by_VPUID/{LEVEL}/{NAME}/SWAT_gwflow_snow_MODEL/snow.sno')\n",
                "snow_sno = pd.read_csv(target_snow_path, skiprows=1, sep= '\\s+')\n",
                "snow_sno['melt_max']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d64a6faf-3df2-4759-abaf-6b8eabfa1b07",
            "metadata": {},
            "outputs": [],
            "source": [
                "output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "49f4f08c-7ade-467b-9c09-23b873b4e7df",
            "metadata": {},
            "outputs": [],
            "source": [
                "output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d5d04caa-b217-49fc-9e1b-aabdbdbb248a",
            "metadata": {},
            "outputs": [],
            "source": [
                "for NAME in NAMES:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4d7887ca-17ec-4759-913a-4a042e32931a",
            "metadata": {},
            "outputs": [],
            "source": [
                "out_file_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e33d9859-75f0-49a5-abde-33d88ffd9a05",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}