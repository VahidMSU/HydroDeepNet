Development of a scalable system for high-resolution hydrogeological modeling for conterminous United States
1. Executive Abstract
	

Chapters


Chapter 1: Development of a highly scalable system for surface water modeling with SWAT+ for the Conterminous United States.

Chapter 2: Development of machine learning model for improving estimation of groundwater hydraulic properties. 

Chapter 3: Development of process-based modeling informed machine learning.









CHAPTER 1: Development of a process-based modeling Informed machine learning 
 
1. Statement of the problem
Michigan's groundwater (GW) resources are under growing pressure due to increasing demand across various sectors, such as agriculture, public water supply, and industry (Steinman et al., 2022). Meanwhile, climate change has been projected to alter precipitation patterns across the Midwest, intensify rainfall events, and increase evapotranspiration rates, and therefore, contribute to reduced GW recharge and availability (Costa et al., 2021). These challenges may be further exacerbated by the expansion of impervious surfaces and the widespread use of agricultural subsurface drainage, posing threats to the long-term sustainability of Michigan's GW resources, which are crucial for the state's economy, ecology, and human populations.
To effectively address these challenges, a comprehensive approach is needed that integrates the influences of climate change, land use change, and growing demand into the development of efficient conservation management practices. However, two critical issues must be addressed: improved data on groundwater hydraulic property (transmissivity, static head, aquifer depth, etc.). Second, we require the ability to forecast the impact of environmental and anthropogenic activities on GW recharge quality and quantity at a large scale. 
Machine Learning (ML) algorithm has been used widely for predicting GW systems dynamics including mapping the spatial distribution of GW salinity and predicting GW level (Pourghasemi et al., 2020), determining the physical properties of aquifers, and predicting the GW response to drought (Schreiner-McGraw and Ajami, 2021). However, machine learning models for predicting recharge or other parameters require observation data that are not available. The next option is simulation data from process-based modeling that can provide data at a variety of temporal or spatial scales. In the literature, there are numerous examples of physics-informed ML machine learning. For example, Wang et al. (2022) addressed the challenge of assessing spatiotemporal variations of GW contamination under climate change scenarios by developing a physics-informed ML surrogate model called U-Net enhanced Fourier Neural Operator (U-FNO). The proposed model could solve partial differential equations for GW flow and transport simulations at the site scale. Moreover, the hypothesis that theory-guided ML models can effectively model the behavior of a real aquifer has been examined by Adombi et al. (2022). The authors conducted one of the first comparative analyses of numerical modeling, traditional ML, and theory-guided ML for the modeling of GW dynamics. The researchers developed a theory-guided multilayer perceptron model (TgMLP) for simulating aquifers and compared its performance with a numerical model (NumMod) and a traditional multilayer perceptron model (MLP) in simulating GW flow dynamics. They found that the TgMLP model better approximated the solution of the partial differential equation at 70% of the locations than other models.  
However, despite the proven effectiveness of the ability of machine learning and deep learning, the problem with data that these machine learning needs to be fed remains a challenge. Not only that, the scaling of these machine learning to state and continental levels where heterogeneity, resolution, and scaling of processes is a major issue has not been addressed. Therefore, the extent of the ability of machine learning models to forecast future changes (which are exceptionally dependent on the data) has not been addressed. 
	Drawing from the acknowledged challenges and research advancements, we propose a novel data science approach for training ML algorithms based on high-resolution processes-based models to estimate recharge under diverse environmental conditions.  To achieve our goal, we developed a highly scalable system for generating SWAT+ for the Conterminous United States based on the national high-resolution hydrographical dataset (NHDPlus HR). We incorporated more than 600,000 water well information for estimating groundwater properties and groundwater water hydraulic variables to generate high-resolution groundwater models and further used in training machine learning for future prediction. We overall used 47 models across Michigan's lower peninsula to train a deep-learning neural network and examine several aspects of such development. 

Our proposed approach and platform offer numerous advantages for empowering policymakers, and stakeholders with actionable insights for preserving GW resources at the most granular level. The key benefits include:

1- Estimation of GW recharge rates across Michigan under diverse environmental and anthropogenic activities.
2- Identification and prioritization of vulnerable areas for enhancing recharge.
3- The potential to serve as a model for other regions, inspiring broader advancements in 		GW management and sustainability.

This project provides an interdisciplinary approach, integrating hydrogeology, data science, and ML to develop a comprehensive process-based model-guided machine learning. 

The project's nature focuses on developing models that incorporate various factors influencing GW recharge and identifying feasible methods to enhance the state's GW resource management. The objective of the project consists of the following components:
1- Developing a scalable system for creating a high-resolution groundwater-surface model for training machine learning and deep learning models. 
2- Developing a machine learning method capable of learning processes from a process-based model and predicting the future based on the processes. 
3- Estimating Michigan groundwater recharge at high resolution. 




CHAPTER 2: Development of a high-resolution SWAT+ model for the conterminous United States

Regional high-resolution modeling
	Regional and National water resources studies are essential parts of water resources management. Modeling tools used in these studies are normally used to predict the extent and impact of natural and anthropogenic changes on water resources at different levels (Van Zanen and Hughes, 2022). The resolution in hydrological modeling is an important part of model accuracy and reliability. Resolution can be normally defined at different aspects of modeling: 1- the spatiotemporal resolution of data used in the modeling, 2- the spatiotemporal resolution of model discretization, 3- the resolution of processes (the number of hydrological processes that a model can compute and incorporate into overall quality and quantity of water/energy and mass balance estimation). The resolution of discretization and processes in the models are constrained by the resolution of data and the ability of computer software and hardware to solve models. In environmental and earth modeling systems, the common resolution of data for the discretization of spatial heterogeneity ranges from 10m (e.g., DEM, Landuse, soil) to thousands of meters (climate data) and temporal resolution of data ranges from 30 minutes (eg., solar radiation) to daily observation (e.g, streamflow, precipitation) to over weekly, monthly observation for groundwater or water quality. Improving the spatiotemporal resolution of models' discretization and processes is essential to achieve accurate prediction. For example, a study by USGS for modeling regional groundwater-surface water in Michigan (Feinstein et al., 2010) with MODFLOW, found that a scale of 1.5km for special resolution is coarse, and therefore a finer resolution was needed to simulate the hydraulic gradients toward Lake Michigan. While this is not new to the modeling community anymore, other studies also have shown the impact of coarser resolution on misestimating groundwater storage (Sulis et al., 2011). 
	Developing national models with high resolution at all dimensions (spatial, temporal, and processes) has been progressing for years in different research. For example, in the realm of grid-based models that benefit from matrix multiplication and compatibility with GPU acceleration software and hardware, the computational enhancement has led to significant improvement in model resolutions. For example, a distributed physically based groundwater-surface water model, ParFlow CONUS version 2.0 (Yang et al., 2023), has been recently updated with a 1km spatial scale for hydrogeological processing, improved from 4 sq km in the previous version. Similarly, and in the context of the United States, there are several examples of national hydrological models based on statistical or process-based modeling that prepared for CONUS. For example, the US national hydrology infrastructure provides up to 110,000 HRUs for simulating hydrological processes across the US. Moreover, the National Agroecosystem model (NAM) that uses the recent version of the model (SWAT+) (White et al 2022) has been established for CONUS with more than 2 million HRUs. 
	Increasing the discretization resolution at national and regional, however, exponentially increases the number of cell sizes. For example, a regional hydrological model with a geographic extent of 100,000 km2 (typical size of states in the US) with a 30m cell size results in more than 140 million cells, which is more than the number of cells resulting from the division of the globe into 250*250m cells. Therefore, most data sources providing national and continental levels of data have remained at 1 to 4 km2. For example, USGS CONUS04 which provides hydrological data, National Solar Radiation which provides atmospheric data, and PRISM which provides climatological data has remained at 2km*2km resolution (PRISM sells its 800m resolution). Moreover, the size of the national database is a challenge. For example, USGS CONUS404 which currently provides a single raster for daily hydrological data at the CONUS level, has more than 7Tb size. Similarly, the size of each year's database for NSRDB which provides 30 minutes of data for several parameters over a year and over North America, is more than 1.4Tb size for each year.  	Exponential increase in the number of cells, also non-linearly increases the computational costs and if associated with large geographic extent or high-resolution processes (like incorporating anthropogenic changes and agricultural management practices) makes the model impractical to work with, specifically for scientific and research purposes that require thousands of simulations for sensitivity analysis, calibration, and uncertainty analysis. Given such geographic extent, achieving high-resolution modeling requires breaking down the large geographic extent into scalable pieces as has been achieved so far by NAM model by providing SWAT+ models at huc14, huc12, huc8. 
	Normally, at the watershed scale, achieving high accuracy in estimating groundwater head is warranted through 100-250m cell size, as shown in our previous studies for two different watersheds in two different continents, climate and data quality, one in Michigan and the other one in Queensland, Australia that we could achieve almost identical groundwater head simulation comparing to observation. 
	To name a few regional high-resolution models, Čerkasova et al. (2021) developed a transboundary SWAT model for Europe, comprising 11 interconnected sub-models, 9,012 subbasins, and 148,212 HRUs. The US National Agroecosystem Model (NAM), built by SWAT+ has 2,296,974 HRUs for the extent of the United States (White et al., 2022). Both of these examples have 2 characteristics in common: 1 - both examples are built upon NHDPlus V2 (National Hydrographical Dataset Plus Version 2) which has a resolution of 1:100k which is known as moderate hydrographical resolution. 2- both are written scalable to enable the user to extract a portion of the model and run the scenarios. A similar method has been also under development for Phase 7 Chesapeake Bay Watershed based on NHDPlus V2.  It is worth noting that NHDPlus V2 employed in these studies is based on a relatively old NHD data set which has not been updated since 2012. On the other hand, a higher resolution of NHD data called NHDPlus HR has been developed and its first release covers the entire US with 1:24k resolution which provides hydrological data at the neighborhood level. However, employing a higher resolution of hydrographical data should be considered both by the limit of computational power and the resolution of other data. Otherwise, employing higher-resolution database hydrographical data will only cause higher computational costs without achieving better results. Here we represent an approach and associated tools for achieving neighborhood-level resolution in regional hydrogeological studies in the United States with national public data within a reasonable time and computational power.
	We built our hydrographical modeling structure entirely based on NHDPlus HR (the US National Hydrological Dataset High Resolution). The NHDPlus HR has a resolution of 1:24k, and details of hydrograph sequences, and watershed delineation are quite substantial. NHDPlus HR is based on relatively current NHD, at 1:24K or better. The NHDPlus V2 which has been used in NAM, is based on an older, less detailed, 1:100k-scale NHD, which has not been updated since 2012 (Buto and Anderson, 2020). The subset of NHDPlus HR for Michigan LP that we present here consists of more than 120,000 catchments, more than 2000 HUC12, 32 HUC8, and 6 HUC4 covering the state boundary. Using NHDPlus HR data and the SWAT+ model, allowed us to model Michigan water resources at the neighborhood level with the highest quality. To signify the resolution of our works, it is worth comparing it with our previous model for the Huron River watershed (a HUC8 model) in southeast Michigan. Our SWAT model consisted of 198 identifiable channels and 9000 HRUs. Our SWAT+ model for the same watershed has 3100 channels and more than 100,000 HRUs. Collectively, the HUC8 models of Michigan have more than 2 million HRUs. Compared with studies with the same geographic extent, our modeling resolutions provide a resolution.
	Our modeling tool is the SWAT+ model which is written by object-oriented Fortran and provides extra flexibility in code development by the community. The SWAT+ suite of software also consists of QSWAT+ which generates databases and GIS projects of the watershed, and SWAT+ editor which writes and manipulates the model in several ways. The flexibility in SWAT+ compared to SWAT is considerable, especially in terms of using object-oriented programming, reducing axillary files, and improving data management which shaped a cutting-edge architecture for hydrological modeling. Also, SWAT+ has an integrated physically-based groundwater model for simulating groundwater-surface interaction modeling (Bailey et al., 2020) which significantly improved computational time compared to the previous SWAT-MODFLOW model. Also, the SWAT+ model structure has one more hydrographical hierarchy compared to the SWAT model. The SWAT+ model has a basin, subbasin, and watersheds compared to SWAT which has only a basin and subbasin hierarchy. 
	Our model selection (SWAT+) was not only based on our experience working with SWAT but the selection was based on the fact that there are no other alternatives that can beat the SWAT+ model in three respects: 1- compatibility with the US public national database, 2- scalability of the model for generating model for a specific area of interests, 2- dedicated groundwater component for estimating the quality and quantity of groundwater recharge. It is worth noting that currently there is no other viable solution to link a surface water model with MODFLOW (standard groundwater modeling tools in US studies) with a surface water model other than SWAT. Of course, many modules are developed for simulating surface hydrology for MODFLOW like the stream routing package (SR2) (Ou et al., 2013). However, SR2 is only limited to stream routing and groundwater and surface water interaction at the river interface. For estimating groundwater recharge for MODFLOW other modeling tools like Precipitation Runoff Modeling System (NHM-PRSM) (Leavesley and Stannard, 1995) has been developed, however, the NHM-PRSM model is based on NHDPlus Version 1, which has only 110,000 HRUs for the entire US. 


CHAPTER 3: Groundwater Hydraulic Parameter Estimation with Kriging, Machine learning and deep learning

Summary
	In this study, we aim to predict annual groundwater recharge across Michigan. Our objectives are to quantify the impact of land use change, climate change, and management practices on the annual groundwater recharge rate with high resolution. To address this objective, we constructed 50 SWAT+ models along with 50 MODFLOW-NWT models. Once these models were established, calibrated, and validated, we used the recharge simulated by SWAT+ models in the machine learning models.   
	The creation of a predefined watershed in SWAT+ necessitates preparing specific inputs and structures for streams, watersheds, and sub-basins. These include defining hydrograph sequences, lake-stream connections, and sub-basin boundary extent. We have automated an approach for generating SWAT+ predefined hydrographical models based on the NHDPlus HR2 database. Briefly, our approach initiates by aggregating data from the NHDPlus databases, performing a series of general preprocessing tasks, and saving the data in a portable size. Following this, we extract a subset of data for a desired watershed or region. The design of this data extraction process enables us to create SWAT+ models covering various geographic extent from a single HUC12/HUC8, to multiple HUC12/HUC8. As a result, by utilizing a combination of HUC12, we can extract predefined watersheds for the drainage area of any desired streamflow station. This design also facilitates the extraction of predefined watersheds for any location covered by the NHDPlus Database. 
	Upon extracting the subset for HUC12s, the data undergoes further modification to align with the SWAT+ predefined watershed rules. This entails a multi-step revision process, beginning with modifying stream sub-basin IDs to adhere to the one-outlet-rule required by the SWAT+ model for sub-basin definition, followed by defining lake connections with streams and then undertaking a series of additional modifications to define minimum areas for sub-basins, ensure quality control, and save the data for transfer to the QSWAT+ environment. Additional requirements for the predefined models were extracted for US national databases for land use (from NLCD), soil (from gSSURGO), and DEM (from USGS). For climate data, we utilized the PRISM dataset with a 4km resolution spanning from 1990 to 2020. The PRISM data was extracted from binary rasters, stacked, and saved in NumPy arrays, with each point in the database representing the coordinate of the centroid of the raster cells. This database, along with QSWAT+ and SWAT+ editors, is assembled and used for writing SWAT+ model input files/tables. 
	In this study, we utilized this workflow to extract predefined watersheds for all streamflow station drainage areas within the Michigan Lower Peninsula, adhering to the following criteria: 1) Only streamflow stations with a maximum drainage area of 15,000 square kilometers were used; 2) Stations selected had less than a 10% data gap between 1999 to 2020. By this definition, 50 streamflow stations were selected for creating their SWAT+ model. The first criterion aimed to limit the number of HRUs and avoid overly large models that pose challenges in solving. It should be noted that utilizing NHDPlus HR2 necessitates the use of a 30m DEM, leading to a fine-scale model. However, when considering the categories of land use, soil, and slope, the combination results in an excessive number of HRUs. Various strategies can be employed independently or concurrently to mitigate this issue. One method involves dissolving smaller streams into larger ones to reduce the number of watersheds within sub-basins. We opted to maintain data integrity and did not pursue this route. Alternatively, a coarser resolution for land use and soil data can be adopted. We achieved this by resampling databases from 30m to 250m, employing a majority area approach for aggregation. Consequently, the maximum number of HRUs was capped at approximately 50,000, while the minimum was 3,000 for smaller models. Employing these strategies enabled the extraction of 50 SWAT+ models, collectively 1 million HRUs that represent surface water hydrological cycles within Michigan's Lower Peninsula.
	In this study, once the SWAT+ models were created, we devised an additional workflow to conduct sensitivity analysis, followed by parameter calibration. For the sensitivity analysis, we employed the Morris approach, entailing a maximum of 600 model evaluations through 4 trajectories for 30 parameters. For parameters calibration, we utilized a Bayesian optimizer. We set two termination criteria for the Bayesian model: 1) a maximum of five iterations when no better result is found, and 2) a maximum of 30 iterations, equating to 1500 model evaluations. Each iteration of the Bayesian model comprises 50 parallel processes. The calibration/sensitivity objective function was considered the sum of NSE for monthly (incremental rate) and daily streamflow rate, with monthly incremental rate used for scoring the model performance for the total monthly discharge and daily aimed to score hydrograph fluctuations. 
	The workflow was designed to initially perform the sensitivity analysis, subsequently, eliminate parameters with negligible impact on model inputs, and then feed the refined sensitivity results as initial starting points to the Bayesian optimizer, thereby ensuring a robust sampling as achieved through Morris. Both the sensitivity analysis and calibration were performed utilizing parallel processing. The model evaluations from the sensitivity analysis as well as the surrogate model of calibration were saved to be used for further refining the calibration of models if their performance after calibration was not satisfactory. Once the calibration process was deemed satisfactory, we performed a 20-year validation period (2000-2020). These analyses were conducted on our server equipped with 256 CPU cores and 512 GB DDR5 RAM with Windows server OS. Given the server's capacity, we managed to execute the sensitivity and calibration processes for five different watersheds simultaneously, leveraging a nested parallel processing pool.
	Once the SWAT+ models were created, we developed an additional workflow to generate MODFLOW-NWT models for the SWAT+ models using the FloPy library. For this endeavor, we utilized approximately 800,000 well observation data points from the Michigan Lower Peninsula. This observation data encompasses water table depth, horizontal and vertical hydrologic conductivity, and aquifer thickness for two layers representing the glacial aquifer and deep aquifer systems. By employing Empirical Bayesian Kriging (EBK), we interpolated this water well observations data to create consistent rasters covering the entire Michigan Lower Peninsula. Although various approaches exist for ordinary kriging, we found EBK to be the most reliable and robust in comparison to others. We conducted the EBK at both 30m and 250m resolution; however, we later discovered that the 250m resolution could provide comparable groundwater head prediction to the 30m resolution but with significantly lower computational demand. Groundwater layer discretization was also carried out to divide the first thickness into two layers, and the second thickness into three layers. Utilizing the rivers and lakes input from the SWAT+ models, the river and drain packages were employed to define boundary conditions for the MODFLOW models. Initially, we used available recharge estimates for the state of Michigan to verify the MODFLOW models' performance, and then we used SWAT+ recharge estimates to verify our recharge estimates.  

Introduction
	Regional and continental groundwater modeling is essential for ensuring water security and effective groundwater resource management (Lancia et al., 2022). Accurate prediction of hydraulic characteristics—such as transmissivity, aquifer depth, and static water level—is integral to reliable groundwater flow simulation and informed decision-making (Merino et al., 2022). Although well-established methods exist for modeling groundwater systems, a high degree of uncertainty continues to plague hydraulic characteristic estimations at broader scales. This is especially true for models relying solely on indirect measurement techniques, like inverse modeling used by Zell and Sanford (2020) for the Contiguous United States (CONUS). Such models often offer non-transferable results owing to the assumptions and constraints involved, including but not limited to, spatial resolution, accuracy of recharge estimates used for model calibrations, as well as model layers/components configurations. Consequently, there's a compelling need for more accurate methods for estimating hydraulic characteristics that reduce model-specific uncertainties and provide consistent coverage of data for cross-platform regional and continental groundwater modeling.
	Various interpolation methods for spatial data exist, broadly categorized as deterministic which is often a simple distance inverse function, and probabilistic such as machine learning, deep learning, and kriging that root in statistical theory. Kriging is extensively and historically used in groundwater modeling studies but comes with strong assumptions. For instance, kriging methods assume the data originates from a specific distribution (often Gaussian) modeled by the estimated semi-variogram. These assumptions occasionally fail in real-world applications, leading to alternative methods like Empirical Bayesian Kriging (EBK), which employ many semi-variogram models rather than a single semi-variogram (Krivoruchko, 2012). However, even EBK underestimates the uncertainty of the estimated mean value when the number of samples is small or overestimates the prediction uncertainty when the density of observation is different across the spatial domain (Krivoruchko and Gribov, 2019), which is a common characteristic in groundwater observation data. 
	On the other hand, while machine learning models do not make specific distributional assumptions like Kriging, they often struggle with capturing spatial continuity inherently. To address this limitation, some machine learning techniques, such as regression models, have employed kernel techniques like the Radial Basis Function to approximate spatial relationships. However, deep learning models, particularly Convolutional Neural Networks (CNNs), have been shown to outperform the RBF kernels by effectively capturing complex spatial dependencies (Goodfellow et al., 2016), thus potentially offering a superior tool for geospatial analysis. However, so far, the application of machine learning, overwhelmingly dominated by random forest, as well as deep learning, has been limited to groundwater potential mapping and water quality metrics in groundwater where the spatial continuity is less important than other indicators such as the source of pollution and the distance to the source of pollution.
	This study aims to critically evaluate the efficacy and robustness of multiple computational techniques—Random Forests, Gradient Boosting, Convolutional Neural Networks, and Kriging—in estimating high-resolution groundwater hydraulic characteristics for regional, national, and continental-scale applications. In our study, we recognized that each computational approach comes with its own set of assumptions and uncertainties. Therefore, we also employed a hybrid approach, integrating machine learning, deep learning, and krigging to achieve a more reliable estimation. We apply our methodologies to Michigan State, using a dataset comprising 800,000 water wells, with lithological data and pumping tests, which represent three homogeneous stratified groundwater layers of varying thickness. For machine learning and deep learning applications, we also incorporated relevant information for hydrographical, and surface terrain features. This comprehensive dataset enables us to rigorously test our methodologies and assumptions. Accompanying the study are the collected/generated datasets and Python scripts, offering a complete package for replicating and extending this study
Background and Literature Review:
“Improving Results of Existing Groundwater Numerical Models Using Machine Learning” by MDPI1. The paper presents a review of papers specifically focused on the use of both numerical and machine learning methods for groundwater level modelling. In the reviewed papers, machine learning models (also called data-driven models) are used to improve the prediction or speed process of existing numerical modelling (Di Salvo, 2022)
“Estimation of Unconfined Aquifer Transmissivity Using a Machine Learning Model” by Springer1. The paper presents a machine learning model to estimate the aquifer hydrodynamic parameters (hydraulic conductivity, transmissivity, specific yield, and storage coefficient) for proper groundwater resource management (Dashti et al., 2023)
GIS-based groundwater potential mapping using boosted regression tree, classification and regression tree, and random forest machine learning models in Iran | SpringerLink : The study aims to produce maps of groundwater spring potential in the Koohrang Watershed, Iran, using three machine learning models: boosted regression tree (BRT), classification and regression tree (CART), and random forest (RF) (Naghibi et al., 2016)
Groundwater Storage Loss Associated With Land Subsidence in Western United States Mapped Using Machine Learning - Smith - 2020 - Water Resources Research - Wiley Online Library: The paper uses machine learning to map land subsidence caused by groundwater extraction in the western United States, using various input data sets such as evapotranspiration, land use, sediment thickness, and InSAR and GPS measurements. The paper applies a random forest model to estimate the average annual subsidence rate from 2015 to 2016, and the corresponding groundwater storage loss from confined aquifers, over a 2-km grid. The paper finds that the western United States lost about 2.0 km3/yr of groundwater storage due to subsidence, with 75% of the loss occurring in California. The paper also identifies the key drivers of subsidence, such as sediment thickness, agricultural density, and precipitation. The paper demonstrates the usefulness of machine learning for estimating subsidence at a large scale and assessing the impacts of land use and climate change on groundwater resources (Smith and Majumdar, 2020)
Nation-wide estimation of groundwater redox conditions and nitrate concentrations through machine learning - IOPscience : The paper uses random forest (RF) and quantile random forest (QRF) to estimate redox conditions and nitrate concentration in groundwater across Germany, based on spatial environmental predictors and monitoring data. RF model performance and predictors: The RF model achieves a good prediction accuracy (R2 = 0.52) for nitrate, with redox conditions, hydrogeological units and arable land as the dominant predictors. The QRF model shows large uncertainties (MPI = 53.0 mg l−1)4. First nation-wide assessment of nitrate: The paper provides the first data-driven spatial distribution of groundwater nitrate concentrations for Germany, which can help to protect water resources and develop mitigation strategies (Knoll et al., 2020)
A hybrid machine learning model to predict and visualize nitrate concentration throughout the Central Valley aquifer, California, USA - ScienceDirect: Hybrid machine learning model for nitrate prediction: The paper uses a boosted regression tree (BRT) method to estimate nitrate concentration in groundwater throughout the Central Valley aquifer, California, USA, based on various input data and model outputs.
Data and methods: The paper uses nitrate measurements from over 6000 private and public supply wells, and 145 predictor variables representing land use, climate, soil, hydrogeology, nitrogen loading, redox conditions, groundwater flow, and groundwater age. The paper applies cross validation and variable reduction techniques to select the best BRT model. Model evaluation and comparison: The paper assesses the accuracy and performance of the BRT model using training and hold-out data sets and compares it with other methods such as ordinary kriging, universal kriging, and multiple linear regression. The paper finds that the BRT model has the highest prediction accuracy and reliability among all methods. 3D mapping and visualization: The paper produces a 3D map of nitrate concentration at different depths of the aquifer using interpolation and visualization software. The paper also maps groundwater age and redox variables to show their influence on nitrate distribution. The paper provides insights into the spatial variability and vulnerability of groundwater nitrate in the region (Ransom et al., 2017)
Bayesian machine learning ensemble approach to quantify model uncertainty in predicting groundwater storage change - ScienceDirect: The paper proposes a Bayesian machine learning ensemble approach to predict groundwater storage change and quantify model uncertainty in the San Joaquin River Basin, California, using three machine learning models: artificial neural network (ANN), support vector machine (SVM), and response surface regression (RSR). The paper uses outputs from a calibrated fine grid version of C2VSim model as input-output data sets for training and testing the machine learning models, which include agricultural water demands, surface water and groundwater supplies, water year types, and groundwater storage change. The paper applies Bayesian model averaging (BMA) to combine the predictions of the machine learning models, evaluate their performance, and assign them weights and variances based on an expectation-maximization algorithm. The paper also compares two types of BMA-weighted model predictions: BMA subregion and BMA region. The paper demonstrates that the machine learning models have high prediction accuracy and computational efficiency, and that the BMA-weighted model predictions are more reliable and robust than single-model predictions. The paper also suggests that agricultural irrigation pumping is the main driver of groundwater storage depletion in the region. The paper provides a useful tool for supporting sustainable groundwater management under uncertainty. The paper demonstrates that the machine learning models have high prediction accuracy and computational efficiency, and that the BMA-weighted model predictions are more reliable and robust than single-model predictions. The paper also suggests that agricultural irrigation pumping is the main driver of groundwater storage depletion in the region. The paper provides a useful tool for supporting sustainable groundwater management under uncertainty (Yin et al., 2021)
Machine learning algorithms for modeling groundwater level changes in agricultural regions of the U.S. - Sahoo - 2017 - Water Resources Research - Wiley Online Library: Machine learning for groundwater level prediction: The paper proposes a hybrid artificial neural network (HANN) model to estimate seasonal groundwater level change in two agricultural regions of the U.S., the High Plains aquifer (HPA) and the Mississippi River Valley alluvial aquifer (MRVA), using various input data sets such as climate, irrigation, and streamflow. Input variable selection and preprocessing: The paper uses a novel method to select and transform the input variables based on singular spectrum analysis (SSA), mutual information, and genetic algorithm, which can capture the nonlinear dependencies and reduce the redundancy among the inputs. Model evaluation and comparison: The paper assesses the performance and uncertainty of the HANN model using training, cross-validation, and testing data sets, and compares it with other empirical models such as multivariate linear regression (MLR) and multivariate nonlinear regression (MNLR). The paper finds that the HANN model has the highest prediction accuracy and reliability among all models. Relative importance of model inputs: The paper applies the connection weight approach to quantify the influence of each input variable on groundwater level change. The paper shows that irrigation demand is the most important input for a majority of the wells in both aquifers, followed by temperature, streamflow, and climate indices (Sahoo et al., 2017)

Convolutional Networks
Convolutional networks (LeCun, 1989), also known as convolutional neural networks or CNNs, are a specialized kind of neural network for processing data that has a known, grid-like topology. The name “convolutional neural network” indicates that the network employs a mathematical operation called convolution. Convolution is a specialized kind of linear operation. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.
Research into convolutional network architectures proceeds so rapidly that a new best architecture for a given benchmark is announced every few weeks to months, rendering it impractical to describe the best architecture in print. However, the best architectures have consistently been composed of the building blocks described here.

X. Methodology
In the Methodology section, we outline the key elements that constitute the core of this study. Initially, we detail the types of data collected and their sources. Next, we delve into the kriging technique employed for pre-processing groundwater-related variables. Finally, we elucidate our approach for deploying a machine learning framework to analyze the data and derive insights.


Figure X: flowchart of the processes took place to generate groundwater hydraulic parameters.

Data sources
Table: Data sources

Features included in Machine learning.
*The static water level (SWL, in ft) recorded in the database is defined as the depth from the land surface to the water table. 

X. Input data inspection
	The Figure X presents a series of Quantile-Quantile (Q-Q) plots for groundwater transmissivity and aquifer depth, each aiming to assess the normality of the data distribution for the respective parameter. Each subplot features a histogram with a Kernel Density Estimate (KDE) in blue, overlaid on the primary Y-axis, which represents the data density. This offers an initial visualization of the data distribution for quick assessment. On the secondary Y-axis, red dots represent "Sample Quantiles," which are the sorted observed values of the parameter being examined. These are plotted against the theoretical quantiles of a standard normal distribution, calculated based on the mean and standard deviation of the observed data. The closer these points lie along a 1:1 line, the more the data adheres to a normal distribution.


Figure X: Quantile-Quantile (Q-Q) Plots for Groundwater Hydraulic Parameters. Each subplot displays a histogram with a Kernel Density Estimate (KDE) in blue on the primary Y-axis, serving as a quick visual assessment of data distribution for a specific parameter. The red dots on the secondary Y-axis represent the Sample Quantiles—sorted observed values—plotted against theoretical quantiles from a standard normal distribution. The proximity of the red dots to a straight line provides an indication of the data's normality, crucial for subsequent statistical analyses and model assumptions.

	The table presents a detailed breakdown of land use types within the grid cells of the designated study area. The grid cells count indicates the number of cells associated with each specific land use category, providing a quantitative measure of spatial extent. The percentage column reflects the proportion of each land use type in relation to the total grid cells analyzed, which sum to 1,768,931. The data reveals that the dominant land use type is 'Cultivated Crops,' occupying 30.4% of the total area, followed by 'Deciduous Forest' at 19.7%, and 'Woody Wetlands' at 16%.
Table X: Distribution of Land Use Types across the Michigan Lower Peninsula based on NLCD 

X. Database Preparation
	The preparation of the database was a multi-faceted process that integrated a wide array of geospatial and observational data. The initial step involved the creation of a fishnet grid comprising 2.6 million cells, each with a size of 250*250 sqm, arranged in 1848 rows and 1457 columns. Firstly, a spatial join operation was performed to associate these grid cells with key hydrographical features such as NHD flowlines, waterbodies, and catchments. This allowed us to merge the grid cells with NHDPlus attribute tables to obtain essential hydrographic parameters, including attributes like water body types, flowline characteristics, catchment elevations (maximum and minimum), slope metrics, stream levels, and stream orders. Similarly, spatial joins were carried out to link grid cells with subsurface characteristics, such as soil categories, glacial land systems, aquifer characteristics of glacial drifts, and general geologic units. For administrative boundaries, county-level vector data were incorporated, which linked the grid cells to 68 counties in the lower peninsula, resulting in approximately 1.7 million active cells. 
	Groundwater observation data was then integrated with the grid cells through another spatial join. The data was grouped by grid index, and median values were calculated for all numerical columns. The reason for selecting the median value within grid cell was that the quality and location of Wellogic data is generally worse than that of the traditionally collected data especially because of the well locations that were recorded based on address matching, before the global positioning system technologies became popular. Also, the water wells in the database are drilled by different drillers. Because of this, data quality from different drillers is different. Therefore, this median value could better serve as a representative measure of the hydraulic characteristics of each grid cell. This resulted approximately 350,000 cells being populated with groundwater parameters. 
	Raster data, such as land use and digital elevation maps, were resampled to a 250-meter resolution. These raster layers were then overlaid on the grid centroids to append land use types and elevations to each cell. Finally, categorical variables like flow and water body types, aquifer characteristics, glacial systems, and geological units were label-encoded (using one hat encoder) to prepare the database for machine learning applications. The final database brings together a wide range of hydrological, hydrographical, and morphological characteristics for krigging and machine learning algorithms. Top of Form
X. K-fold cross-validation:
K-fold cross-validation is a popular technique used to evaluate the performance of a machine-learning model. It involves dividing the dataset into "K" equally sized folds or subsets. The model is then trained on "K-1" folds and tested on the remaining fold. This process is repeated "K" times, with each fold serving as the test set exactly once. The final performance metric is the average of the performance metrics obtained from each iteration. The folds can be prepared via grouping k-fold method, stratified k-fold method, or random k-fold method. 
X. Nested Cross-Validation for Robust Groundwater Parameter Prediction
Single K-fold cross-validation provides an unbiased estimate of model performance but is susceptible to overfitting on the validation set when hyperparameters are tuned. This is because the same fold might be repeatedly used for validation and hyperparameter tuning, leading to overly optimistic performance estimates. In contrast, nested cross-validation separates the data used for hyperparameter tuning and performance evaluation, providing a more generalizable performance metric. The outer loop assesses how well the model generalizes to new data, while the inner loop focuses on hyperparameter tuning.
In our case, to construct a robust machine learning model capable of predicting groundwater parameters across 68 counties in the lower peninsula of Michigan, a nested cross-validation approach was employed. Our approach involves two model evaluation protocols: an outer loop that uses Group K-Fold cross-validation to ensure generalizability across different counties, and an inner loop using standard K-Fold cross-validation for hyperparameter optimization. Note that we are not aware of any research finding that suggests that the number of folds in the outer cross-validation loop and the number of folds in the inner cross-validation loop need to be the same or different (Krstajic et al., 2014). 
X.1. Outer Loop: Group K-Fold Cross-Validation: In the outer loop, the dataset was partitioned based on the counties, amounting to 68 groups. A 4-fold Group K-Fold cross-validation was employed, effectively dividing the 68 counties into four unique groups, each containing 17 counties. In each iteration of the outer loop, three groups (comprising 51 counties) were utilized for training (passing the training dataset to the inner for hyperparameter tuning), while the remaining group of 17 counties was reserved for testing the model performance for the best parameter found during hyperparameter optimization.
X.1. Inner Loop: Optuna-Driven Hyperparameter Optimization: Within the inner loop, hyperparameter tuning was conducted on the training set from the outer loop, which consisted of the data from 51 counties. Here, a standard 5-fold K-Fold cross-validation was employed. Optuna, an hyperparameter optimization framework (Akiba et al., 2019), was used for the automated hyperparameter tuning of the machine-learning models. During each fold in the inner loop, Optuna searched through the hyperparameter space to identify the optimal set of hyperparameters that minimizes the objective function (in this case Kling Gupta efficiency (Gupta et al., 2009)). 
X.1. Model Evaluation: After hyperparameter optimization, the best-performing model parameters were selected, and the model was retrained on the entire training set from the outer loop. Subsequently, the model was evaluated on the reserved test set of 17 counties to gauge its performance. This process was repeated for each of the four folds in the outer loop, providing a robust measure of the model's ability to generalize across different counties.
X.1. Final Machine Learning Model: The model parameters that demonstrated the most robust performance across the four test sets in the outer loop were selected as the final model parameters. This final model serves as a reliable tool for predicting groundwater parameters across any of the 68 counties in the lower peninsula of Michigan.
X. Utilization of the Radial Basis Function (RBF) Kernel in Capturing Spatial Correlations
The Radial Basis Function (RBF) kernel is an instrumental mathematical function extensively employed in machine learning applications to gauge the similarity between data points in a transformed feature space. The current study elucidates the mathematical structure and interpretational significance of the RBF kernel, particularly in the context of spatial data analysis. When employed in the analysis of spatial coordinates (e.g., latitude and longitude), the RBF kernel effectively encapsulates spatial relationships and patterns. Its potential applications span numerous scientific domains, including environmental science and geospatial analysis. In these contexts, it excels in modeling local interactions, correlations, and spatial continuity.
X. 1. Mathematical Formulation
The mathematical representation of the RBF kernel function is given as follows:

Where  signifies the kernel function value between data points x and y,  is a hyperparameter that controls the decay rate of the similarity with distance and  is the squared Euclidean distance between the data points x and y.
Interpretational Framework
The RBF kernel accomplishes several core objectives in data analysis including:
Distance-to-Similarity Transformation: The function converts the squared Euclidean distance into a similarity measure. The proximity of data points in the original space is positively correlated with their similarity in the transformed space.
Decay Functionality: The hyperparameter  modulates the sensitivity of the kernel. A smaller  yields a broader similarity scope, whereas a larger  results in a rapid decay of similarity with increased distance.
Computational Approaches: The RBFSampler function in the sci-kit-learn library serves as a computational approximation of the exact RBF kernel. It offers the benefit of computational efficiency, particularly when dealing with large datasets while maintaining the essential properties of the exact kernel. 
X. Empirical Bayesian Kriging
Empirical Bayesian Kriging (Krivoruchko, 2012) builds upon classical Kriging methods to offer a probabilistic approach to spatial interpolation. While classical Kriging provides "best linear unbiased estimators," Bayesian Kriging employs a Bayesian framework to capture and quantify the uncertainties associated with spatial predictions more comprehensively.
Model Framework: The method starts by defining a stochastic spatial model, often in terms of a Gaussian Process, which incorporates both observed data and unknown parameters. Unlike classical Kriging, Bayesian Kriging allows for a flexible model specification that can include random effects, covariates, and other complex structures.
Prior Distribution: One of the key features of Bayesian Kriging is the incorporation of prior beliefs about the unknown parameters (e.g., variogram parameters) through a prior distribution. This prior distribution is updated with observed data using Bayes' theorem to yield a posterior distribution.
Posterior Distribution: The resulting posterior distribution provides not only point estimates but also probabilistic uncertainty measures (e.g., confidence intervals) for the spatial predictions. This enables a richer, more nuanced understanding of the spatial process under investigation.
Computation: Computational methods such as Markov Chain Monte Carlo (MCMC) or Variational Inference may be used to sample from the posterior distribution. These computational techniques allow Bayesian Kriging to handle larger, more complex datasets and models efficiently.
Advantages: Uncertainty Quantification: Provides a fuller picture of uncertainty, accounting for both data variability and model uncertainty.
Model Flexibility: Allows for complex model specifications, including non-stationarity and incorporation of external covariates.
Robustness: More robust to variations in data density and data irregularities, as it leverages prior information.
Applications: Bayesian Kriging is particularly useful in fields such as hydrology, environmental science, epidemiology, and geosciences, where understanding uncertainty is as crucial as prediction accuracy.
Conclusion: Bayesian Kriging offers a powerful and flexible approach for spatial data interpolation, extending the capabilities of classical Kriging methods. Its probabilistic nature allows for a more comprehensive understanding of spatial processes and associated uncertainties, making it a valuable tool for scientific research.
X.1. Application of EBK in this study
	In this study, we utilized EBK to interpolate groundwater data for each county within Michigan to achieve an initial but consistent coverage of groundwater parameters across the state. This enabled us to interpolate critical groundwater hydraulic data, including horizontal and vertical hydraulic conductivity, static water wells, and groundwater layer depth. The data parameters are summarized as follows:
	Before proceeding with the kriging process, we performed data preprocessing to ensure the data's reliability and accuracy. Firstly, data with a Static Water Level (SWL) exceeding 520 meters, which corresponds to the maximum elevation of Michigan, was excluded. Moreover, for each county and parameter, any data that fell outside the 95th percentile range was discarded. To enhance prediction accuracy at the county borders, a 5km buffer was implemented. This strategic buffering incorporated water wells from neighboring counties into the kriging analysis, thereby minimizing edge-related errors after the clipping and cleaning processes.
	EBK necessitates the selection of various parameters such as the Maximum number of points in each local model, Local model area overlap factor, number of simulated semi-variograms, cell size, and semi-variogram model type. Although the impact of different combinations of these parameters is comparatively less critical in EBK than other kriging methods, we conducted a random search with various parameter combinations to ensure optimal interpolation. The chosen product was the one with the lowest root mean square error between observation and predictions. For instance, the maximum local points value was selected from values between 64 to 256. The overlap factor was a random value between 1 to 3 in increments of 0.1. The number of semi-variograms was chosen from three options: 100, 200, 300. The semi-variogram type was always set to exponential, and the transformation type was set to empirical. Along with this, we also generated the standard error of kriging for each county. The resulting raster files were then smoothed using the Focal Statistics tool, utilizing a 3x3 cell rectangular neighborhood size.
	Upon generating and smoothing individual tiles for each county, we utilized the county boundaries to perform an extraction by mask, thereafter, creating a consistent mosaic from the county raster’s for each water well parameter.








Methodology
Data Sources used in this study:




Task 1, Climate change, land use, and conservation practice scenarios:
Climate change scenario: We will employ the Generalized Extreme Value (GEV) theory (Coles et al., 2001; Katz et al., 2002; Towler et al., 2010) to develop the historical and potential future intensity-duration-frequency (IDF) curves of rainfall and design recharge of various durations and intensity across the Michigan state, considering both stationary (constant in time) and non-stationary climatic regimes. The non-stationary climate for the 20th and 21st centuries will be addressed by incorporating temporal trends (i.e., time as a covariate) in the location, scaling, and shape parameters of the GEV distribution. The better-performing method will be adopted for estimating recharge in the historical and future periods. Each design recharge's 95% confidence limits will be computed to account for uncertainties. Historic climate (e.g., precipitation and temperature) data for 1900-2021 will be obtained from NOAA (NCEI, 2022) and open-access regional' databases. Future (2025-2100) climate projections will be obtained for different representative scenarios (e.g., RCP 4.5 and 8.5) from the statistically downscaled Multivariate Adaptive Constructed Analogs (MACA; Abatzoglou and Brown, 2012) and Localized Constructed Analogs (LOCA; Pierce et al., 2014) datasets. We will also explore the North American Regional Climate Change Assessment 12 Program (NARCCAP, 2022), which has dynamically downscaled projections of various GCMs nationwide using regional climate models. We will use the dataset that best represents our study area's historical and future design recharge. 
Land use change scenarios: we will employ two primary datasets for representing the current and baseline of land use/land cover across Michigan. The first dataset is 2019 Cropland Data (USGS., 2019) which we already used to calibrate and validate the GW-SW model of the representative watershed. The baseline dataset for analyzing land use change will obtain from Michigan Natural Features Inventory (MNFI). The MNFI was developed based on a survey performed by the General Land office in the mid-1800 and contained 30 different landcover classes. The dataset is available through the MNFI website: Vegetation circa 1800 - Michigan Natural Features Inventory (msu.edu) 

Task 2,  Development of the Hydro-informed Machine Learning Recharge (HMLR) Model
	In this task, we will develop a theory-guided multi-layer perceptron (MLP) model, referred to as the Hydro-informed Machine Learning Recharge (HMLR) model. The HMLR learning is constrained not only by observational data, such as GW head and streamflow rate but also by the theory governing GW flow system and surface water (SW) processes, including evapotranspiration, runoff, lateral flow, infiltration, and percolation.
	In the initial phase of this study, we will focus on data preparation and model integration. We will collect preprocess observational data for GW head and streamflow rate across Michigan and obtain input data for variables such as precipitation, temperature, evapotranspiration, land use, soil type, and topography (see Table 2). We will then assemble the required parameters for governing equations from SWAT and MODFLOW. Subsequently, we will develop a hybrid machine learning framework that incorporates the major governing hydrological and hydrogeological equations from SWAT and MODFLOW, including the three-dimensional GW flow equation and the SCS-CN method for infiltration and runoff. This framework will also integrate snowmelt, plant phenological development, and actual evapotranspiration processes from the SWAT model and feature a custom multi-layer perceptron (MLP) machine learning algorithm, forming the foundation of HMLR model. 
	We will train the HMLR model using collected observational data and governing equations, implementing a loss function that minimizes differences among the following components: 1) observed GW head and streamflow rate vs the HMLR-simulated GW heads and streamflow; 2) the residuals of the mass conservation equation defined for the HMLR model; and 3) SWAT-MODFLOW simulation results (from the scenarios defined in task 1) for recharge and the recharge simulated by HMLR model. Optimization techniques, such as stochastic gradient descent, will be applied to iteratively update the trained model's parameters and improve performance. Figure 1 shows the conceptual diagram of training HMLR model.


Figure 2. Schematic diagram of the HMLR model

Task 3: Validation, Performance Evaluation, and Application of the HMLR Model
	In this task, we will validate, evaluate, and apply the HMLR model to ensure its reliability, accuracy, and robustness in predicting recharge patterns under various spatiotemporal conditions. First, we will divide the available data into training and testing datasets. The training dataset will be used for model development, while the testing dataset will assess the model's performance in predicting recharge patterns for unseen data.
	To evaluate the model's performance, we will use various statistical measures, such as the coefficient of determination (R2), mean absolute error (MAE), and root mean square error (RMSE). To further ensure the robustness of the HMLR model, we will employ k-fold cross-validation techniques. This process involves partitioning the data into k equally sized subsets, training the model on k-1 subsets, and validating it on the remaining subset. Repeating this process k times, with each subset serving as the validation dataset once, will provide an unbiased estimate of the model's performance.
	In addition, we will enhance the model's spatiotemporal prediction capacity by strategically splitting climate and land use datasets for training and testing. For instance, we will divide rainfall data into dry years for training and wet years for testing. Similarly, we will split land use data into historic, current, and future datasets for training and testing. Numerous combinations of land use and climate data will be created and tested to ensure the HMLR model's robustness across various spatiotemporal conditions.

Task 4, Predictive model creation

	After validating the HMLR model's performance, we will apply it to predict monthly recharge patterns under various climate change, land use change, and management practice scenarios across Michigan. The insights gained from these predictions will help inform water resource management strategies, enabling decision-makers to implement effective measures for sustainable groundwater management under changing environmental conditions.	
	To ensure maximum accessibility and scalability, the HMLR model will be deployed as a cloud-based web application, leveraging modern web development frameworks such as ReactJS and NodeJS, and scalable cloud infrastructure like AWS as provided by MSU cloud services. The web application will be built using modern web development frameworks like ReactJS and NodeJS, with a focus on performance, scalability, and user experience. The website will feature an intuitive user interface, enabling users to input relevant parameters and generate predictions on groundwater recharge. The tool will incorporate the HMLR machine learning models developed by our team, which will be continuously updated with the latest data using automated pipelines and data integration techniques like ETL (Extract, Transform, Load).



SWAT+ predefined watersheds
	The creation of a predefined watershed in SWAT+ necessitates preparing specific inputs and structures for streams, watersheds, and sub-basins. These include defining hydrograph sequences, lake-stream connections, and sub-basin boundary extent. We have automated an approach for generating SWAT+ predefined hydrographical models based on the NHDPlus HR2 database. Briefly, our approach initiates by aggregating data from the NHDPlus databases, performing a series of general preprocessing tasks, and saving the data in a portable size. Following this, we extract a subset of data for a desired watershed or region. The design of this data extraction process enables us to create SWAT+ models covering various geographic extent from a single HUC12/HUC8, to multiple HUC12/HUC8. More importantly, by utilizing a combination of HUC12, we can extract predefined watersheds for the drainage area of any desired streamflow station. This design also facilitates the extraction of predefined watersheds for any location covered by the NHDPlus Database. 
	Upon extracting the subset for HUC12s, the data undergoes further modification to align with the SWAT+ predefined watershed rules. This entails a multi-step revision process, beginning with modifying stream subbasin IDs to adhere to the one-outlet-rule required by the SWAT+ model for sub-basin definition, followed by defining lake connections with streams and then undertaking a series of additional modifications to define minimum areas for sub-basins, ensure quality control, and save the data for transfer to the QSWAT+ environment. Additional requirements for the predefined models were extracted from US national databases for land use (from NLCD), soil (from gSSURGO), and DEM (from USGS). For climate data, we utilized the PRISM dataset with a 4km resolution spanning from 1990 to 2022. The PRISM data was extracted from binary rasters, stacked, and saved in NumPy arrays, with each point in the database representing the coordinate of the centroid of the raster cells. This database, along with QSWAT+ and SWAT+ editors, is assembled and used for writing SWAT+ model input files/tables. 
	In this study, we utilized this workflow to extract predefined watersheds for all streamflow station drainage areas within the Michigan Lower Peninsula, adhering to the following criteria: 1) Only streamflow stations with a maximum drainage area of 1,500 square kilometers were used; 2) Stations selected had less than a 10% data gap between 1999 to 2022. By this definition, 50 streamflow stations were selected for creating their SWAT+ model. The first criterion aimed to limit the number of HRUs and avoid overly large models that pose challenges to solving the numerical equations. It should be noted that utilizing NHDPlus HR2 necessitates the use of a 30m DEM, leading to a fine-scale model. However, when considering the categories of land use, soil, and slope, the combination results in an excessive number of HRUs. Various strategies can be employed independently or concurrently to mitigate this issue. One method involves dissolving smaller streams into larger ones to reduce the number of watersheds within sub-basins. We dissolved the streamlines with drainage areas less than 10% of their corresponding subbasins into their downstream. Also, we adopted a coarser resolution for land use and soil characteristics to reduce the number of HRUs. We achieved this by resampling land use and soil databases from 30m to 250m resolution, employing a majority area approach for aggregation. Consequently, the maximum number of HRUs was capped at 42,000[XXXX] for the largest model with 1500sqkm, while the minimum number of HRUs was 480[XXXX] for the smallest model with a 50 sqkm drainage area. The distribution of HRUs, area versus the model count are shown in the following Figure. Employing these strategies enabled the extraction of 50 SWAT+ models, collectively 730,000 HRUs that represent surface water hydrological cycles within Michigan's Lower Peninsula.

Figure 1. Distribution of HRUs and drainage area versus the number of models. 


Figure 2. Distribution of four major types of land use in SWAT+ models. 

Sensitivity analysis and calibration processes
	Once the SWAT+ models were created, we devised an additional workflow to conduct a global sensitivity analysis, followed by parameter calibration.  

Sensitivity analysis
	We selected the Morris method (Morris, 1991) for performing sensitivity analysis on our SWAT+ models’ parameters. The Morris method, often referred to as Morris Screening or the Elementary Effects Method can assess the global importance of each input parameter across a wide range of possible values. However, we selected the Morris method for two primary reasons: First, its efficiency in screening unnecessary parameters with a relatively low number of samples compared to other global sensitivity analyses like Sobol, which requires a larger sample size, even with quasi-random sampling. Second, the Morris method aligns with our aim of reducing dimensionality for calibration, as it focuses on identifying significant parameters without the need to understand parameter interactions, which is a strength of the Sobol method. This approach is particularly beneficial for our task of calibrating 50 SWAT+ models in bulk and parallel processing.
	In the sensitivity analysis using the Morris method, we specifically focused on two key elements: the number of levels and the number of trajectories.
	Levels: In the Morris method, 'levels' refers to the discretization of the input parameter space. The number of levels indeed determines how finely the range of each input parameter is divided. This discretization facilitates the Morris method's systematic exploration of the parameter space by altering one parameter at a time, keeping others constant, to understand each parameter's impact on the model output.
	Trajectories: A trajectory in the Morris method represents a unique path through the parameter space, involving a sequence of steps where each step changes one parameter by one level. However, it's important to note that after changing one variable, the subsequent step involves changing another variable while keeping the previously altered variable at its new value and others at their start values. This process continues until all input variables are changed. The method is repeated multiple times (usually between 5 and 15 runs), each starting from a different set of values, leading to r(k + 1) model runs, where k is the number of inputs variables​​.
Through trial and error, we determined that a maximum of 20 trajectories for 30 parameters was optimal for our problem in terms of computational time, identifying impactful parameters, and removing insensitive parameters before passing to calibration. This results in ~600 model evaluations before calibration. Overall, by employing the Morris method with these settings, we were able to effectively evaluate the global sensitivity of the SWAT+ model parameters.
	For filter purposes, first, we used a relative thresholding approach where parameters with mu_star values greater than 0.1% of the maximum mu_star and sigma values greater than 0.1% of the maximum sigma from the Morris method are selected. This step ensures that only parameters with substantial mean effects and variances are considered further. Next, we compute the coefficient of variation (CV) for these parameters. The CV, defined as the ratio of sigma to the absolute mu_star, provides a normalized measure of variability relative to the mean effect. Parameters with CVs exceeding the 10th percentile of the CV distribution are retained. This criterion filters out parameters that, despite having significant mean effects, exhibit inconsistent variability across the sampled space, thus focusing on those with both high influence and reliability.
Bayesian Optimizer
	The Bayesian optimizer was configured with a Gradient Boosting Regression Trees (GBRT) as the base estimator. This choice allows for robust, nonlinear modeling of the calibration landscape, leveraging the power of gradient-boosting methods. The acquisition function was set to 'Expected Improvement' (EI), a strategy that strikes a balance between exploring new areas of the parameter space and exploiting known good areas. This approach is particularly useful in guiding the optimizer to efficiently search for optimal solutions. Additionally, we configured the optimizer's acquisition function with an 'auto' setting, allowing it to automatically determine the best method for optimizing the acquisition function. This dynamic selection ensures that the optimizer remains efficient and effective across a range of different optimization landscapes. The optimizer was also set with a model queue size of 100, meaning that it keeps a history of the last 100 evaluations. This history helps in speeding up the fitting of the surrogate model, thereby making the optimization process more efficient.
	For the calibration process, we set two termination criteria for the Bayesian model: 1) a maximum of five iterations when no better result is found, and 2) a maximum of 30 iterations, equating to 1500 model evaluations. Each iteration of the Bayesian model comprises 50 parallel processes. The calibration/sensitivity objective function was considered the sum of NSE for monthly (incremental rate) and daily streamflow rate, with monthly incremental rate used for scoring the model performance for the total monthly discharge and daily aimed to score daily hydrograph fluctuations.
	Table 1. SWAT+ parameters used in Sensitivity analysis and calibration.

	The workflow was designed to initially perform the sensitivity analysis, subsequently, eliminate parameters with negligible impact on model inputs, and then feed the refined sensitivity results as initial starting points to the Bayesian optimizer, thereby ensuring a robust sampling as achieved through Morris. The ranking of parameter sensitivity analysis for these models is shown in Table 2. 
Table 2. Morris’s sensitivity ranking based on μ*

	Both the sensitivity analysis and calibration were performed utilizing parallel processing. The model evaluations from the sensitivity analysis as well as the surrogate model of calibration were saved to be used for further refining the calibration of models if their performance after calibration was not satisfactory. Once the calibration process was deemed satisfactory, we performed a 20-year validation period (2000-2020). These analyses were conducted on our server equipped with 256 CPU cores and 512 GB DDR5 RAM with Windows server OS. Given the server's capacity, we managed to execute the sensitivity and calibration processes for five different watersheds simultaneously, leveraging a nested parallel processing pool.
Table 3. The daily and monthly SWAT+ model performance for different streamflow stations 

Parallel processing 

	The primary challenges we faced in managing SWAT+ model executions in parallel processing revolved around three main key aspects: 1- the inherent single core architecture of SWAT+ models, 2- the varying execution times of each SWAT+ model, 2- the distinct computational nature of sensitivity analysis and calibration processes. Our aim in parallel processing was to use a sustainable, scalable, and efficient use of our computational power, minimizing the efforts as much as possible for monitoring the system resource or manual scheduling of different model calibration/sensitivity analysis. Moreover, we also required a flexible and scalable approach to eliminate any potential for resource contention among SWAT+ models in the one node server and be as much as flexible to be utilized in a distributed computing system for our future studies. 
	SWAT+ modeling architecture: SWAT+ is a sequential computing different processes operate as a single threaded application (each model occupying one logical core). The SWAT model code architecture cannot be parallelized, and even if the internal loops that result in bottleneck be detected in SWAT to become parallel processes, it does not guarantee improvement over execution time and can potentially results in models that their CPU usage will not be consistent over the simulation period, resulting in resource contention in a parallel processing environment involving a dozen of models. However, the SWAT models can be created for different watersheds and run in parallel to complete a full picture of a larger watershed or a region. Also, it should be noted that SWAT+, like any other process-based hydrological models, uses a series of empirical equations to predict hydrological processes (runoff, recharge, snow melting, etc.). This empirical nature of the model results in different optimal set of parameters for different watersheds with different characteristics. Therefore, unlike fully physically based grid-based models, many of the SWAT model parameters does not have a physical representation on the ground. Therefore, the best way to calibrate SWAT models is in cluster modeling. 
Varying Resource Usage:  SWAT+ models, depending on their complexity and the number of HRUs, exhibit significant variability in memory usage (from less than 100MB to over 2GB) and execution time (couple of minutes for small models (~3000 HRUs) to more than 20 hours for large models (~40,000 HRUs). Also, the SWAT+ execution times are not uniformly predictable (even among the same models but with different parameter configuration). This presents a challenge in scheduling and managing these models in a parallel processing environment while keeping CPU and memory usage at a certain level for sustainable resource usage and long-time model processing. Furthermore, each model requires a suitable wall time to prevent computational stagnation which usually occurs in sensitivity or calibration when an incorrect combination of parameters cause the model stuck in a non-optimal state that hinders its ability to reach a conclusion within a reasonable time frame. 
Differing Nature of Sensitivity Analysis and Calibration: Sensitivity analysis and calibration processes inherently have different computational demands. Since samples for sensitivity analysis are pre-defined, the pool execution can be as large as the number of available CPUs or the total number of samples which in our case with using one server node, the number of available CPU was the limiting factor. On the other hand, the calibration process, involving Bayesian optimization, need to be updated regularly to add new models with new parameters in the parallel processing pool, therefore, it does not benefit from excessive parallel processing beyond a certain point.  
Number of SWAT models: As previously discussed, we have prepared individual SWAT+ models for different watersheds with the aim of reducing unnecessary model simulations for the watersheds or catchments without monitoring station. Therefore, each model requires a separate line of processing for sensitivity analysis and calibration. Acknowledging that we have 50 SWAT+ models with varying duration for sensitivity analysis and calibration (couple of hours to several days), an efficient job scheduling to use CPU and RAM as efficiently and constantly as possible is required. Therefore, our system should be able to dynamically allocate resources based on the current demands of each model's sensitivity analysis and calibration processes.
To address these challenges, we developed a two-layered approach in our parallel processing environment using Python’s Multiprocessing module. The code architecture includes mechanisms to monitor system load and ongoing model calibration/sensitivity analysis at outer layer as follows:  
Outer Layer for Process Management: This layer is responsible for efficiently allocating computational resources and managing the scheduling of SWAT+ model sensitivity and calibrations. It dynamically initiates model processing programs based on current system resource utilization and the status of ongoing processes. This layer ensures that new models are only executed when there is sufficient computational capacity, preventing system overload. This approach helped optimize the use of resources since once a model sensitivity analysis is about to finish, the CPU usage drops, therefore a new sensitivity analysis can be executed while the first sensitivity transit to calibration. The outer layer eliminates the reliance to find an optimal number of pool size for calibration/sensitivity analysis or the wall time for the model execution because once the outer layer recognize the CPU usage has dropped (or the number of models under simulations) it initiates a new sensitivity analysis that fill the gap in CPU usage.  
Inner Layer for Task-Specific Processing: Within this layer, we implemented customized process pools for sensitivity analysis and calibration. Sensitivity analysis utilized up to 200 parallel processes (consuming up to 80% of total 256 logical cores, leaving the rest of 20% free for other routine jobs in the server). The calibration process was limited to a maximum of 100 parallel processes. This decision, as said, was informed by the nature of Bayesian optimization used in calibration, which does not linearly benefit from increased parallelization beyond a certain threshold, especially considering its periodic updating mechanism and the potential for early termination based on predefined criteria. 
	The parallel processing approach implemented for sensitivity analysis begins by creating a manager and a queue to manage the distribution of tasks among multiple processes. For each point in the sensitivity analysis space, a process is initiated and added to a queue. These processes execute the model evaluation function in the background. The system dynamically controls the number of active processes based on the available computational resources, by monitoring CPU, memory, and number of models under execution. When a process completes its task, it is removed from the active pool, and its results are collected. The next point will be added to the pool only if there is enough computational capacity. 
	In the Bayesian optimization function, a dynamic batch processing method is used. The function initiates a specified number of processes (a parallel pool with maximum 100 model in the parallel processing) to evaluate different sets of parameters concurrently. As each process completes its task, the results are collected and evaluated. The system continuously adds new processes to replace completed ones, keeping the number of active processes close to the pool size. This approach allows for a steady flow of process execution and result collection, efficiently utilizing available computational resources. Contrastingly, in the sensitivity analysis function, a queue-based parallel processing approach is employed. Here, a manager creates a queue where model evaluation tasks are added. Processes are spawned to handle tasks from this queue, running up to the defined number of parallel jobs. This method ensures that new tasks are continuously added to the queue and processed as soon as a CPU becomes available, maintaining high CPU usage and efficient resource allocation. Unlike the batch processing in Bayesian optimization, the queue system in sensitivity analysis keeps adding and processing tasks individually until all tasks are completed.
	More importantly, the success of this approach isn't heavily reliant on selecting optimal parameters for pool sizes or wall times. This is primarily because the size of the parallel processes is flexible, allowing the parallel processing pool to shrink or expand based on the availability of resources. Additionally, resource contention is effectively managed as we consistently maintain a 10% buffer zone in the server node. This buffer zone accommodates any extra resource usage arising from variations in the sizes of the models or our routine work on the server. 
	Overall, this dynamic two-layer scheduling was instrumental in maintaining a sustainable level of efficiency, allowing for simultaneous processing of multiple SWAT+ models while ensuring that each model received the necessary computational resources for its specific requirements. Consequently, this approach allowed us to maximize the throughput and effectiveness of our high-resource computing environment, handling the diverse and variable computational needs of the SWAT+ models with precision and efficiency.
National Solar Radiation DataBase (NSRDB) 
"The National Solar Radiation Database (NSRDB) utilizes a hierarchical data format (HDF5) to manage its datasets efficiently. Each annual dataset within NSRDB is encapsulated in a singular HDF5 file. These files contain a wide array of variables. A complete list of variables is as follows: ['air_temperature', 'alpha', 'aod', 'asymmetry', 'cld_opd_dcomp', 'cld_reff_dcomp', 'clearsky_dhi', 'clearsky_dni', 'clearsky_ghi', 'cloud_press_acha', 'cloud_type', 'coordinates', 'dew_point', 'dhi', 'dni', 'fill_flag', 'ghi', 'meta', 'ozone', 'relative_humidity', 'solar_zenith_angle', 'ssa', 'surface_albedo', 'surface_pressure', 'time_index', 'total_precipitable_water', 'wind_direction', 'wind_speed']. 

The variables, excluding 'coordinates' and 'meta data', are structured in matrices with dimensions (17520, 2018392), representing half-hourly time steps and location points, respectively.

Our primary objective is to reduce the database size by filtering out unnecessary variables. To this end, we will retain only the 'ghi', 'relative_humidity', and 'wind_speed' variables for further analysis, thus reducing the file size from 1.7TB to around 200GB. During the model preparation stage, we will resample the data from 30-minute intervals to daily aggregates. This resampling will involve using summation for 'ghi' and averaging for 'relative_humidity' and 'wind_speed'. I wanted to keep wind direction as well, but aggregating the wind direction does not make sense to me as they are in degree. 


For a long-term plan, there is an option to compile the h5py library with MPI support. This approach would facilitate parallel processing and make us able to simultaneously write into an h5 database. Furthermore, there's potential for future development in SWAT+ model, by integrating similar data management strategies for storing HRU output data. 

Reference for HDF5 and h5py library: 

The HDF5® Library & File Format - The HDF Group
HDF5 for Python — h5py 3.11.0 documentation"


Results




















Future studies: 
Future studies have a variety of options to continue this work. This include several topics that we list which are already under development. 
1- Assessment of Management Practices: This involves evaluating the impact of various GW management practices and identifying the most effective ones for improving GW recharge rates across Michigan.
2- Climate Change Analysis: This includes investigating the implications of climate change on GW recharge and incorporating these findings into the predictive tool.
3- Land Use Change Analysis: This entails assessing the effects of land use changes on GW recharge across Michigan.
1- We automated the entire process starting with extracting NHDPlus watersheds for SWAT+ models, to sensitivity analysis, calibration, and verification. However, the missing link to automate the processes from 0 to 100 with no human intervention is to write SWAT+ input files directly. Probably the best approach will be still using the established programs like QSWAT+ and SWAT+ editor, which provide user interface for potential users specifically interested in one watershed.  
3- We implemented SNODAS into SWAT which resulted in improvement for the watershed, as well as, eliminating the need to include snowmelt parameters in the SWAT+.  

















Supplementary material
Terminology:
Watershed: The largest component of the model.
Subbasin: The largest component of a watershed, containing several river/channel
Catchment: The smallest component of a subbasin, containing one river/channel
Hydrologic Response Unit: The homogenous components of a catchment for water balance simulation 

Abbreviations:
NHD: National Hydrographical Dataset
VPUID: Vectorized Processing Unit ID
VAA: Value Added Attribute 
WBD: Watershed Boundary Dataset
HUC: Hydrologic Unit Code

1. SWAT+ Hydrographical Model Construction Using NHDPlus HR2
	This document outlines the methodology for constructing predefined SWAT+ predefined models based on the NHDPlus High-Resolution (HR2) database.
1.1. Data Preparation and Import
	The primary hydrographical data were extracted from the NHDPlus database for the Vector Processing Units (VPUIDs) of 405, 406, 407, 408, 409, and 410 covering the entire Michigan lower Peninsula. 
  - WBDHU8 and WBDHU12: representing basin/subbasin boundary.
  - NHDFlowline: representing streamlines.
  - NHDPlusCatchment: representing the drainage area of a streamline.
  - NHDWaterbody: Representing reservoirs, lakes, wetlands, and playa. 
  - NHDPlusFlowlineVAA: value-added attribute table for streams and catchments characteristics. The following attribute values were used for preparing the SWAT+ predefined watershed. 
VPUID: Identifier for Vector Processing Unit (equivalent to HUC4)
NHDPlusID: Common key among NHDPlusFlowlines, NHDPlusCatchment and NHDPlusVAA.
StreamOrder: The stream order is based on the Strahler method.
UpHydroSeq: The hydrological sequence number for the upstream.
HydroSeq: The hydrological sequence number for the flowline.
DnHydroSeq: The hydrological sequence number for the downstream.
StartFlag: Flag indicating if the flowline is a headwater.
TerminalFl: Flag indicating if the flowline is a terminal point (outlet).
Divergence: Indicates if a flowline diverges into multiple downstream paths.
Permanent_Identifier: A permanent identifier for each flowline.
WBArea_Permanent_Identifier: Identifier for the associated water body area.
MaxElevSmo: Smoothed maximum elevation of the flowline.
MinElevSmo: Smoothed minimum elevation of the flowline.
AreaSqKm: Drainage area in square kilometers.
LengthKM: Original length of the flowline in kilometers (later converted to meters).
TotDASqKm: Total drainage area in square kilometers.

1.2. General preprocessing:
CRS: The geometries were projected to EPSG: 26990
Removing Second Divergence: Streams with a divergence code of 2 were eliminated and their corresponding drainage area was merged to their downstream. 
Data Consistency Check: We excluded flowlines that did not have corresponding drainage areas.
Coastal Line Removal: Flowlines designated as coastal lines were removed from the dataset.
Isolated Streams: Streams that were isolated, i.e., neither flowing into nor out of other streams without corresponding drainage area in the NHDPlusCatchment were removed. 
Length Unit Conversion: The lengths of the streams, originally in kilometers, were converted to meters.
Drop Calculation: Drop is required by SWAT+ and represents the difference between the maximum and minimum elevation of a flowline. 
Reset Flags: The Start and Terminal flags were reset based on the above revision.
Identifying Hydrologic Unit Codes (HUC): The centroids of the catchment were spatially joined with the HUC shapefiles. The process is carried out separately for HUC8 and HUC12 levels.

1.3. Extracting HUC12 Based on Streamflow Stations
	In this step, the focus is on pinpointing the specific Hydrologic Unit Codes (HUC12) that are directly related to each streamflow monitoring station. This is essential for understanding the drainage areas contributing to each station. The methodological steps are outlined below:
Data Import: Streamflow station data and processed stream data will be imported into GeoDataFrames. They are also aligned to a common Coordinate Reference System (CRS) EPSG:26990 for compatibility with our case study Michigan.
Data Pre-filtering: Stations are filtered based on drainage area and data availability. This results in a selection of streamflow stations (along with their HydroSeq numbers) that meet specified criteria, such as a drainage area below 1500 sqkm with a maximum 5 percent of data gaps for the period between 1999 to 2020.
Upstream HydroSeq Identification: For each selected streamflow station, a recursive function identifies all upstream HydroSeq numbers. This process builds a list of HydroSeq numbers that ultimately drain into the location where the station is situated.
HUC12 Extraction: For each upstream HydroSeq, the corresponding HUC12 codes are extracted from the stream data. These codes are compiled into a list that represents the HUC12 units contributing to each streamflow station.

1.4. Configuring NHDPlus Data Based on SWAT+ Hydrographical Logic: The One-Outlet Rule
	SWAT model has a one-outlet rule that requires each subbasin to have only one stream flowing into one downstream subbasin. However, HUC12 boundaries, when used along with NHDPlus HR, often do not adhere to this rule, leading to subbasins with multiple outlets. Several conditions that violate this rule include 1-isolated subbasins within a HUC12, 2- the confluence of two streams draining into a downstream subbasin, and 3- a normal condition when a subbasin has different outlets leading to two or more downstream subbasins.
	To enforce the one-outlet rule, a systematic review of the subbasins is conducted to identify the number of outlets each possesses. An outlet is defined as a point where a stream exits a subbasin, either entering another subbasin or leaving the hydrological domain. In cases where a subbasin has more than one outlet, we used a recursive function to trace all upstream segments of a multi-outlet subbasin. These segments subsequently form a new subbasin, replacing the original one with multiple outlets. Once the subbasins are redefined, they are assigned new unique identifiers. The process involves iterative checks to ensure that each subbasin adheres to the one-outlet criterion.
1.5. Defining Flowline-Waterbody Connections
	For incorporating waterbodies into the hydrograph of the SWAT model, it's essential to identify lake inlet and outlet segments within flowlines. This includes the following steps:
Stream Classification: Waterbodies are categorized into four types: lakes, reservoirs, wetlands, and playas, based on their 'Ftype'.
Area Threshold: Waterbodies smaller than 0.1 sq km are excluded from the dataset.
Lake Identification (LakeId): To identify LakeId in flowlines, flowline data is merged with waterbody datasets, aligning the 'Permanent_Identifier' from waterbodies with the 'WBArea_Permanent_Identifier' in flowlines. Flowlines associated with lakes are initially considered as LakeWithin.
Lake Inlets (LakeIn): LakeIn segments are identified by traversing from headwater to outlet. Segments draining into a lake (indicated by a downstream LakeId) are tagged as LakeIn.
Lake Outlets (LakeOut): Each stream segment is examined to ascertain if it has a LakeId upstream, marking it as an outflow from a lake.
Main Lake Outlet (LakeMain): LakeMain is determined based on stream order, selecting the segment with the highest order in each lake as the primary outflow channel.
Updating LakeWithin: Initially, all segments within a lake's boundary are classified as LakeWithin. This classification is updated as outflow segments (LakeOut) are identified.
Special Cases Handling: In creating SWAT+ lakes from NHDPlus data, certain special cases conforming to SWAT+'s hydrographical structure are addressed. For instance, adjacent lakes in NHDPlus data with different permanent IDs are merged to share the same LakeId, or, if a lake's outlet coincides with the watershed's outlet, the last LakeWithin segment before the outlet is specifically designated as the LakeOut.
Subbasin refinement
Once the streams (flowlines) are prepared as described above, we join them with NHDCatchments based on the NHDPlusID. Subsequently, the subbasins were created based on the dissolving catchments by their subbasin number that we obtained during stream modification. Given the changes to the subbasins that we made to follow one-outlet-rule, we had to refine the subbasins to avoid excessively small subbasins. To do this, we performed another round of simplification by increasing the size of a subbasin to a minimum of 250*250*1000m. We accomplish this by iteratively dissolving subbasins to their downstream subbasins until the size of new subbasins are above the threshold. This threshold size is equal to 75% of the average size of HUC12 in Michigan. At the end, the subbasins below this threshold were typically isolated with no upstream or downstream connection. 
1.6 Quality assurance check:
Single Association for Streams and Catchments: Confirmed that each stream is associated with only one catchment, and vice versa.
Unique Subbasin Association: Verified that each catchment or stream is associated with only one subbasin.
Lake Outlet Requirement: Ensured that every lake has at least one outlet.
Avoidance of Circular Hydrographs: Checked for and eliminated any divergence that could cause circular hydrographs.
Single Outlet per Subbasin: Make certain that each subbasin has only one outlet.
ID Initialization: Ensured that both channel and Lake IDs start from 1.



The geological and hydrological aspects of Michigan you inquired about are well-documented in various sources. Here are the references for the statements in your text:
Sedimentary Layers in Michigan During the Paleozoic Era:
The Paleozoic period, lasting about 325 million years, saw significant geological changes. During this time, Michigan's Lower Peninsula, part of the Michigan Basin, accumulated hundreds of meters of sediments, which later turned into rocks and associated deposits like halite, gypsum, petroleum, lime, clay, sandstone, and coal​​. paleozoic rocks in Michigan (msu.edu)
The sedimentary rocks in the Michigan Basin, including limestones, sandstones, and shales, are approximately 500 million years old. In the center of the basin, these sedimentary rocks can be about 14,000 feet thick​​.
Glacial Layer and Landscape in the Lower Peninsula:
The Pleistocene glaciers, particularly during the Wisconsin stage, shaped Michigan's landscape. These glaciers leveled hills, filled valleys, blocked rivers, gouged out basins for the Great Lakes, and changed the surface through various geological processes​​. glacial landforms (msu.edu)
Glacial landforms dominate the surface of the whole state except the western half of the Upper Peninsula, where eroded remnants of some of the oldest mountains on Earth are found. glacial landforms (msu.edu)
Six major landform types emerged from glacial activities in Michigan, including moraines, till plains, outwash plains, lake clay plains, lake sand plains, and rock outcrop areas​​. glacial landforms (msu.edu)
Glacial activity led to the formation of "kettles," deep, steep-sided depressions, some of which became lake basins​​. glacial landforms (msu.edu)
Outwash plains in northern Michigan, part of the glacial deposits, have poorer soils and have been historically challenging for agriculture​​.
Lake plains, like those in Saginaw and Monroe counties, are rich in agriculture but require careful management and drainage​​.
Shallow Groundwater Formation in Michigan:
In Michigan, sufficient water supplies can be accessed by digging wells into the surface layers of glacial material or into rock layers acting as aquifers, with sandstone and limestone being significant contributors to groundwater storage. The sedimentary rock aquifers in the Lower Peninsula, part of the Michigan Basin, are particularly important for springs and artesian wells​​.
Groundwater plays a vital role in the Great Lakes ecosystem, acting as a reservoir for water and replenishing the lakes through base flow in tributaries. It is also a crucial source of drinking water for many communities​​.
As water passes through subsurface areas, it dissolves and suspends materials from the soils, contributing to the 'hard' water commonly found in the lower Great Lakes basin​​.
These references provide a detailed understanding of Michigan's geological and hydrological characteristics, shaped significantly by both its ancient sedimentary history and the extensive glacial activity during the Pleistocene period.


Snow Data Assimilation System (SNODAS): 
Snow Data Assimilation System (SNODAS) (Barrett, 2003) is a modeling and data assimilation system developed by the National Weather Service’s National Operational Hydrologic Remote Sensing Center (NOHRSC). SNODAS provides the best possible estimates of snow cover and associated variables over the conterminous United States by employing a physically consistent framework to integrate snow data from satellites, airborne platforms, and ground stations with model estimates of snow cover. The daily output of SNODAS data is provided by the National Snow and Ice Data Center (NSIDC) and has a 1km*km resolution. SNODAS database consists of a suite of 8 daily variables from 28 September 2003 to the present, including snow melt rate, snow water equivalent, snow depth, snow melt runoff, sublimation from the snowpack, sublimation of blowing snow, solid precipitation, and liquid precipitation.  
For this study and achieve to a consistent and physically based estimate of snow melt, we have integrated SNODAS output data with the SWAT+ model. This is achieved by modifying the SWAT+ snow module to use the SNODAS snow melt estimates where available and using its default model where no data is available. Where or when the SNODAS data are not, we use the built-in snow module of the SWAT+ and curb its estimation by comparing it with the median monthly snowmelt rate estimated by SNODAS.  This integration required modifying some of the SWAT+ source codes including snowdb_read.f90 subroutine, and sq_snom.f90. 


  





1. Snowpack average temperature: This typically refers to the average air temperature over a specific time period. It's crucial for understanding the thermal regime of the snowpack and its surroundings.
2. Blowing Snow Sublimation Rate: This measures the rate at which snow is sublimated (converted directly from solid to gas) during blowing snow events. This process can significantly reduce snowpack volume, especially in windy, dry conditions.
3. Melt Rate: The rate at which snow melts, often expressed in terms of water equivalent per unit time. This is influenced by air temperature, solar radiation, and other factors.
4. Snow Layer Thickness: The depth of the snowpack. It can indicate the volume of water stored in the snowpack and is important for assessing snowpack stability and hydrological importance.
5. Snow Water Equivalent (SWE): A common measure in hydrology and snow science, SWE represents the amount of water contained within the snowpack. It's a critical variable for understanding water supply, flood risk, and the overall water cycle.
6. Snowpack Sublimation Rate: Similar to blowing snow sublimation, but this refers to the rate of sublimation from the stationary snowpack. It's an important component of the snowpack's mass balance.
7. Non-Snow Accumulation: This might refer to the accumulation of precipitation in forms other than snow, like rain or sleet, particularly in areas with mixed precipitation types.
8. Snow Accumulation: The rate or amount of snowfall accumulation over time. It's essential for understanding seasonal snow cover changes and water resource management.

Parallel processing in Bayesian Optimization:
We implemented a dynamic approach to process management during the Bayesian optimization loop. This approach includes an adaptive handling of the number of active processes and early termination based on the progress of the optimization. Here is a key summary:
Initial Pool of Processes: We start by creating an initial pool of processes equal to a certain number of parallel processes. In the next stage, we created a monitoring loop to check if the number of active processes is less than the number of parallel jobs, and if so, we add new processes up to the parallel job limit. Each new process is given a 30-second start-up period.
Termination Based on Iteration Progress: The code checks if 90% of the total iterations (n_iter) have been completed. If so, it terminates all remaining processes.
Periodic Check for Best Objective Value: The code periodically checks for the best objective value every 50 simulations. This is done by collecting results, updating the optimizer, and logging the best objective value found so far.


Reference
Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M., 2019. Optuna: A next-generation hyperparameter optimization framework. Presented at the Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2623–2631.
Bailey, R.T., Bieger, K., Arnold, J.G., Bosch, D.D., 2020. A new physically-based spatially-distributed groundwater flow module for SWAT+. Hydrology 7, 75.
Barrett, A.P., 2003. National operational hydrologic remote sensing center snow data assimilation system (SNODAS) products at NSIDC. National Snow and Ice Data Center, Cooperative Institute for Research in ….
Buto, S.G., Anderson, R.D., 2020. NHDPlus high resolution (NHDPlus HR)---A hydrography framework for the nation (No. 2327–6932). US Geological Survey.
Costa, D., Zhang, H., Levison, J., 2021. Impacts of climate change on groundwater in the Great Lakes Basin: A review. Journal of Great Lakes Research 47, 1613–1625.
Dashti, Z., Nakhaei, M., Vadiati, M., Karami, G.H., Kisi, O., 2023. Estimation of Unconfined Aquifer Transmissivity Using a Comparative Study of Machine Learning Models. Water Resources Management 37, 4909–4931.
Di Salvo, C., 2022. Improving results of existing groundwater numerical models using machine learning techniques: A review. Water 14, 2307.
Feinstein, D., Hunt, R., Reeves, H., 2010. Regional groundwater-flow model of the Lake Michigan Basin in support of Great Lakes Basin water availability and use studies. U. S. Geological Survey.
Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep learning. MIT press.
Gupta, H.V., Kling, H., Yilmaz, K.K., Martinez, G.F., 2009. Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology 377, 80–91.
Knoll, L., Breuer, L., Bach, M., 2020. Nation-wide estimation of groundwater redox conditions and nitrate concentrations through machine learning. Environmental Research Letters 15, 064004.
Krivoruchko, K., 2012. Empirical bayesian kriging. ArcUser Fall 6, 1145.
Krivoruchko, K., Gribov, A., 2019. Evaluation of empirical Bayesian kriging. Spatial Statistics 32, 100368.
Lancia, M., Yao, Y., Andrews, C.B., Wang, X., Kuang, X., Ni, J., Gorelick, S.M., Scanlon, B.R., Wang, Y., Zheng, C., 2022. The China groundwater crisis: A mechanistic analysis with implications for global sustainability. Sustainable Horizons 4, 100042.
Leavesley, G., Stannard, L., 1995. The precipitation-runoff modeling system-PRMS. Computer models of watershed hydrology. 281–310.
Merino, N., Jackson, T.R., Campbell, J.H., Kersting, A.B., Sackett, J., Fisher, J.C., Bruckner, J.C., Zavarin, M., Hamilton-Brehm, S.D., Moser, D.P., 2022. Subsurface microbial communities as a tool for characterizing regional-scale groundwater flow. Science of The Total Environment 842, 156768.
Morris, M.D., 1991. Factorial sampling plans for preliminary computational experiments. Technometrics 33, 161–174.
Naghibi, S.A., Pourghasemi, H.R., Dixon, B., 2016. GIS-based groundwater potential mapping using boosted regression tree, classification and regression tree, and random forest machine learning models in Iran. Environmental monitoring and assessment 188, 1–27.
Ou, G., Chen, X., Kilic, A., Bartelt-Hunt, S., Li, Y., Samal, A., 2013. Development of a cross-section based streamflow routing package for MODFLOW. Environmental modelling & software 50, 132–143.
Pourghasemi, H.R., Sadhasivam, N., Yousefi, S., Tavangar, S., Nazarlou, H.G., Santosh, M., 2020. Using machine learning algorithms to map the groundwater recharge potential zones. Journal of Environmental Management 265, 110525.
Ransom, K.M., Nolan, B.T., A. Traum, J., Faunt, C.C., Bell, A.M., Gronberg, J.A.M., Wheeler, D.C., Z. Rosecrans, C., Jurgens, B., Schwarz, G.E., Belitz, K., M. Eberts, S., Kourakos, G., Harter, T., 2017. A hybrid machine learning model to predict and visualize nitrate concentration throughout the Central Valley aquifer, California, USA. Science of The Total Environment 601–602, 1160–1172. https://doi.org/10.1016/j.scitotenv.2017.05.192
Sahoo, S., Russo, T.A., Elliott, J., Foster, I., 2017. Machine learning algorithms for modeling groundwater level changes in agricultural regions of the U.S. Water Resources Research 53, 3878–3895. https://doi.org/10.1002/2016WR019933
Schreiner-McGraw, A.P., Ajami, H., 2021. Delayed response of groundwater to multi-year meteorological droughts in the absence of anthropogenic management. Journal of Hydrology 603, 126917.
Smith, R.G., Majumdar, S., 2020. Groundwater storage loss associated with land subsidence in Western United States mapped using machine learning. Water Resources Research 56, e2019WR026621.
Steinman, A.D., Uzarski, D.G., Lusch, D.P., Miller, C., Doran, P., Zimnicki, T., Chu, P., Allan, J., Asher, J., Bratton, J., 2022. Groundwater in crisis? Addressing groundwater challenges in Michigan (USA) as a template for the Great Lakes. Sustainability 14, 3008.
Sulis, M., Paniconi, C., Camporese, M., 2011. Impact of grid resolution on the integrated and distributed response of a coupled surface–subsurface hydrological model for the des Anglais catchment, Quebec. Hydrological Processes 25, 1853–1865.
Van Zanen, A., Hughes, S., 2022. Assessing Policy Drivers and Barriers for Sustainable Groundwater Management in Michigan.
White, M.J., Arnold, J.G., Bieger, K., Allen, P.M., Gao, J., Čerkasova, N., Gambone, M., Park, S., Bosch, D.D., Yen, H., 2022. Development of a field scale SWAT+ modeling framework for the contiguous US. JAWRA Journal of the American Water Resources Association 58, 1545–1560.
Yang, C., Tijerina-Kreuzer, D., Tran, H., Condon, L., Maxwell, R., 2023. A high-resolution, 3D groundwater-surface water simulation of the contiguous US: Advances in the integrated ParFlow CONUS 2.0 modeling platform.
Yin, J., Medellín-Azuara, J., Escriva-Bou, A., Liu, Z., 2021. Bayesian machine learning ensemble approach to quantify model uncertainty in predicting groundwater storage change. Science of The Total Environment 769, 144715. https://doi.org/10.1016/j.scitotenv.2020.144715
Zell, W.O., Sanford, W.E., 2020. Calibrated simulation of the long‐term average surficial groundwater system and derived spatial distributions of its characteristics for the contiguous United States. Water Resources Research 56, e2019WR026724.
National Snow and Ice Data Center. (2004). Snow Data Assimilation System (SNODAS) Data Products at NSIDC, Version 1. Boulder, Colorado USA: National Snow and Ice Data Center. DOI: 10.7265/N5TB14TC.
Kumar, M. and Sihag, P., 2019. Assessment of infiltration rate of soil using empirical and machine learning‐based models. Irrigation and drainage, 68(3), pp.588-601.
NRCS (United States Department of Agriculture, Natural Resources Conservation Service), 2021. Gridded Soil Survey Geographic. Available online at http://datagateway.nrcs.usda.gov/. Accessed 9-20-2022.
USGS (The United States Geological Survey), 2019. National Land Cover Database. Available online at https://www.usgs.gov/centers/eros/science/national-land-cover-database. Accessed 9-20-2022.  
USGS (The United States Geological Survey), 2022a. 3D Elevation Program. Available online at https://www.usgs.gov/3d-elevation-program. Accessed 9-20-2022.  
DEQ (Department of Environmental Quality), 2022. Wellogic System. Available online at https://www.egle.state.mi.us/wellogic/. Accessed 9-20-2022.  
EGLE (Michigan Department of Environment, Great Lakes, and Energy), 2020. Inland lake assessment units. Available online at https://gis-egle.hub.arcgis.com/datasets/egle::inland-lakes-assessment-units-2020/. Accessed 9-20-2022. 
Adombi, A.V.D.P., Chesnaux, R., & Boucher, M.-A. (2022). Comparing numerical modelling, traditional machine learning and theory-guided machine learning in inverse modeling of groundwater dynamics: A first study case application. Elsevier B.V.
Steinman, A.D., Uzarski, D.G., Lusch, D.P., Miller, C., Doran, P., Zimnicki, T., Chu, P., Allan, J., Asher, J., Bratton, J. and Carpenter, D., 2022. Groundwater in crisis? Addressing groundwater challenges in Michigan (USA) as a template for the Great Lakes. Sustainability, 14(5), p.3008.
Wang, L., Kurihana, T., Meray, A., Mastilovic, I., Praveen, S., Xu, Z., Memarzadeh, M., Lavin, A. and Wainwright, H., 2022. Multi-scale Digital Twin: Developing a fast and physics-informed surrogate model for groundwater contamination with uncertain climate models. arXiv preprint arXiv:2211.10884.
Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M., 2019. Optuna: A next-generation hyperparameter optimization framework. Presented at the Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2623–2631.
Bailey, R.T., Bieger, K., Arnold, J.G., Bosch, D.D., 2020. A new physically-based spatially-distributed groundwater flow module for SWAT+. Hydrology 7, 75.
Barrett, A.P., 2003. National operational hydrologic remote sensing center snow data assimilation system (SNODAS) products at NSIDC. National Snow and Ice Data Center, Cooperative Institute for Research in ….
Buto, S.G., Anderson, R.D., 2020. NHDPlus high resolution (NHDPlus HR)---A hydrography framework for the nation (No. 2327–6932). US Geological Survey.
Costa, D., Zhang, H., Levison, J., 2021. Impacts of climate change on groundwater in the Great Lakes Basin: A review. Journal of Great Lakes Research 47, 1613–1625.
Dashti, Z., Nakhaei, M., Vadiati, M., Karami, G.H., Kisi, O., 2023. Estimation of Unconfined Aquifer Transmissivity Using a Comparative Study of Machine Learning Models. Water Resources Management 37, 4909–4931.
Di Salvo, C., 2022. Improving results of existing groundwater numerical models using machine learning techniques: A review. Water 14, 2307.
Feinstein, D., Hunt, R., Reeves, H., 2010. Regional groundwater-flow model of the Lake Michigan Basin in support of Great Lakes Basin water availability and use studies. U. S. Geological Survey.
Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep learning. MIT press.
Gupta, H.V., Kling, H., Yilmaz, K.K., Martinez, G.F., 2009. Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology 377, 80–91.
Knoll, L., Breuer, L., Bach, M., 2020. Nation-wide estimation of groundwater redox conditions and nitrate concentrations through machine learning. Environmental Research Letters 15, 064004.
Krivoruchko, K., 2012. Empirical bayesian kriging. ArcUser Fall 6, 1145.
Krivoruchko, K., Gribov, A., 2019. Evaluation of empirical Bayesian kriging. Spatial Statistics 32, 100368.
Lancia, M., Yao, Y., Andrews, C.B., Wang, X., Kuang, X., Ni, J., Gorelick, S.M., Scanlon, B.R., Wang, Y., Zheng, C., 2022. The China groundwater crisis: A mechanistic analysis with implications for global sustainability. Sustainable Horizons 4, 100042.
Leavesley, G., Stannard, L., 1995. The precipitation-runoff modeling system-PRMS. Computer models of watershed hydrology. 281–310.
Merino, N., Jackson, T.R., Campbell, J.H., Kersting, A.B., Sackett, J., Fisher, J.C., Bruckner, J.C., Zavarin, M., Hamilton-Brehm, S.D., Moser, D.P., 2022. Subsurface microbial communities as a tool for characterizing regional-scale groundwater flow. Science of The Total Environment 842, 156768.
Morris, M.D., 1991. Factorial sampling plans for preliminary computational experiments. Technometrics 33, 161–174.
Naghibi, S.A., Pourghasemi, H.R., Dixon, B., 2016. GIS-based groundwater potential mapping using boosted regression tree, classification and regression tree, and random forest machine learning models in Iran. Environmental monitoring and assessment 188, 1–27.
Ou, G., Chen, X., Kilic, A., Bartelt-Hunt, S., Li, Y., Samal, A., 2013. Development of a cross-section based streamflow routing package for MODFLOW. Environmental modelling & software 50, 132–143.
Pourghasemi, H.R., Sadhasivam, N., Yousefi, S., Tavangar, S., Nazarlou, H.G., Santosh, M., 2020. Using machine learning algorithms to map the groundwater recharge potential zones. Journal of Environmental Management 265, 110525.
Ransom, K.M., Nolan, B.T., A. Traum, J., Faunt, C.C., Bell, A.M., Gronberg, J.A.M., Wheeler, D.C., Z. Rosecrans, C., Jurgens, B., Schwarz, G.E., Belitz, K., M. Eberts, S., Kourakos, G., Harter, T., 2017. A hybrid machine learning model to predict and visualize nitrate concentration throughout the Central Valley aquifer, California, USA. Science of The Total Environment 601–602, 1160–1172. https://doi.org/10.1016/j.scitotenv.2017.05.192
Sahoo, S., Russo, T.A., Elliott, J., Foster, I., 2017. Machine learning algorithms for modeling groundwater level changes in agricultural regions of the U.S. Water Resources Research 53, 3878–3895. https://doi.org/10.1002/2016WR019933
Schreiner-McGraw, A.P., Ajami, H., 2021. Delayed response of groundwater to multi-year meteorological droughts in the absence of anthropogenic management. Journal of Hydrology 603, 126917.
Smith, R.G., Majumdar, S., 2020. Groundwater storage loss associated with land subsidence in Western United States mapped using machine learning. Water Resources Research 56, e2019WR026621.
Steinman, A.D., Uzarski, D.G., Lusch, D.P., Miller, C., Doran, P., Zimnicki, T., Chu, P., Allan, J., Asher, J., Bratton, J., 2022. Groundwater in crisis? Addressing groundwater challenges in Michigan (USA) as a template for the Great Lakes. Sustainability 14, 3008.
Sulis, M., Paniconi, C., Camporese, M., 2011. Impact of grid resolution on the integrated and distributed response of a coupled surface–subsurface hydrological model for the des Anglais catchment, Quebec. Hydrological Processes 25, 1853–1865.
Van Zanen, A., Hughes, S., 2022. Assessing Policy Drivers and Barriers for Sustainable Groundwater Management in Michigan.
White, M.J., Arnold, J.G., Bieger, K., Allen, P.M., Gao, J., Čerkasova, N., Gambone, M., Park, S., Bosch, D.D., Yen, H., 2022. Development of a field scale SWAT+ modeling framework for the contiguous US. JAWRA Journal of the American Water Resources Association 58, 1545–1560.
Yang, C., Tijerina-Kreuzer, D., Tran, H., Condon, L., Maxwell, R., 2023. A high-resolution, 3D groundwater-surface water simulation of the contiguous US: Advances in the integrated ParFlow CONUS 2.0 modeling platform.
Yin, J., Medellín-Azuara, J., Escriva-Bou, A., Liu, Z., 2021. Bayesian machine learning ensemble approach to quantify model uncertainty in predicting groundwater storage change. Science of The Total Environment 769, 144715. https://doi.org/10.1016/j.scitotenv.2020.144715
Zell, W.O., Sanford, W.E., 2020. Calibrated simulation of the long‐term average surficial groundwater system and derived spatial distributions of its characteristics for the contiguous United States. Water Resources Research 56, e2019WR026724.

U.S. Geological Survey (n.d.). 3D Elevation Program, Digital Elevation Model. Accessed on August 29, 2023, at USGS National Map Downloadable Data Collection. Available online at https://apps.nationalmap.gov/downloader/.
U.S. Geological Survey (2017). National Hydrography Dataset Plus High Resolution, NHDPlus HR. Accessed on August 29, 2023, at USGS National Map Downloadable Data Collection. Available online at https://apps.nationalmap.gov/downloader/.
Dewitz, J., & U.S. Geological Survey (2021). National Land Cover Database (NLCD) 2019 Products (ver. 2.0, June 2021). Accessed on August 29, 2023. DOI: 10.5066/P9KZCM54.
Soil Survey Staff, Natural Resources Conservation Service, United States Department of Agriculture (n.d.). U.S. General Soil Map (STATSGO2). Accessed on August 29, 2023. Available online at https://sdmdataaccess.sc.egov.usda.gov.
Department of Technology, Management, & Budget (DTMB) (n.d.). Water Wells By County With Lithology. Accessed on August 29, 2023. Available online at https://www.michigan.gov/dtmb/services/maps/static/water-wells-by-county.
Michigan Department of Environment, Great Lakes, and Energy (EGLE) (n.d.). Glacial Land Systems. Accessed on August 29, 2023. Available online at https://gis-michigan.opendata.arcgis.com/datasets/egle::glacial-landsystems/explore.
Michigan Department of Natural Resources (DNR) (n.d.). Aquifer Characteristics Of Glacial Drift. Accessed on August 29, 2023. Available online at https://gis-michigan.opendata.arcgis.com/datasets/egle::glacial-landsystems/explore.
Michigan Department of Natural Resources (DNR) (n.d.). Aquifer Characteristics Of Glacial Drift. Accessed on August 29, 2023. Available online at https://gis-midnr.opendata.arcgis.com/datasets/b52cd98bb02047ca8927eec8879726ef_4/explore.
Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M., 2019. Optuna: A next-generation hyperparameter optimization framework. Presented at the Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2623–2631.
Bailey, R.T., Bieger, K., Arnold, J.G., Bosch, D.D., 2020. A new physically-based spatially-distributed groundwater flow module for SWAT+. Hydrology 7, 75.
Barrett, A.P., 2003. National operational hydrologic remote sensing center snow data assimilation system (SNODAS) products at NSIDC. National Snow and Ice Data Center, Cooperative Institute for Research in ….
Buto, S.G., Anderson, R.D., 2020. NHDPlus high resolution (NHDPlus HR)---A hydrography framework for the nation (No. 2327–6932). US Geological Survey.
Costa, D., Zhang, H., Levison, J., 2021. Impacts of climate change on groundwater in the Great Lakes Basin: A review. Journal of Great Lakes Research 47, 1613–1625.
Dashti, Z., Nakhaei, M., Vadiati, M., Karami, G.H., Kisi, O., 2023. Estimation of Unconfined Aquifer Transmissivity Using a Comparative Study of Machine Learning Models. Water Resources Management 37, 4909–4931.
Di Salvo, C., 2022. Improving results of existing groundwater numerical models using machine learning techniques: A review. Water 14, 2307.
Feinstein, D., Hunt, R., Reeves, H., 2010. Regional groundwater-flow model of the Lake Michigan Basin in support of Great Lakes Basin water availability and use studies. U. S. Geological Survey.
Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep learning. MIT press.
Gupta, H.V., Kling, H., Yilmaz, K.K., Martinez, G.F., 2009. Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology 377, 80–91.
Knoll, L., Breuer, L., Bach, M., 2020. Nation-wide estimation of groundwater redox conditions and nitrate concentrations through machine learning. Environmental Research Letters 15, 064004.
Krivoruchko, K., 2012. Empirical bayesian kriging. ArcUser Fall 6, 1145.
Krivoruchko, K., Gribov, A., 2019. Evaluation of empirical Bayesian kriging. Spatial Statistics 32, 100368.
Lancia, M., Yao, Y., Andrews, C.B., Wang, X., Kuang, X., Ni, J., Gorelick, S.M., Scanlon, B.R., Wang, Y., Zheng, C., 2022. The China groundwater crisis: A mechanistic analysis with implications for global sustainability. Sustainable Horizons 4, 100042.
Leavesley, G., Stannard, L., 1995. The precipitation-runoff modeling system-PRMS. Computer models of watershed hydrology. 281–310.
Merino, N., Jackson, T.R., Campbell, J.H., Kersting, A.B., Sackett, J., Fisher, J.C., Bruckner, J.C., Zavarin, M., Hamilton-Brehm, S.D., Moser, D.P., 2022. Subsurface microbial communities as a tool for characterizing regional-scale groundwater flow. Science of The Total Environment 842, 156768.
Morris, M.D., 1991. Factorial sampling plans for preliminary computational experiments. Technometrics 33, 161–174.
Naghibi, S.A., Pourghasemi, H.R., Dixon, B., 2016. GIS-based groundwater potential mapping using boosted regression tree, classification and regression tree, and random forest machine learning models in Iran. Environmental monitoring and assessment 188, 1–27.
Ou, G., Chen, X., Kilic, A., Bartelt-Hunt, S., Li, Y., Samal, A., 2013. Development of a cross-section based streamflow routing package for MODFLOW. Environmental modelling & software 50, 132–143.
Pourghasemi, H.R., Sadhasivam, N., Yousefi, S., Tavangar, S., Nazarlou, H.G., Santosh, M., 2020. Using machine learning algorithms to map the groundwater recharge potential zones. Journal of Environmental Management 265, 110525.
Ransom, K.M., Nolan, B.T., A. Traum, J., Faunt, C.C., Bell, A.M., Gronberg, J.A.M., Wheeler, D.C., Z. Rosecrans, C., Jurgens, B., Schwarz, G.E., Belitz, K., M. Eberts, S., Kourakos, G., Harter, T., 2017. A hybrid machine learning model to predict and visualize nitrate concentration throughout the Central Valley aquifer, California, USA. Science of The Total Environment 601–602, 1160–1172. https://doi.org/10.1016/j.scitotenv.2017.05.192
Sahoo, S., Russo, T.A., Elliott, J., Foster, I., 2017. Machine learning algorithms for modeling groundwater level changes in agricultural regions of the U.S. Water Resources Research 53, 3878–3895. https://doi.org/10.1002/2016WR019933
Schreiner-McGraw, A.P., Ajami, H., 2021. Delayed response of groundwater to multi-year meteorological droughts in the absence of anthropogenic management. Journal of Hydrology 603, 126917.
Smith, R.G., Majumdar, S., 2020. Groundwater storage loss associated with land subsidence in Western United States mapped using machine learning. Water Resources Research 56, e2019WR026621.
Steinman, A.D., Uzarski, D.G., Lusch, D.P., Miller, C., Doran, P., Zimnicki, T., Chu, P., Allan, J., Asher, J., Bratton, J., 2022. Groundwater in crisis? Addressing groundwater challenges in Michigan (USA) as a template for the Great Lakes. Sustainability 14, 3008.
Sulis, M., Paniconi, C., Camporese, M., 2011. Impact of grid resolution on the integrated and distributed response of a coupled surface–subsurface hydrological model for the des Anglais catchment, Quebec. Hydrological Processes 25, 1853–1865.
Van Zanen, A., Hughes, S., 2022. Assessing Policy Drivers and Barriers for Sustainable Groundwater Management in Michigan.
White, M.J., Arnold, J.G., Bieger, K., Allen, P.M., Gao, J., Čerkasova, N., Gambone, M., Park, S., Bosch, D.D., Yen, H., 2022. Development of a field scale SWAT+ modeling framework for the contiguous US. JAWRA Journal of the American Water Resources Association 58, 1545–1560.
Yang, C., Tijerina-Kreuzer, D., Tran, H., Condon, L., Maxwell, R., 2023. A high-resolution, 3D groundwater-surface water simulation of the contiguous US: Advances in the integrated ParFlow CONUS 2.0 modeling platform.
Yin, J., Medellín-Azuara, J., Escriva-Bou, A., Liu, Z., 2021. Bayesian machine learning ensemble approach to quantify model uncertainty in predicting groundwater storage change. Science of The Total Environment 769, 144715. https://doi.org/10.1016/j.scitotenv.2020.144715
Zell, W.O., Sanford, W.E., 2020. Calibrated simulation of the long‐term average surficial groundwater system and derived spatial distributions of its characteristics for the contiguous United States. Water Resources Research 56, e2019WR026724.
