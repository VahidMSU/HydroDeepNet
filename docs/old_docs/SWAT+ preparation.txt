Advancing regional water resources modeling to neighborhood level

Abstract:

Highlights:
This study presents one of the highest possible resolutions for creating semi-distributed hydrological models. 
This study presents the findings of modeling groundwater-surface water resources across 50 watersheds in Michigan. 
The hydrographical modeling structure entirely relies on the US National Hydrological dataset High resolution (NHDPlus). 
NHDPlus has been created based on 1/3-Arc Second (30m) or better and details of hydrograph sequences, and watershed delineation are quite substantial. The subset of NHDPlus for Michigan LP consists of more than 120,000 catchments, more than 2000 HUC12, 32 HUC8, and 6 HUC4. Using NHDPlus data and the SWAT+ model, allowed us to model Michigan hydrological processes at the neighborhood level. To signify the resolution of our works, it is worth comparing it with previous studies. First, our previous model for the Huron River watershed (a HUC8 model) consisted of 198 channels and 9000 HRUs. Our SWAT+ model for the same watershed has 3100 channels and 100,000 HRUs. Collectively, the HUC8 models of Michigan have more than 2 million HRUs. Compared with studies with the same geographic extent, our modeling dwarfs any predecessors. For example, XXX et al have presented a SWAT model for the Great Lake region with a modified snow estimation module. Their models consist of only 12000 HRUs. Our modeling results can accurately capture any streamflow stations with drainage areas as small as 50 sq km to as large as 1,500 sq km. 
The climate data has been represented with a PRISM dataset with 4kqm coverage, the snow melting has been preset by 1Km*1Km data from the Snow Data Assimilation System, and groundwater has been represented by a 250*250 meter physically based model integrated into SWAT+. 
To solve large and substantially computationally extensive HUC8 models, we first prepared 50 SWAT+ models with varying geographic extents from 50 sq km to 1,500 sq km corresponding to 50 streamflow stations consisting of a total of 700,000 HRUs have been created. For each, a global sensitivity analysis, followed by a Bayesian calibration has been conducted. The information gained from sensitivity analysis and calibration has been used for final parameter estimation in HUC18 models. The final result that we present in this study, is a high-resolution estimate of annual groundwater recharge that separately has been validated with a MODFLOW model and verified against observation data. 


Terminologies:


Introduction
	Regional and National water resources studies are essential parts of water resources management to link natural and anthropogenic global changes to local water management policies. Hydrological modeling studies are an essential part of this link to bring complex groundwater and surface water management into actionable information for communities, townships and states decision-making (Van Zanen and Hughes, 2022). The resolution in hydrological modeling is an important part of model accuracy and reliability. Resolution can be normally defined with multiple dimensions as 1- the resolution of spatial distribution, 2- the temporal resolution, and 3- the resolution of processes (the number of hydrological processes that a model can compute and incorporate into overall quality and quantity of water balance estimation). In environmental and earth modeling systems, the common resolution of data for reconstructing spatial heterogeneity ranges from 10m (e.g., DEM, Landuse, soil) to thousands of meters (climate data) and the common temporal resolution is one day. However, creating models with high resolution at all dimensions (spatial, temporal, and processes) has been progressing for years in different hydrologic research branch. For example, a distributed physically based groundwater-surface water model, ParFlow CONUS version 2.0 (Yang et al., 2023), has been recently updated with a 1km spatial scale for hydrogeological processing, improved from 4 sq km in the previous version. This cell size while considering the geographic extent of the model coverage is the highest resolution that is achieved through a consistent grid-based modeling. Regardless of the modeling type, this resolution still may not reveal the details that regional scale studies require. As such, a study by USGS for modeling regional groundwater-surface water in Michigan (Feinstein et al., 2010) with MODFLOW, found that a scale of 1.5km for special resolution is still coarse, and therefore a finer resolution was needed to simulate the hydraulic gradients toward Lake Michigan. While this is not new to the modeling community anymore, other studies also have been shown the impact of coarser resolution on misestimating groundwater storage (Sulis et al., 2011).
	Going beyond 1sqkm resolution in national and regional studies, however, exponentially increases the number of cell size. For example, a regional hydrological model with a geographic extent of 100,000 sqkm with 1 sqkm cell size results in 100,000 cells, while a model with 250m resolution, results in 1.6 million cells. This exponential increase in the number of cells, also non-linearly increases the computational costs and if associated with large geographic extent or high-resolution processes (like incorporating anthropogenic changes and agricultural management practices) makes the model impractical to work with, specifically for scientific and research purposes that require thousands of simulations for sensitivity analysis, calibration, and uncertainty analysis. Given such geographic extent, achieving high-resolution modeling requires breaking down the large geographic extent into scalable pieces as has been achieved so far by   
	Normally, at watershed scale, achieving high accuracy in estimating groundwater head is warranted through 100-250m cell size, as shown in our previous studies for two different watersheds in two different continents, climate and data quality, one in Michigan and the other one in Queensland, Australia that we could achieve almost identical groundwater head simulation comparing to observation. 
	To name a few regional high-resolution models, Čerkasova et al. (2021) developed a transboundary SWAT model for Europe, comprising 11 interconnected sub-models, 9,012 subbasins, and 148,212 HRUs. The US National Agroecosystem Model (NAM), built by SWAT+ has 2,296,974 HRUs for the extent of the United States (White et al., 2022) and is built upon NHDPlus V2 (National Hydrographical Dataset Plus Version 2) which has a resolution of 1:100k. Here we represent an approach and associated tools for achieving neighborhood level resolution in regional hydrogeological studies in the United States with national public data within a reasonable time and computational power. 
	Our modeling tool is SWAT+ model which is written by object-oriented Fortran and provide extra flexibility in code development by the community. The flexibility in SWAT+ comparing to SWAT is considerable, especially in terms of using objected oriented language, reducing axillary files, and its new architecture that, as we will show in the methodology section, allowed us to incorporate Snow Assimilation Dataset (a physically based snow melt simulation modeling data) into SWAT+ modeling codes. Also, SWAT+ has an integrated physically-based groundwater model for simulating groundwater-surface interaction modeling (Bailey et al., 2020) which significantly improved computational time comparing to previous SWAT-MODFLOW model. Also, SWAT+ model structure has one more hierarchy comparing to SWAT model. The SWAT+ model has basin, subbasin and watersheds comparing to SWAT that has only basin and subbasin hierarchy. 
	We built our hydrographical modeling structure entirely based on NHDPlus HR (the US National Hydrological Dataset High Resolution). The NHDPlus HR has a resolution of 1:24k, and details of hydrograph sequences, and watershed delineation are quite substantial. NHDPlus HR is based on relatively current NHD, at 1:24K or better. The NHDPlus V2 which has been used in NAM, is based on an older, less detailed, 1:100k-scale NHD, which has not been updated since 2012 (Buto and Anderson, 2020). The subset of NHDPlus HR for Michigan LP that we present here consists of more than 120,000 catchments, more than 2000 HUC12, 32 HUC8, and 4 complete HUC4 and 2 partially HUC4 inside the state boundary. Using NHDPlus HR data and the SWAT+ model, allowed us to model Michigan water resources at the neighborhood level with the highest quality. To signify the resolution of our works, it is worth comparing it with our previous model for the Huron River watershed (a HUC8 model) in southeast Michigan. Our SWAT model consisted of 198 identifiable channels and 9000 HRUs. Our SWAT+ model for the same watershed has 3100 channels and 100,000 HRUs. Collectively, the HUC8 models of Michigan have more than 2 million HRUs. Compared with studies with the same geographic extent, our modeling resolutions dwarfs any predecessors. 
	Our model selection (SWAT+) was not only based our experience working with it, but the selection was based on the fact that there are no other alternatives for estimating quality and quantity of groundwater recharge better than SWAT and at the same time, there is no other viable solution to link a surface water model with MODFLOW (standard groundwater modeling tools in US studies) with a surface water model other than SWAT. Of course, many modules are developed for simulating surface hydrology for MODFLOW like stream routing package (SR2) (Ou et al., 2013). However, SR2 is only limited to stream routing and groundwater and surface water interaction at river interface. For estimating groundwater recharge for MODFLOW another modeling tools like Precipitation Runoff Modeling System (PRSM) (Leavesley and Stannard, 1995) has been developed, however, no other surface water modeling tools like SWAT can provide all these information plus the impact of agricultural management practices on water quality estimation at the same time.  


SWAT+ predefined watersheds
	The creation of a predefined watershed in SWAT+ necessitates preparing specific inputs and structures for streams, watersheds, and sub-basins. These include defining hydrograph sequences, lake-stream connections, and sub-basin boundary extent. We have automated an approach for generating SWAT+ predefined hydrographical models based on the NHDPlus HR2 database. Briefly, our approach initiates by aggregating data from the NHDPlus databases, performing a series of general preprocessing tasks, and saving the data in a portable size. Following this, we extract a subset of data for a desired watershed or region. The design of this data extraction process enables us to create SWAT+ models covering various geographic extent from a single HUC12/HUC8, to multiple HUC12/HUC8. More importantly, by utilizing a combination of HUC12, we can extract predefined watersheds for the drainage area of any desired streamflow station. This design also facilitates the extraction of predefined watersheds for any location covered by the NHDPlus Database. 
	Upon extracting the subset for HUC12s, the data undergoes further modification to align with the SWAT+ predefined watershed rules. This entails a multi-step revision process, beginning with modifying stream subbasin IDs to adhere to the one-outlet-rule required by the SWAT+ model for sub-basin definition, followed by defining lake connections with streams and then undertaking a series of additional modifications to define minimum areas for sub-basins, ensure quality control, and save the data for transfer to the QSWAT+ environment. Additional requirements for the predefined models were extracted from US national databases for land use (from NLCD), soil (from gSSURGO), and DEM (from USGS). For climate data, we utilized the PRISM dataset with a 4km resolution spanning from 1990 to 2022. The PRISM data was extracted from binary rasters, stacked, and saved in NumPy arrays, with each point in the database representing the coordinate of the centroid of the raster cells. This database, along with QSWAT+ and SWAT+ editors, is assembled and used for writing SWAT+ model input files/tables. 
	In this study, we utilized this workflow to extract predefined watersheds for all streamflow station drainage areas within the Michigan Lower Peninsula, adhering to the following criteria: 1) Only streamflow stations with a maximum drainage area of 1,500 square kilometers were used; 2) Stations selected had less than a 10% data gap between 1999 to 2022. By this definition, 50 streamflow stations were selected for creating their SWAT+ model. The first criterion aimed to limit the number of HRUs and avoid overly large models that pose challenges to solving the numerical equations. It should be noted that utilizing NHDPlus HR2 necessitates the use of a 30m DEM, leading to a fine-scale model. However, when considering the categories of land use, soil, and slope, the combination results in an excessive number of HRUs. Various strategies can be employed independently or concurrently to mitigate this issue. One method involves dissolving smaller streams into larger ones to reduce the number of watersheds within sub-basins. We dissolved the streamlines with drainage areas less than 10% of their corresponding subbasins into their downstream. Also, we adopted a coarser resolution for land use and soil characteristics to reduce the number of HRUs. We achieved this by resampling land use and soil databases from 30m to 250m resolution, employing a majority area approach for aggregation. Consequently, the maximum number of HRUs was capped at 42,000[XXXX] for the largest model with 1500sqkm, while the minimum number of HRUs was 480[XXXX] for the smallest model with a 50 sqkm drainage area. The distribution of HRUs, area versus the model count are shown in the following Figure. Employing these strategies enabled the extraction of 50 SWAT+ models, collectively 730,000 HRUs that represent surface water hydrological cycles within Michigan's Lower Peninsula.

Figure 1. Distribution of HRUs and drainage area versus the number of models. 


Figure 2. Distribution of four major types of land use in SWAT+ models. 

Sensitivity analysis and calibration processes
	Once the SWAT+ models were created, we devised an additional workflow to conduct a global sensitivity analysis, followed by parameter calibration.  

Sensitivity analysis
	We selected the Morris method (Morris, 1991) for performing sensitivity analysis on our SWAT+ models’ parameters. The Morris method, often referred to as Morris Screening or the Elementary Effects Method can assess the global importance of each input parameter across a wide range of possible values. However, we selected the Morris method for two primary reasons: First, its efficiency in screening unnecessary parameters with a relatively low number of samples compared to other global sensitivity analyses like Sobol, which requires a larger sample size, even with quasi-random sampling. Second, the Morris method aligns with our aim of reducing dimensionality for calibration, as it focuses on identifying significant parameters without the need to understand parameter interactions, which is a strength of the Sobol method. This approach is particularly beneficial for our task of calibrating 50 SWAT+ models in bulk and parallel processing.
	In the sensitivity analysis using the Morris method, we specifically focused on two key elements: the number of levels and the number of trajectories.
	Levels: In the Morris method, 'levels' refers to the discretization of the input parameter space. The number of levels indeed determines how finely the range of each input parameter is divided. This discretization facilitates the Morris method's systematic exploration of the parameter space by altering one parameter at a time, keeping others constant, to understand each parameter's impact on the model output.
	Trajectories: A trajectory in the Morris method represents a unique path through the parameter space, involving a sequence of steps where each step changes one parameter by one level. However, it's important to note that after changing one variable, the subsequent step involves changing another variable while keeping the previously altered variable at its new value and others at their start values. This process continues until all input variables are changed. The method is repeated multiple times (usually between 5 and 15 runs), each starting from a different set of values, leading to r(k + 1) model runs, where k is the number of inputs variables​​.
Through trial and error, we determined that a maximum of 20 trajectories for 30 parameters was optimal for our problem in terms of computational time, identifying impactful parameters, and removing insensitive parameters before passing to calibration. This results in ~600 model evaluations before calibration. Overall, by employing the Morris method with these settings, we were able to effectively evaluate the global sensitivity of the SWAT+ model parameters.
	For filter purposes, first, we used a relative thresholding approach where parameters with mu_star values greater than 0.1% of the maximum mu_star and sigma values greater than 0.1% of the maximum sigma from the Morris method are selected. This step ensures that only parameters with substantial mean effects and variances are considered further. Next, we compute the coefficient of variation (CV) for these parameters. The CV, defined as the ratio of sigma to the absolute mu_star, provides a normalized measure of variability relative to the mean effect. Parameters with CVs exceeding the 10th percentile of the CV distribution are retained. This criterion filters out parameters that, despite having significant mean effects, exhibit inconsistent variability across the sampled space, thus focusing on those with both high influence and reliability.
Bayesian Optimizer
	The Bayesian optimizer was configured with a Gradient Boosting Regression Trees (GBRT) as the base estimator. This choice allows for robust, nonlinear modeling of the calibration landscape, leveraging the power of gradient-boosting methods. The acquisition function was set to 'Expected Improvement' (EI), a strategy that strikes a balance between exploring new areas of the parameter space and exploiting known good areas. This approach is particularly useful in guiding the optimizer to efficiently search for optimal solutions. Additionally, we configured the optimizer's acquisition function with an 'auto' setting, allowing it to automatically determine the best method for optimizing the acquisition function. This dynamic selection ensures that the optimizer remains efficient and effective across a range of different optimization landscapes. The optimizer was also set with a model queue size of 100, meaning that it keeps a history of the last 100 evaluations. This history helps in speeding up the fitting of the surrogate model, thereby making the optimization process more efficient.
	For the calibration process, we set two termination criteria for the Bayesian model: 1) a maximum of five iterations when no better result is found, and 2) a maximum of 30 iterations, equating to 1500 model evaluations. Each iteration of the Bayesian model comprises 50 parallel processes. The calibration/sensitivity objective function was considered the sum of NSE for monthly (incremental rate) and daily streamflow rate, with monthly incremental rate used for scoring the model performance for the total monthly discharge and daily aimed to score daily hydrograph fluctuations.
	Table 1. SWAT+ parameters used in Sensitivity analysis and calibration.

	The workflow was designed to initially perform the sensitivity analysis, subsequently, eliminate parameters with negligible impact on model inputs, and then feed the refined sensitivity results as initial starting points to the Bayesian optimizer, thereby ensuring a robust sampling as achieved through Morris. The ranking of parameter sensitivity analysis for these models is shown in Table 2. 
Table 2. Morris’s sensitivity ranking based on μ*

	Both the sensitivity analysis and calibration were performed utilizing parallel processing. The model evaluations from the sensitivity analysis as well as the surrogate model of calibration were saved to be used for further refining the calibration of models if their performance after calibration was not satisfactory. Once the calibration process was deemed satisfactory, we performed a 20-year validation period (2000-2020). These analyses were conducted on our server equipped with 256 CPU cores and 512 GB DDR5 RAM with Windows server OS. Given the server's capacity, we managed to execute the sensitivity and calibration processes for five different watersheds simultaneously, leveraging a nested parallel processing pool.
Table 3. The daily and monthly SWAT+ model performance for different streamflow stations 

Parallel processing 

	The primary challenges we faced in managing SWAT+ model executions in parallel processing revolved around three main key aspects: 1- the inherent single core architecture of SWAT+ models, 2- the varying execution times of each SWAT+ model, 2- the distinct computational nature of sensitivity analysis and calibration processes. Our aim in parallel processing was to use a sustainable, scalable, and efficient use of our computational power, minimizing the efforts as much as possible for monitoring the system resource or manual scheduling of different model calibration/sensitivity analysis. Moreover, we also required a flexible and scalable approach to eliminate any potential for resource contention among SWAT+ models in the one node server and be as much as flexible to be utilized in a distributed computing system for our future studies. 
	SWAT+ modeling architecture: SWAT+ is a sequential computing different processes operate as a single threaded application (each model occupying one logical core). The SWAT model code architecture cannot be parallelized, and even if the internal loops that result in bottleneck be detected in SWAT to become parallel processes, it does not guarantee improvement over execution time and can potentially results in models that their CPU usage will not be consistent over the simulation period, resulting in resource contention in a parallel processing environment involving a dozen of models. However, the SWAT models can be created for different watersheds and run in parallel to complete a full picture of a larger watershed or a region. Also, it should be noted that SWAT+, like any other process-based hydrological models, uses a series of empirical equations to predict hydrological processes (runoff, recharge, snow melting, etc.). This empirical nature of the model results in different optimal set of parameters for different watersheds with different characteristics. Therefore, unlike fully physically based grid-based models, many of the SWAT model parameters does not have a physical representation on the ground. Therefore, the best way to calibrate SWAT models is in cluster modeling. 
Varying Resource Usage:  SWAT+ models, depending on their complexity and the number of HRUs, exhibit significant variability in memory usage (from less than 100MB to over 2GB) and execution time (couple of minutes for small models (~3000 HRUs) to more than 20 hours for large models (~40,000 HRUs). Also, the SWAT+ execution times are not uniformly predictable (even among the same models but with different parameter configuration). This presents a challenge in scheduling and managing these models in a parallel processing environment while keeping CPU and memory usage at a certain level for sustainable resource usage and long-time model processing. Furthermore, each model requires a suitable wall time to prevent computational stagnation which usually occurs in sensitivity or calibration when an incorrect combination of parameters cause the model stuck in a non-optimal state that hinders its ability to reach a conclusion within a reasonable time frame. 
Differing Nature of Sensitivity Analysis and Calibration: Sensitivity analysis and calibration processes inherently have different computational demands. Since samples for sensitivity analysis are pre-defined, the pool execution can be as large as the number of available CPUs or the total number of samples which in our case with using one server node, the number of available CPU was the limiting factor. On the other hand, the calibration process, involving Bayesian optimization, need to be updated regularly to add new models with new parameters in the parallel processing pool, therefore, it does not benefit from excessive parallel processing beyond a certain point.  
Number of SWAT models: As previously discussed, we have prepared individual SWAT+ models for different watersheds with the aim of reducing unnecessary model simulations for the watersheds or catchments without monitoring station. Therefore, each model requires a separate line of processing for sensitivity analysis and calibration. Acknowledging that we have 50 SWAT+ models with varying duration for sensitivity analysis and calibration (couple of hours to several days), an efficient job scheduling to use CPU and RAM as efficiently and constantly as possible is required. Therefore, our system should be able to dynamically allocate resources based on the current demands of each model's sensitivity analysis and calibration processes.
To address these challenges, we developed a two-layered approach in our parallel processing environment using Python’s Multiprocessing module. The code architecture includes mechanisms to monitor system load and ongoing model calibration/sensitivity analysis at outer layer as follows:  
Outer Layer for Process Management: This layer is responsible for efficiently allocating computational resources and managing the scheduling of SWAT+ model sensitivity and calibrations. It dynamically initiates model processing programs based on current system resource utilization and the status of ongoing processes. This layer ensures that new models are only executed when there is sufficient computational capacity, preventing system overload. This approach helped optimize the use of resources since once a model sensitivity analysis is about to finish, the CPU usage drops, therefore a new sensitivity analysis can be executed while the first sensitivity transit to calibration. The outer layer eliminates the reliance to find an optimal number of pool size for calibration/sensitivity analysis or the wall time for the model execution because once the outer layer recognize the CPU usage has dropped (or the number of models under simulations) it initiates a new sensitivity analysis that fill the gap in CPU usage.  
Inner Layer for Task-Specific Processing: Within this layer, we implemented customized process pools for sensitivity analysis and calibration. Sensitivity analysis utilized up to 200 parallel processes (consuming up to 80% of total 256 logical cores, leaving the rest of 20% free for other routine jobs in the server). The calibration process was limited to a maximum of 100 parallel processes. This decision, as said, was informed by the nature of Bayesian optimization used in calibration, which does not linearly benefit from increased parallelization beyond a certain threshold, especially considering its periodic updating mechanism and the potential for early termination based on predefined criteria. 
	The parallel processing approach implemented for sensitivity analysis begins by creating a manager and a queue to manage the distribution of tasks among multiple processes. For each point in the sensitivity analysis space, a process is initiated and added to a queue. These processes execute the model evaluation function in the background. The system dynamically controls the number of active processes based on the available computational resources, by monitoring CPU, memory, and number of models under execution. When a process completes its task, it is removed from the active pool, and its results are collected. The next point will be added to the pool only if there is enough computational capacity. 
	In the Bayesian optimization function, a dynamic batch processing method is used. The function initiates a specified number of processes (a parallel pool with maximum 100 model in the parallel processing) to evaluate different sets of parameters concurrently. As each process completes its task, the results are collected and evaluated. The system continuously adds new processes to replace completed ones, keeping the number of active processes close to the pool size. This approach allows for a steady flow of process execution and result collection, efficiently utilizing available computational resources. Contrastingly, in the sensitivity analysis function, a queue-based parallel processing approach is employed. Here, a manager creates a queue where model evaluation tasks are added. Processes are spawned to handle tasks from this queue, running up to the defined number of parallel jobs. This method ensures that new tasks are continuously added to the queue and processed as soon as a CPU becomes available, maintaining high CPU usage and efficient resource allocation. Unlike the batch processing in Bayesian optimization, the queue system in sensitivity analysis keeps adding and processing tasks individually until all tasks are completed.
	More importantly, the success of this approach isn't heavily reliant on selecting optimal parameters for pool sizes or wall times. This is primarily because the size of the parallel processes is flexible, allowing the parallel processing pool to shrink or expand based on the availability of resources. Additionally, resource contention is effectively managed as we consistently maintain a 10% buffer zone in the server node. This buffer zone accommodates any extra resource usage arising from variations in the sizes of the models or our routine work on the server. 
	Overall, this dynamic two-layer scheduling was instrumental in maintaining a sustainable level of efficiency, allowing for simultaneous processing of multiple SWAT+ models while ensuring that each model received the necessary computational resources for its specific requirements. Consequently, this approach allowed us to maximize the throughput and effectiveness of our high-resource computing environment, handling the diverse and variable computational needs of the SWAT+ models with precision and efficiency.

Results

































Solar Radiation Data
We obtained high resolution (4km, hourly) solar radiation from National Solar Radiation Database.  NSRDB (Sengupta et al., 2018), consist of three mostly widely used measurements of solar radiation-global horizontal, direct normal and diffuse horizontal irradiance as well as other meteorological data such as wind speed, relative humidity and wind direction that are normally missed in climate and climate change dataset. The current NSRDB is modeled using National Renewable Energy Laboratory (NREL) Physical solar Model (PSM) with inputs from multi-channel measurements obtained from the Geostationary Operational Interactive Multisensor Snow and Ice Mapping System (IMS) of National Ice Center (NIC) and the Moderate Resolution Imaging Spectroradiometer (MODIS) and Modern Era Retrospective analysis for Research and Applications, version 2 (MERRA-2), of the National Aeronautics and Space administration (NASA). 






Supplementary material

Terminology:
Watershed: The largest component of the model
Subbasin: The largest component of a watershed
Catchment: The smallest component of a subbasin

Abbreviations:
NHD: National Hydrographical Dataset
VPUID: Vectorized Processing Unit ID
VAA: Value Added Attribute 
WBD: Watershed Boundary Dataset
HUC: Hydrologic Unit Code

1. SWAT+ Hydrographical Model Construction Using NHDPlus HR2
	This document outlines the methodology for constructing predefined SWAT+ predefined models based on the NHDPlus High-Resolution (HR2) database.
1.1. Data Preparation and Import
	The primary hydrographical data were extracted from the NHDPlus database for the Vector Processing Units (VPUIDs) of 405, 406, 407, 408, 409, and 410 covering the entire Michigan lower Peninsula. 
  - WBDHU8 and WBDHU12: representing basin/subbasin boundary.
  - NHDFlowline: representing streamlines.
  - NHDPlusCatchment: representing the drainage area of a streamline.
  - NHDWaterbody: Representing reservoirs, lakes, wetlands, and playa. 
  - NHDPlusFlowlineVAA: value-added attribute table for streams and catchments characteristics. The following attribute values were used for preparing the SWAT+ predefined watershed. 
VPUID: Identifier for Vector Processing Unit (equivalent to HUC4)
NHDPlusID: Common key among NHDPlusFlowlines, NHDPlusCatchment and NHDPlusVAA.
StreamOrder: The stream order is based on the Strahler method.
UpHydroSeq: The hydrological sequence number for the upstream.
HydroSeq: The hydrological sequence number for the flowline.
DnHydroSeq: The hydrological sequence number for the downstream.
StartFlag: Flag indicating if the flowline is a headwater.
TerminalFl: Flag indicating if the flowline is a terminal point (outlet).
Divergence: Indicates if a flowline diverges into multiple downstream paths.
Permanent_Identifier: A permanent identifier for each flowline.
WBArea_Permanent_Identifier: Identifier for the associated water body area.
MaxElevSmo: Smoothed maximum elevation of the flowline.
MinElevSmo: Smoothed minimum elevation of the flowline.
AreaSqKm: Drainage area in square kilometers.
LengthKM: Original length of the flowline in kilometers (later converted to meters).
TotDASqKm: Total drainage area in square kilometers.

1.2. General preprocessing:
CRS: The geometries were projected to EPSG: 26990
Removing Second Divergence: Streams with a divergence code of 2 were eliminated and their corresponding drainage area was merged to their downstream. 
Data Consistency Check: We excluded flowlines that did not have corresponding drainage areas.
Coastal Line Removal: Flowlines designated as coastal lines were removed from the dataset.
Isolated Streams: Streams that were isolated, i.e., neither flowing into nor out of other streams without corresponding drainage area in the NHDPlusCatchment were removed. 
Length Unit Conversion: The lengths of the streams, originally in kilometers, were converted to meters.
Drop Calculation: Drop is required by SWAT+ and represents the difference between the maximum and minimum elevation of a flowline. 
Reset Flags: The Start and Terminal flags were reset based on the above revision.
Identifying Hydrologic Unit Codes (HUC): The centroids of the catchment were spatially joined with the HUC shapefiles. The process is carried out separately for HUC8 and HUC12 levels.

1.3. Extracting HUC12 Based on Streamflow Stations
	In this step, the focus is on pinpointing the specific Hydrologic Unit Codes (HUC12) that are directly related to each streamflow monitoring station. This is essential for understanding the drainage areas contributing to each station. The methodological steps are outlined below:
Data Import: Streamflow station data and processed stream data will be imported into GeoDataFrames. They are also aligned to a common Coordinate Reference System (CRS) EPSG:26990 for compatibility with our case study Michigan.
Data Pre-filtering: Stations are filtered based on drainage area and data availability. This results in a selection of streamflow stations (along with their HydroSeq numbers) that meet specified criteria, such as a drainage area below 1500 sqkm with a maximum 5 percent of data gaps for the period between 1999 to 2020.
Upstream HydroSeq Identification: For each selected streamflow station, a recursive function identifies all upstream HydroSeq numbers. This process builds a list of HydroSeq numbers that ultimately drain into the location where the station is situated.
HUC12 Extraction: For each upstream HydroSeq, the corresponding HUC12 codes are extracted from the stream data. These codes are compiled into a list that represents the HUC12 units contributing to each streamflow station.

1.4. Configuring NHDPlus Data Based on SWAT+ Hydrographical Logic: The One-Outlet Rule
	SWAT model has a one-outlet rule that requires each subbasin to have only one stream flowing into one downstream subbasin. However, HUC12 boundaries often do not adhere to this rule, leading to subbasins with multiple outlets. Several conditions that violate this rule include 1-isolated subbasins within a HUC12, 2- the confluence of two streams draining into a downstream subbasin, and 3- a normal condition when a subbasin has different outlets leading to two or more downstream subbasins.
	To enforce the one-outlet rule, a systematic review of the subbasins is conducted to identify the number of outlets each possesses. An outlet is defined as a point where a stream exits a subbasin, either entering another subbasin or leaving the hydrological domain. In cases where a subbasin has more than one outlet, we used a recursive function to trace all upstream segments of a multi-outlet subbasin. These segments subsequently form a new subbasin, replacing the original one with multiple outlets. Once the subbasins are redefined, they are assigned new unique identifiers. The process involves iterative checks to ensure that each subbasin adheres to the one-outlet criterion.
1.5. Defining Flowline-Waterbody Connections
	For incorporating waterbodies into the hydrograph of the SWAT model, it's essential to identify lake inlet and outlet segments within flowlines. This includes the following steps:
Stream Classification: Waterbodies are categorized into four types: lakes, reservoirs, wetlands, and playas, based on their 'Ftype'.
Area Threshold: Waterbodies smaller than 0.1 sq km are excluded from the dataset.
Lake Identification (LakeId): To identify LakeId in flowlines, flowline data is merged with waterbody datasets, aligning the 'Permanent_Identifier' from waterbodies with the 'WBArea_Permanent_Identifier' in flowlines. Flowlines associated with lakes are initially considered as LakeWithin.
Lake Inlets (LakeIn): LakeIn segments are identified by traversing from headwater to outlet. Segments draining into a lake (indicated by a downstream LakeId) are tagged as LakeIn.
Lake Outlets (LakeOut): Each stream segment is examined to ascertain if it has a LakeId upstream, marking it as an outflow from a lake.
Main Lake Outlet (LakeMain): LakeMain is determined based on stream order, selecting the segment with the highest order in each lake as the primary outflow channel.
Updating LakeWithin: Initially, all segments within a lake's boundary are classified as LakeWithin. This classification is updated as outflow segments (LakeOut) are identified.
Special Cases Handling: In creating SWAT+ lakes from NHDPlus data, certain special cases conforming to SWAT+'s hydrographical structure are addressed. For instance, adjacent lakes in NHDPlus data with different permanent IDs are merged to share the same LakeId, or, if a lake's outlet coincides with the watershed's outlet, the last LakeWithin segment before the outlet is specifically designated as the LakeOut.
Subbasin refinement
Once the streams (flowlines) are prepared as described above, we join them with NHDCatchments based on the NHDPlusID. Subsequently, the subbasins were created based on the dissolving catchments by their subbasin number that we obtained during stream modification. Given the changes to the subbasins that we made to follow one-outlet-rule, we had to refine the subbasins to avoid excessively small subbasins. To do this, we performed another round of simplification by increasing the size of a subbasin to a minimum of 250*250*1000m. We accomplish this by iteratively dissolving subbasins to their downstream subbasins until the size of new subbasins are above the threshold. This threshold size is equal to 75% of the average size of HUC12 in Michigan. At the end, the subbasins below this threshold were typically isolated with no upstream or downstream connection. 
1.6 Quality assurance check:
Single Association for Streams and Catchments: Confirmed that each stream is associated with only one catchment, and vice versa.
Unique Subbasin Association: Verified that each catchment or stream is associated with only one subbasin.
Lake Outlet Requirement: Ensured that every lake has at least one outlet.
Avoidance of Circular Hydrographs: Checked for and eliminated any divergence that could cause circular hydrographs.
Single Outlet per Subbasin: Make certain that each subbasin has only one outlet.
ID Initialization: Ensured that both channel and Lake IDs start from 1.



The geological and hydrological aspects of Michigan you inquired about are well-documented in various sources. Here are the references for the statements in your text:
Sedimentary Layers in Michigan During the Paleozoic Era:
The Paleozoic period, lasting about 325 million years, saw significant geological changes. During this time, Michigan's Lower Peninsula, part of the Michigan Basin, accumulated hundreds of meters of sediments, which later turned into rocks and associated deposits like halite, gypsum, petroleum, lime, clay, sandstone, and coal​​. paleozoic rocks in Michigan (msu.edu)
The sedimentary rocks in the Michigan Basin, including limestones, sandstones, and shales, are approximately 500 million years old. In the center of the basin, these sedimentary rocks can be about 14,000 feet thick​​.
Glacial Layer and Landscape in the Lower Peninsula:
The Pleistocene glaciers, particularly during the Wisconsin stage, shaped Michigan's landscape. These glaciers leveled hills, filled valleys, blocked rivers, gouged out basins for the Great Lakes, and changed the surface through various geological processes​​. glacial landforms (msu.edu)
Glacial landforms dominate the surface of the whole state except the western half of the Upper Peninsula, where eroded remnants of some of the oldest mountains on Earth are found. glacial landforms (msu.edu)
Six major landform types emerged from glacial activities in Michigan, including moraines, till plains, outwash plains, lake clay plains, lake sand plains, and rock outcrop areas​​. glacial landforms (msu.edu)
Glacial activity led to the formation of "kettles," deep, steep-sided depressions, some of which became lake basins​​. glacial landforms (msu.edu)
Outwash plains in northern Michigan, part of the glacial deposits, have poorer soils and have been historically challenging for agriculture​​.
Lake plains, like those in Saginaw and Monroe counties, are rich in agriculture but require careful management and drainage​​.
Shallow Groundwater Formation in Michigan:
In Michigan, sufficient water supplies can be accessed by digging wells into the surface layers of glacial material or into rock layers acting as aquifers, with sandstone and limestone being significant contributors to groundwater storage. The sedimentary rock aquifers in the Lower Peninsula, part of the Michigan Basin, are particularly important for springs and artesian wells​​.
Groundwater plays a vital role in the Great Lakes ecosystem, acting as a reservoir for water and replenishing the lakes through base flow in tributaries. It is also a crucial source of drinking water for many communities​​.
As water passes through subsurface areas, it dissolves and suspends materials from the soils, contributing to the 'hard' water commonly found in the lower Great Lakes basin​​.
These references provide a detailed understanding of Michigan's geological and hydrological characteristics, shaped significantly by both its ancient sedimentary history and the extensive glacial activity during the Pleistocene period.

Exceptional watersheds extracted from 
The Jordan River is one of the most stable flowing streams in Michigan and has one of the highest baseflow yields in the state. It is one of only a few streams in the state that capture groundwater from adjacent watersheds. 

Groundwater flows into the Jordan River also benefit from being located at a lower altitude than adjacent inland river basins. This river captures additional groundwater from these adjacent basins, creating exceptionally high base flows

The Jordan River has a very large baseflow yield of about 2.5 cfs/mi2. 

The Jordan River is an exception in that the ratio of 1.2 indicates that the discharge is greater than what can be accounted for in precipitation falling within the catchment. It appears that the Jordan River catchment captures groundwater from more elevated inland watersheds to the southeast, most likely the Manistee and Au Sable Rivers (Hendrickson and Doonan 1972).

Snow Data Assimilation System (SNODAS): 
Snow Data Assimilation System (SNODAS) (Barrett, 2003) is a modeling and data assimilation system developed by the National Operational Hydrologic Remote Sensing Center (NOHRSC). SNODAS provides the best possible estimates of snow cover and associated variables over the conterminous United States by employing a physically consistent framework to integrate snow data from satellites, airborne platforms, and ground stations with model estimates of snow cover. The daily output of SNODAS data is provided by the National Snow and Ice Data Center (NSIDC) and has a 1km*km resolution. SNODAS database consists of a suite of 8 daily variables from 28 September 2003 to the present, including snow melt rate, snow water equivalent, snow depth, snow melt runoff, sublimation from the snowpack, sublimation of blowing snow, solid precipitation, and liquid precipitation.  
For this study and achieve to a consistent and physically based estimate of snow melt, we have integrated SNODAS output data with the SWAT+ model. This is achieved by modifying the SWAT+ snow module to use the SNODAS snow melt estimates where available and using its default model where no data is available. Where or when the SNODAS data are not, we use the built-in snow module of the SWAT+ and curb its estimation by comparing it with the median monthly snowmelt rate estimated by SNODAS.  This integration required modifying some of the SWAT+ source codes including snowdb_read.f90 subroutine, and sq_snom.f90. 


  





1. Snowpack average temperature: This typically refers to the average air temperature over a specific time period. It's crucial for understanding the thermal regime of the snowpack and its surroundings.
2. Blowing Snow Sublimation Rate: This measures the rate at which snow is sublimated (converted directly from solid to gas) during blowing snow events. This process can significantly reduce snowpack volume, especially in windy, dry conditions.
3. Melt Rate: The rate at which snow melts, often expressed in terms of water equivalent per unit time. This is influenced by air temperature, solar radiation, and other factors.
4. Snow Layer Thickness: The depth of the snowpack. It can indicate the volume of water stored in the snowpack and is important for assessing snowpack stability and hydrological importance.
5. Snow Water Equivalent (SWE): A common measure in hydrology and snow science, SWE represents the amount of water contained within the snowpack. It's a critical variable for understanding water supply, flood risk, and the overall water cycle.
6. Snowpack Sublimation Rate: Similar to blowing snow sublimation, but this refers to the rate of sublimation from the stationary snowpack. It's an important component of the snowpack's mass balance.
7. Non-Snow Accumulation: This might refer to the accumulation of precipitation in forms other than snow, like rain or sleet, particularly in areas with mixed precipitation types.
8. Snow Accumulation: The rate or amount of snowfall accumulation over time. It's essential for understanding seasonal snow cover changes and water resource management.

Parallel processing in Bayesian Optimization:
We implemented a dynamic approach to process management during the Bayesian optimization loop. This approach includes an adaptive handling of the number of active processes and early termination based on the progress of the optimization. Here is a key summary:
Initial Pool of Processes: We start by creating an initial pool of processes equal to a certain number of parallel processes. In the next stage, we created a monitoring loop to check if the number of active processes is less than the number of parallel jobs, and if so, we add new processes up to the parallel job limit. Each new process is given a 30-second start-up period.
Termination Based on Iteration Progress: The code checks if 90% of the total iterations (n_iter) have been completed. If so, it terminates all remaining processes.
Periodic Check for Best Objective Value: The code periodically checks for the best objective value every 50 simulations. This is done by collecting results, updating the optimizer, and logging the best objective value found so far.


Reference
Bailey, R.T., Bieger, K., Arnold, J.G., Bosch, D.D., 2020. A new physically-based spatially-distributed groundwater flow module for SWAT+. Hydrology 7, 75.
Barrett, A.P., 2003. National operational hydrologic remote sensing center snow data assimilation system (SNODAS) products at NSIDC. National Snow and Ice Data Center, Cooperative Institute for Research in ….
Buto, S.G., Anderson, R.D., 2020. NHDPlus high resolution (NHDPlus HR)---A hydrography framework for the nation (No. 2327–6932). US Geological Survey.
Feinstein, D., Hunt, R., Reeves, H., 2010. Regional groundwater-flow model of the Lake Michigan Basin in support of Great Lakes Basin water availability and use studies. U. S. Geological Survey.
Leavesley, G., Stannard, L., 1995. The precipitation-runoff modeling system-PRMS. Computer models of watershed hydrology. 281–310.
Morris, M.D., 1991. Factorial sampling plans for preliminary computational experiments. Technometrics 33, 161–174.
Ou, G., Chen, X., Kilic, A., Bartelt-Hunt, S., Li, Y., Samal, A., 2013. Development of a cross-section based streamflow routing package for MODFLOW. Environmental modelling & software 50, 132–143.
Sengupta, M., Xie, Y., Lopez, A., Habte, A., Maclaurin, G., Shelby, J., 2018. The national solar radiation data base (NSRDB). Renewable and sustainable energy reviews 89, 51–60.
Sulis, M., Paniconi, C., Camporese, M., 2011. Impact of grid resolution on the integrated and distributed response of a coupled surface–subsurface hydrological model for the des Anglais catchment, Quebec. Hydrological Processes 25, 1853–1865.
Van Zanen, A., Hughes, S., 2022. Assessing Policy Drivers and Barriers for Sustainable Groundwater Management in Michigan.
White, M.J., Arnold, J.G., Bieger, K., Allen, P.M., Gao, J., Čerkasova, N., Gambone, M., Park, S., Bosch, D.D., Yen, H., 2022. Development of a field scale SWAT+ modeling framework for the contiguous US. JAWRA Journal of the American Water Resources Association 58, 1545–1560.
Yang, C., Tijerina-Kreuzer, D., Tran, H., Condon, L., Maxwell, R., 2023. A high-resolution, 3D groundwater-surface water simulation of the contiguous US: Advances in the integrated ParFlow CONUS 2.0 modeling platform.
National Snow and Ice Data Center. (2004). Snow Data Assimilation System (SNODAS) Data Products at NSIDC, Version 1. Boulder, Colorado USA: National Snow and Ice Data Center. DOI: 10.7265/N5TB14TC.